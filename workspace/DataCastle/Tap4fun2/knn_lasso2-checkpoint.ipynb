{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tap_fun_test.csv', 'xgb_feat_train.pkl', '.DS_Store', 'test1.txt', 'xgb_feat_test.pkl', 'test2.txt', 'test_proba.pkl', 'label.pkl', 'tap4fun 数据字段解释.xlsx', 'nn_train_data.pkl', 'tap_fun_train.csv', 'knn_lasso_data.pkl', 'label1.pkl', 'test_id.pkl', 'label2.pkl', 'gbdt2_train_data.pkl', 'train2.txt', 'test_id2.pkl', 'sub_sample.csv', 'train1.txt', 'test_id1.pkl']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangle/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/wangle/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv('../data/tap_fun_train.csv')\n",
    "test_df = pd.read_csv('../data/tap_fun_test.csv')\n",
    "test_df[\"prediction_pay_price\"] = -1\n",
    "test_usid = test_df.user_id\n",
    "data = pd.concat([train_df, test_df])\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n",
    "# We're going to be calculating memory usage a lot,\n",
    "# so we'll create a function to save us some time!\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "gl_int = data.select_dtypes(include=['int'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "gl_float = data.select_dtypes(include=['float'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_float.apply(pd.to_numeric,downcast='float')\n",
    "del gl_int, gl_float\n",
    "gc.collect()\n",
    "\n",
    "#只取pay_price不为0的用户\n",
    "data = data[data.pay_price != 0]\n",
    "#添加注册用户的注册时间为周几的特征\n",
    "new = data[['user_id', 'register_time']]\n",
    "new['register_time'] = pd.to_datetime(new['register_time'])\n",
    "new['weekday'] = new['register_time'].apply(lambda x:x.weekday())\n",
    "data = pd.merge(data, new[['user_id', 'weekday']], how = 'left', on = 'user_id')\n",
    "\n",
    "def process(data):    \n",
    "    data[\"wood_sub_value_abs\"] = np.abs(data[\"wood_add_value\"] - data[\"wood_reduce_value\"])\n",
    "    data[\"stone_sub_value_abs\"] = np.abs(data[\"stone_add_value\"] - data[\"stone_reduce_value\"])\n",
    "    data[\"ivory_sub_value_abs\"] = np.abs(data[\"ivory_add_value\"] - data[\"ivory_reduce_value\"])\n",
    "    data[\"meat_sub_value_abs\"] = np.abs(data[\"meat_add_value\"] - data[\"meat_reduce_value\"])\n",
    "    data[\"magic_sub_value_abs\"] = np.abs(data[\"magic_add_value\"] - data[\"magic_reduce_value\"])\n",
    "    data[\"infantry_sub_value_abs\"] = np.abs(data[\"infantry_add_value\"] - data[\"infantry_reduce_value\"])\n",
    "    data[\"cavalry_sub_value_abs\"] = np.abs(data[\"cavalry_add_value\"] - data[\"cavalry_reduce_value\"])\n",
    "    data[\"shaman_sub_value_abs\"] = np.abs(data[\"shaman_add_value\"] - data[\"shaman_reduce_value\"])\n",
    "    data[\"wound_infantry_sub_value_abs\"] = np.abs(data[\"wound_infantry_add_value\"] - data[\"wound_infantry_reduce_value\"])\n",
    "    data[\"wound_cavalry_sub_value_abs\"] = np.abs(data[\"wound_cavalry_add_value\"] - data[\"wound_cavalry_reduce_value\"])\n",
    "    data[\"wound_shaman_sub_value_abs\"] = np.abs(data[\"wound_shaman_add_value\"] - data[\"wound_shaman_reduce_value\"])\n",
    "    data[\"general_acceleration_sub_value_abs\"] = np.abs(data[\"general_acceleration_add_value\"] - data[\"general_acceleration_reduce_value\"])\n",
    "    data[\"building_acceleration_sub_value_abs\"] = np.abs(data[\"building_acceleration_add_value\"] - data[\"building_acceleration_reduce_value\"])\n",
    "    data[\"reaserch_acceleration_sub_value_abs\"] = np.abs(data[\"reaserch_acceleration_add_value\"] - data[\"reaserch_acceleration_reduce_value\"])\n",
    "    data[\"training_acceleration_sub_value_abs\"] = np.abs(data[\"training_acceleration_add_value\"] - data[\"training_acceleration_reduce_value\"])\n",
    "    data[\"treatment_acceleration_sub_value_abs\"] = np.abs(data[\"treatment_acceleraion_add_value\"] - data[\"treatment_acceleration_reduce_value\"])\n",
    "    data[\"bd_mean\"] = data.iloc[:,34:50].mean(1)\n",
    "    data[\"bd_std\"] = data.iloc[:,34:50].std(1)\n",
    "    data[\"bd_max\"] = data.iloc[:,34:50].max(1)\n",
    "    data[\"bd_min\"] = data.iloc[:,34:50].min(1)\n",
    "    data[\"sr_mean\"] = data.iloc[:,50:99].mean(1)\n",
    "    data[\"sr_std\"] = data.iloc[:,50:99].std(1)\n",
    "    data[\"sr_max\"] = data.iloc[:,50:99].max(1)\n",
    "    data[\"sr_min\"] = data.iloc[:,50:99].min(1)\n",
    "    data[\"source_add_value_mean\"] = data.iloc[:,2:11:2].mean(1)\n",
    "    data[\"source_add_value_std\"] = data.iloc[:,2:11:2].std(1)\n",
    "    data[\"source_add_value_max\"] = data.iloc[:,2:11:2].max(1)\n",
    "    data[\"source_add_value_min\"] = data.iloc[:,2:11:2].min(1)\n",
    "    data[\"source_reduce_value_mean\"] = data.iloc[:,3:12:2].mean(1)\n",
    "    data[\"source_reduce_value_std\"] = data.iloc[:,3:12:2].std(1)\n",
    "    data[\"source_reduce_value_max\"] = data.iloc[:,3:12:2].max(1)\n",
    "    data[\"source_reduce_value_min\"] = data.iloc[:,3:12:2].min(1)\n",
    "    data[\"military_add_value_mean\"] = data.iloc[:,12:17:2].mean(1)\n",
    "    data[\"military_add_value_std\"] = data.iloc[:,12:17:2].std(1)\n",
    "    data[\"military_add_value_max\"] = data.iloc[:,12:17:2].max(1)\n",
    "    data[\"military_add_value_min\"] = data.iloc[:,12:17:2].min(1)\n",
    "    data[\"military_reduce_value_mean\"] = data.iloc[:,13:18:2].mean(1)\n",
    "    data[\"military_reduce_value_std\"] = data.iloc[:,13:18:2].std(1)\n",
    "    data[\"military_reduce_value_max\"] = data.iloc[:,13:18:2].max(1)\n",
    "    data[\"military_reduce_value_min\"] = data.iloc[:,13:18:2].min(1)\n",
    "    data[\"wound_add_value_mean\"] = data.iloc[:,18:23:2].mean(1)\n",
    "    data[\"wound_add_value_std\"] = data.iloc[:,18:23:2].std(1)\n",
    "    data[\"wound_add_value_max\"] = data.iloc[:,18:23:2].max(1)\n",
    "    data[\"wound_add_value_min\"] = data.iloc[:,18:23:2].min(1)\n",
    "    data[\"wound_reduce_value_mean\"] = data.iloc[:,19:24:2].mean(1)\n",
    "    data[\"wound_reduce_value_std\"] = data.iloc[:,19:24:2].std(1)\n",
    "    data[\"wound_reduce_value_max\"] = data.iloc[:,19:24:2].max(1)\n",
    "    data[\"wound_reduce_value_min\"] = data.iloc[:,19:24:2].min(1)\n",
    "    data[\"acceleration_add_value_mean\"] = data.iloc[:,24:33:2].mean(1)\n",
    "    data[\"acceleration_add_value_std\"] = data.iloc[:,24:33:2].std(1)\n",
    "    data[\"acceleration_add_value_max\"] = data.iloc[:,24:33:2].max(1)\n",
    "    data[\"acceleration_add_value_min\"] = data.iloc[:,24:33:2].min(1)\n",
    "    data[\"acceleration_reduce_value_mean\"] = data.iloc[:,25:34:2].mean(1)\n",
    "    data[\"acceleration_reduce_value_std\"] = data.iloc[:,25:34:2].std(1)\n",
    "    data[\"acceleration_reduce_value_max\"] = data.iloc[:,25:34:2].max(1)\n",
    "    data[\"acceleration_reduce_value_min\"] = data.iloc[:,25:34:2].min(1)\n",
    "    data[\"atk_level_mean\"] = data[[\"sr_infantry_atk_level\",\"sr_cavalry_atk_level\",\"sr_shaman_atk_level\",\"sr_troop_attack_level\"]].mean(1)\n",
    "    data[\"atk_level_std\"] = data[[\"sr_infantry_atk_level\",\"sr_cavalry_atk_level\",\"sr_shaman_atk_level\",\"sr_troop_attack_level\"]].std(1)\n",
    "    data[\"atk_level_max\"] = data[[\"sr_infantry_atk_level\",\"sr_cavalry_atk_level\",\"sr_shaman_atk_level\",\"sr_troop_attack_level\"]].max(1)\n",
    "    data[\"atk_level_min\"] = data[[\"sr_infantry_atk_level\",\"sr_cavalry_atk_level\",\"sr_shaman_atk_level\",\"sr_troop_attack_level\"]].min(1)\n",
    "    data[\"def_level_mean\"] = data[[\"sr_infantry_def_level\",\"sr_cavalry_def_level\",\"sr_shaman_def_level\",\"sr_troop_defense_level\"]].mean(1)\n",
    "    data[\"def_level_std\"] = data[[\"sr_infantry_def_level\",\"sr_cavalry_def_level\",\"sr_shaman_def_level\",\"sr_troop_defense_level\"]].std(1)\n",
    "    data[\"def_level_max\"] = data[[\"sr_infantry_def_level\",\"sr_cavalry_def_level\",\"sr_shaman_def_level\",\"sr_troop_defense_level\"]].max(1)\n",
    "    data[\"def_level_min\"] = data[[\"sr_infantry_def_level\",\"sr_cavalry_def_level\",\"sr_shaman_def_level\",\"sr_troop_defense_level\"]].min(1)\n",
    "    data[\"hp_level_mean\"] = data[[\"sr_infantry_hp_level\",\"sr_cavalry_hp_level\",\"sr_shaman_hp_level\"]].mean(1)\n",
    "    data[\"hp_level_std\"] = data[[\"sr_infantry_hp_level\",\"sr_cavalry_hp_level\",\"sr_shaman_hp_level\"]].std(1)\n",
    "    data[\"hp_level_max\"] = data[[\"sr_infantry_hp_level\",\"sr_cavalry_hp_level\",\"sr_shaman_hp_level\"]].max(1)\n",
    "    data[\"hp_level_min\"] = data[[\"sr_infantry_hp_level\",\"sr_cavalry_hp_level\",\"sr_shaman_hp_level\"]].min(1)\n",
    "    data[\"sr_prod_level_mean\"] = data[[\"sr_rss_a_prod_levell\",\"sr_rss_b_prod_level\",\"sr_rss_c_prod_level\",\"sr_rss_d_prod_level\"]].mean(1)\n",
    "    data[\"sr_prod_level_std\"] = data[[\"sr_rss_a_prod_levell\",\"sr_rss_b_prod_level\",\"sr_rss_c_prod_level\",\"sr_rss_d_prod_level\"]].std(1)\n",
    "    data[\"sr_prod_level_max\"] = data[[\"sr_rss_a_prod_levell\",\"sr_rss_b_prod_level\",\"sr_rss_c_prod_level\",\"sr_rss_d_prod_level\"]].max(1)\n",
    "    data[\"sr_prod_level_min\"] = data[[\"sr_rss_a_prod_levell\",\"sr_rss_b_prod_level\",\"sr_rss_c_prod_level\",\"sr_rss_d_prod_level\"]].min(1)\n",
    "    data[\"sr_gather_level_mean\"] = data[[\"sr_rss_a_gather_level\",\"sr_rss_b_gather_level\",\"sr_rss_c_gather_level\",\"sr_rss_d_gather_level\"]].mean(1)\n",
    "    data[\"sr_gather_level_std\"] = data[[\"sr_rss_a_gather_level\",\"sr_rss_b_gather_level\",\"sr_rss_c_gather_level\",\"sr_rss_d_gather_level\"]].std(1)\n",
    "    data[\"sr_gather_level_max\"] = data[[\"sr_rss_a_gather_level\",\"sr_rss_b_gather_level\",\"sr_rss_c_gather_level\",\"sr_rss_d_gather_level\"]].max(1)\n",
    "    data[\"sr_gather_level_min\"] = data[[\"sr_rss_a_gather_level\",\"sr_rss_b_gather_level\",\"sr_rss_c_gather_level\",\"sr_rss_d_gather_level\"]].min(1)\n",
    "\n",
    "    import time\n",
    "    a = data['register_time'].apply(lambda x:time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M:%S\")))\n",
    "#     a /= 3600\n",
    "    a /= (3600 * 24)  #这里试试天数会不会更有效果\n",
    "    data['regedit_diff_day'] = (a - min(a))\n",
    "    \n",
    "    data['regedit_diff_week'] = a.apply(lambda x:int(x)) % 7\n",
    "    \n",
    "    a = data['register_time'].apply(lambda x:time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    a /= 3600\n",
    "    data['regedit_diff_hour'] = (a - min(a))\n",
    "    \n",
    "    #\n",
    "    new = data[['prediction_pay_price','user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-02-02', '2018-02-09', '2018-02-16', '2018-02-23', '2018-03-02','2018-03-09','2018-03-16']\n",
    "    new['date_week'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_week', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    new = data[['prediction_pay_price','user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-02-13', '2018-03-16']\n",
    "    new['date_week_two'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_week_two', 'user_id']], on='user_id',how='left')\n",
    "\n",
    "    \n",
    "    #\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-03-10', '2018-02-19']\n",
    "    new['date_holiday'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_holiday', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    ###\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[1])\n",
    "    new['date'] = new['date'].apply(lambda x:int(x.split(':')[0]))\n",
    "    new['date_h_1'] = new.date.apply(lambda x:1 if ((x >= 0) & (x < 4) )else 0)\n",
    "    new['date_h_2'] = new.date.apply(lambda x:1 if ((x >= 4) & (x < 8) )else 0)\n",
    "    new['date_h_3'] = new.date.apply(lambda x:1 if ((x >= 8) & (x < 12) )else 0)\n",
    "    new['date_h_4'] = new.date.apply(lambda x:1 if ((x >= 12) & (x < 16) )else 0)\n",
    "    new['date_h_5'] = new.date.apply(lambda x:1 if ((x >= 16) & (x < 20) )else 0)\n",
    "    new['date_h_6'] = new.date.apply(lambda x:1 if ((x >= 20) & (x < 24) )else 0)\n",
    "    data = pd.merge(data, new[['date_h_2','date_h_3','user_id']], on='user_id',how='left')\n",
    "    \n",
    "    data['register_time'] = pd.to_datetime(data['register_time'])\n",
    "    data['dow'] = data['register_time'].apply(lambda x:x.dayofweek)\n",
    "    data['doy'] = data['register_time'].apply(lambda x:x.dayofyear)\n",
    "    data['day'] = data['register_time'].apply(lambda x:x.day)\n",
    "    data['month'] = data['register_time'].apply(lambda x:x.month)\n",
    "    data['hour'] = data['register_time'].apply(lambda x:x.hour)\n",
    "    data['minute'] = data['register_time'].apply(lambda x:x.hour*60 + x.minute)\n",
    "    del data['register_time']\n",
    "    \n",
    "    #做一些比例的组合特征\n",
    "    #每次充钱的比例\n",
    "    data['pay_price_ave'] = data['pay_price'] / data['pay_count']\n",
    "    data['pay_price_ave'] = data['pay_price_ave'].fillna(0)\n",
    "    #副本赢得比例\n",
    "    data['pve_win_ave'] = data['pve_win_count'] / data['pve_battle_count']\n",
    "    data['pve_win_ave'] = data['pve_win_ave'].fillna(0)\n",
    "    #主动发起副本的次数比例\n",
    "    data['pve_lanch_ave'] = data['pve_lanch_count'] / data['pve_battle_count']\n",
    "    data['pve_lanch_ave'] = data['pve_lanch_ave'].fillna(0)\n",
    "    #人赢得比例\n",
    "    data['pvp_win_ave'] = data['pvp_win_count'] / data['pvp_battle_count']\n",
    "    data['pvp_win_ave'] = data['pvp_win_ave'].fillna(0)\n",
    "    #主动发起与人战争的次数比例\n",
    "    data['pvp_lanch_ave'] = data['pvp_lanch_count'] / data['pvp_battle_count']\n",
    "    data['pvp_lanch_ave'] = data['pvp_lanch_ave'].fillna(0)\n",
    "    \n",
    "        #在线时长和PVE\n",
    "    data['pve_battle_count_min'] = data['pve_battle_count'] / data['avg_online_minutes']\n",
    "    data.loc[data.avg_online_minutes == 0, 'pve_battle_count_min'] = 0\n",
    "    data['pve_battle_count_min'] = data['pve_battle_count_min'].fillna(0)\n",
    "    #在线时长和主动PVE\n",
    "    data['pve_lanch_count_min'] = data['pve_lanch_count'] / data['avg_online_minutes']\n",
    "    data.loc[data.avg_online_minutes == 0, 'pve_lanch_count_min'] = 0\n",
    "    data['pve_lanch_count_min'] = data['pve_lanch_count_min'].fillna(0)\n",
    "\n",
    "    #在线时长和PVp\n",
    "    data['pvp_battle_count_min'] = data['pvp_battle_count'] / data['avg_online_minutes']\n",
    "    data.loc[data.avg_online_minutes == 0, 'pvp_battle_count_min'] = 0\n",
    "    data['pvp_battle_count_min'] = data['pvp_battle_count_min'].fillna(0)\n",
    "    #在线时长和主动PVp\n",
    "    data['pvp_lanch_count_min'] = data['pvp_lanch_count'] / data['avg_online_minutes']\n",
    "    data.loc[data.avg_online_minutes == 0, 'pvp_lanch_count_min'] = 0\n",
    "    data['pvp_lanch_count_min'] = data['pvp_lanch_count_min'].fillna(0)\n",
    "    \n",
    "    def get_poly_fea(data):\n",
    "        fea_list = ['ivory_add_value', 'wood_add_value', 'stone_add_value', 'general_acceleration_add_value', 'ivory_reduce_value', 'meat_add_value', 'wood_reduce_value', \n",
    "                'training_acceleration_add_value']\n",
    "        for i in fea_list:\n",
    "#             data[i + str(i) + 'square'] = data[i] ** 2\n",
    "#             data[i + str(i) + 'cubic'] = data[i] ** 3\n",
    "            data[i + str(i) + 'sqrt'] = data[i] ** 0.5\n",
    "        return data\n",
    "    \n",
    "    data = get_poly_fea(data)\n",
    "    \n",
    "    \n",
    "    def add_SumZeros(data, features):\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        if 'SumZeros' in features:\n",
    "            data.insert(1, 'SumZeros', (data[flist] == 0).astype(int).sum(axis=1))\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        return data\n",
    "    def add_sumValues(data, features):\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        if 'SumValues' in features:\n",
    "            data.insert(1, 'SumValues', (data[flist] != 0).astype(int).sum(axis=1))\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        return data\n",
    "    data = add_SumZeros(data, ['SumZeros'])\n",
    "    data = add_sumValues(data, ['SumValues'])\n",
    "\n",
    "    def km(data):\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        flist_kmeans = []\n",
    "        for ncl in range(5,15):\n",
    "            cls = KMeans(n_clusters=ncl)\n",
    "            cls.fit_predict(normalize(data[flist].values, axis=0))\n",
    "            data['kmeans_cluster_'+str(ncl)] = cls.predict(normalize(data[flist].values, axis=0))\n",
    "            flist_kmeans.append('kmeans_cluster_'+str(ncl))\n",
    "        print(flist_kmeans)\n",
    "        return data\n",
    "    data = km(data)\n",
    "    return data\n",
    "\n",
    "data = process(data)#数据处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "484b5e7a4043363488403db7725529d0b23e7497",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(data):\n",
    "    train = data[data.prediction_pay_price != -1]\n",
    "    test = data[data.prediction_pay_price == -1]\n",
    "    y = train['prediction_pay_price'].values.astype(np.float32)    \n",
    "    numeric_feats = data.drop(['prediction_pay_price', 'user_id'],axis=0).dtypes[data.drop(['prediction_pay_price', 'user_id'],axis=0).dtypes != \"object\"].index\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "#     skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "    print(skewed_feats.shape)\n",
    "    skewed_feats = skewed_feats.index\n",
    "    train[skewed_feats] = np.log1p(train[skewed_feats])\n",
    "    test[skewed_feats] = np.log1p(test[skewed_feats])\n",
    "    test_usid = test.user_id\n",
    "    del test['user_id']\n",
    "    del test['prediction_pay_price']\n",
    "    test_X = test.values.astype(np.float32)\n",
    "    del train['user_id']\n",
    "    del train['prediction_pay_price']\n",
    "    X = train.values.astype(np.float32)\n",
    "    col = train.columns\n",
    "    return X, y, test_X, train, test, col, test_usid\n",
    "\t\n",
    "# 分成星期几来做\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, ElasticNetCV, RidgeCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "test_with_pay = pd.DataFrame()\n",
    "test_with_pay['user_id'] = data[data.prediction_pay_price == -1].user_id\n",
    "\n",
    "test_with_pay['prediction_pay_price'] = 0\n",
    "\n",
    "flist = [5.98, 6.97, 10.98, 16.96, 11.97, 9.99, 26.95, 21.95, 3.96, 11.96]\n",
    "for i in list(data.pay_price.value_counts().index):\n",
    "    if (len(data[data.pay_price == i]) > 300) and (i not in flist):\n",
    "        X, y, test_X, train, test, col, test_with_pay_usid  = split(data[data.pay_price == i])\n",
    "        y = np.log1p(y)\n",
    "\n",
    "        #gbdt加入新特征进去\n",
    "        clf = xgb.XGBRegressor(\n",
    "            n_estimators=30,#三十棵树\n",
    "            learning_rate =0.1,\n",
    "            max_depth=3,\n",
    "            min_child_weight=1,\n",
    "            gamma=0.3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=1,\n",
    "            reg_lambda=1,\n",
    "            seed=27)\n",
    "        model_sklearn=clf.fit(X, y)\n",
    "        y_sklearn= clf.predict(test_X)\n",
    "        print('max:',np.expm1(y_sklearn).max())\n",
    "        train_new_feature= clf.apply(X)#每个样本在每颗树叶子节点的索引值\n",
    "        test_new_feature= clf.apply(test_X)\n",
    "        X = train_new_feature.copy()\n",
    "        test_X = test_new_feature.copy()\n",
    "\n",
    "        from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, ElasticNetCV, RidgeCV, LassoLarsCV, BayesianRidge\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        def rmse_cv(model):\n",
    "            rmse= np.sqrt(-cross_val_score(model, X, np.expm1(y), scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "            return(rmse)\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsRegressor\n",
    "        knn = KNeighborsRegressor(n_neighbors=2, weights='distance')\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        ss = StandardScaler()\n",
    "\n",
    "        knn.fit(X, np.expm1(y))\n",
    "\n",
    "        print('pay_price of i',i, 'rmse:', rmse_cv(knn).mean())\n",
    "        lasso_preds = knn.predict(test_X)\n",
    "        test_with_pay.loc[test_with_pay.user_id.isin(test_with_pay_usid), 'prediction_pay_price'] = lasso_preds\n",
    "\n",
    "save_usid = []\n",
    "for i in list(data.pay_price.value_counts().index):\n",
    "    if (len(data[data.pay_price == i]) <= 300) or (i in flist):\n",
    "        save_usid += (data[data.pay_price == i].user_id.tolist())\n",
    "\t\t\n",
    "X, y, test_X, train, test, col, test_with_pay_usid  = split(data[data.user_id.isin(save_usid)])\n",
    "y = np.log1p(y)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, ElasticNetCV, RidgeCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "xgb_regressor = xgb.XGBRegressor()\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.005,0.003,  0.001, 0.0005, 0.0001])\n",
    "sfm = SelectFromModel(model_lasso)\n",
    "sfm.fit(X, y)\n",
    "X = sfm.transform(X)\n",
    "test_X = sfm.transform(test_X)\n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, ElasticNetCV, RidgeCV, LassoLarsCV, BayesianRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.005,0.003,  0.001, 0.0005, 0.0001])\n",
    "model_lasso = make_pipeline(RobustScaler(), model_lasso).fit(X, y)\n",
    "lasso_preds = np.expm1(model_lasso.predict(test_X))\n",
    "\n",
    "\n",
    "test_with_pay.loc[test_with_pay.user_id.isin(test_with_pay_usid), 'prediction_pay_price'] = lasso_preds\n",
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = test_usid.values\n",
    "sub['prediction_pay_price'] = 0\n",
    "sub.loc[sub.user_id.isin(test_with_pay.user_id), 'prediction_pay_price'] = test_with_pay['prediction_pay_price'].values * 1.432\n",
    "print(sub.head(), '\\n')\n",
    "print(sub.describe())\n",
    "sub.to_csv('knn_lasso.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f33cb19d953fb55e88dd00a43158814abdbaa98",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_with_pay[test_with_pay.prediction_pay_price != 0].sort_values('prediction_pay_price', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a7853c5487e7ddfe471ea6f2c415220184efef9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "943ab8052d0fea483ee49752fda8addd36f52655",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

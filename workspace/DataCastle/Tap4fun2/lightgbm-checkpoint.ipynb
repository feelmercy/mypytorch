{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ff01fd236115e52fe2f1126ad8963b3542026ce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "269d955bf0a4663da9145037a5b52723f2af6bb1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.model_selection._split import check_cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.model_selection._split import check_cv\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1b17158e90a6616ea9129f0d722d128bae2b97e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/tap_fun_train.csv')\n",
    "test_df = pd.read_csv('../input/tap_fun_test.csv')\n",
    "\n",
    "test_df[\"prediction_pay_price\"] = -1\n",
    "test_usid = test_df.user_id\n",
    "data = pd.concat([train_df, test_df])\n",
    "del train_df, test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c922276ff0c14ff631cc05adc604487bb2ee6924",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "\n",
    "gl_int = data.select_dtypes(include=['int'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "\n",
    "gl_float = data.select_dtypes(include=['float'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_float.apply(pd.to_numeric,downcast='float')\n",
    "\n",
    "del gl_int, gl_float\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a70dd2bae9c15df7bede05df8b18bf8af6bc7008",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(data):\n",
    "    import time\n",
    "    a = data['register_time'].apply(lambda x:time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    a /= 3600\n",
    "    data['regedit_diff'] = (a - min(a))\n",
    "    \n",
    "    #\n",
    "    new = data[['prediction_pay_price','user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "    new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    new['date_week'] = new.date.apply(lambda x:1 if x in ['27','28','03', '04','10','11','17','18','24','25'] else 0)\n",
    "    data = pd.merge(data, new[['date_week', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    #\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "    new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    new['date_holiday'] = new.date.apply(lambda x:1 if x in ['14','15','16'] else 0)\n",
    "    data = pd.merge(data, new[['date_holiday', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    #\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[1])\n",
    "    new['date'] = new['date'].apply(lambda x:int(x.split(':')[0]))\n",
    "#     new['date_h_1'] = new.date.apply(lambda x:1 if ((x >= 0) & (x < 4) )else 0)\n",
    "    new['date_h_2'] = new.date.apply(lambda x:1 if ((x >= 4) & (x < 8) )else 0)\n",
    "    new['date_h_3'] = new.date.apply(lambda x:1 if ((x >= 8) & (x < 12) )else 0)\n",
    "#     new['date_h_4'] = new.date.apply(lambda x:1 if ((x >= 12) & (x < 16) )else 0)\n",
    "#     new['date_h_5'] = new.date.apply(lambda x:1 if ((x >= 16) & (x < 20) )else 0)\n",
    "#     new['date_h_6'] = new.date.apply(lambda x:1 if ((x >= 20) & (x < 24) )else 0)\n",
    "    data = pd.merge(data, new[['date_h_2','date_h_3','user_id']], on='user_id',how='left')\n",
    "    del data['register_time']\n",
    "    \n",
    "    #做一些比例的组合特征\n",
    "    #每次充钱的比例\n",
    "    data['pay_price_ave'] = data['pay_price'] / data['pay_count']\n",
    "    data['pay_price_ave'] = data['pay_price_ave'].fillna(0)\n",
    "    #副本赢得比例\n",
    "    data['pve_win_ave'] = data['pve_win_count'] / data['pve_battle_count']\n",
    "    data['pve_win_ave'] = data['pve_win_ave'].fillna(0)\n",
    "    #主动发起副本的次数比例\n",
    "    data['pve_lanch_ave'] = data['pve_lanch_count'] / data['pve_battle_count']\n",
    "    data['pve_lanch_ave'] = data['pve_lanch_ave'].fillna(0)\n",
    "    #人赢得比例\n",
    "    data['pvp_win_ave'] = data['pvp_win_count'] / data['pvp_battle_count']\n",
    "    data['pvp_win_ave'] = data['pvp_win_ave'].fillna(0)\n",
    "    #主动发起与人战争的次数比例\n",
    "    data['pvp_lanch_ave'] = data['pvp_lanch_count'] / data['pvp_battle_count']\n",
    "    data['pvp_lanch_ave'] = data['pvp_lanch_ave'].fillna(0)\n",
    "    \n",
    "    def add_SumZeros(data, features):\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        if 'SumZeros' in features:\n",
    "            data.insert(1, 'SumZeros', (data[flist] == 0).astype(int).sum(axis=1))\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        return data\n",
    "    def add_sumValues(data, features):\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        if 'SumValues' in features:\n",
    "            data.insert(1, 'SumValues', (data[flist] != 0).astype(int).sum(axis=1))\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        return data\n",
    "    data = add_SumZeros(data, ['SumZeros'])\n",
    "    data = add_sumValues(data, ['SumValues'])\n",
    "    def km(data):\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        flist_kmeans = []\n",
    "        for ncl in range(2,11):\n",
    "            cls = KMeans(n_clusters=ncl)\n",
    "            cls.fit_predict(data[flist].values)\n",
    "            data['kmeans_cluster_'+str(ncl)] = cls.predict(data[flist].values)\n",
    "            flist_kmeans.append('kmeans_cluster_'+str(ncl))\n",
    "        print(flist_kmeans)\n",
    "        return data\n",
    "    data = km(data)\n",
    "    def pca_pro(data):\n",
    "        flist = [x for x in data.columns if not x in ['user_id','prediction_pay_price']]\n",
    "        n_components = 20\n",
    "        flist_pca = []\n",
    "        pca = PCA(n_components=n_components)\n",
    "        x_train_projected = pca.fit_transform(normalize(data[flist], axis=0))\n",
    "        for npca in range(0, n_components):\n",
    "            data.insert(1, 'PCA_'+str(npca+1), x_train_projected[:, npca])\n",
    "            flist_pca.append('PCA_'+str(npca+1))\n",
    "        print(flist_pca)\n",
    "        return data\n",
    "    data = pca_pro(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65248fd2fd3a2ab95219689db2b0505f10abec25",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0a45fb978f869a2303ab0d12628dabb37cd2598",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_with_payPrice(data):  #一步步的加入特征试试效果\n",
    "    train = data.loc[data.prediction_pay_price != -1]\n",
    "    test = data[data.prediction_pay_price == -1]\n",
    "\n",
    "    test_with_pay_usid = test[test.pay_price != 0].user_id\n",
    "    test_usid = data[data.prediction_pay_price == -1].user_id\n",
    "    del test['user_id']\n",
    "    del test['prediction_pay_price']\n",
    "    test_X = test[test.pay_price != 0].values.astype(np.float32)\n",
    "\n",
    "    del train['user_id']\n",
    "    # y = np.log1p(train['prediction_pay_price'].values)\n",
    "    y = train[train.pay_price != 0]['prediction_pay_price'].values.astype(np.float32)\n",
    "    del train['prediction_pay_price']\n",
    "    X = train[train.pay_price != 0].values.astype(np.float32)\n",
    "    col = train.columns\n",
    "\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    return X, y, test_X, test_with_pay_usid, test_usid, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52cf177536cd1fda7059bc6f6d46e0e4d87d15c1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, test_X, test_with_pay_usid, test_usid, col = split_with_payPrice(data)\n",
    "y = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33fcf8813d129638d20219e09a8898f543dafcd1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_train(space, X_train, y_train, X_test):\n",
    "    lgb_params ={'task':'train', 'boosting_type':'gbdt', 'objective':'regression', 'metric': {'rmse'},\n",
    "                 'num_leaves': space['num_leaves'], 'learning_rate': space['learning_rate'], 'max_bin': space['max_bin'], \n",
    "                 'max_depth': space['max_depth'], 'min_child_samples':space['min_child_samples'], 'subsample': space['subsample'],\n",
    "                 'colsample_bytree': space['colsample_bytree'], 'nthread':4, 'verbose': 0}\n",
    "    lgbtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    lgbtrain.construct()\n",
    "    lgb_model = lgb.train(lgb_params, lgbtrain, num_boost_round=space['num_boost_round'])\n",
    "    preds = lgb_model.predict(X_test, num_iteration=space['num_boost_round'])\n",
    "    return lgb_model, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0edd96dfb34a915199a8ee7ddced58eb35056c32",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_best_params = {'colsample_bytree': 0.8548944462148844, 'learning_rate': 0.1, 'max_bin': 428, 'max_depth': 10, \\\n",
    "'min_child_samples': 8, 'num_boost_round': 538, 'num_leaves': 53, 'subsample': 0.9111911754599543}\n",
    "\n",
    "lgb_model, preds = lgb_train(lgb_best_params, X, y, test_X)\n",
    "test_with_pay = pd.DataFrame()\n",
    "test_with_pay['user_id'] = test_with_pay_usid\n",
    "preds[preds < 0] = 0\n",
    "test_with_pay['prediction_pay_price'] = np.expm1(preds) * 1.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ededb0f661417f9510ed8feb164d6cb0676eeea7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = test_usid\n",
    "# sub['prediction_pay_price'] = preds\n",
    "sub['prediction_pay_price'] = 0\n",
    "sub.loc[sub.user_id.isin(test_with_pay.user_id), 'prediction_pay_price'] = test_with_pay['prediction_pay_price']\n",
    "\n",
    "print(sub.head(), '\\n')\n",
    "print(sub.describe())\n",
    "sub.to_csv('first_hyperopt_model_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d66e9e716ac77c835dce14466fe376035970d450",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

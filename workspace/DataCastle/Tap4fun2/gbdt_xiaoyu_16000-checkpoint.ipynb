{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T07:30:05.856542Z",
     "start_time": "2018-09-20T07:29:30.319911Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6e0b90a9136a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"prediction_pay_price\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mtest_usid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5420\u001b[0m             b = make_block(\n\u001b[1;32m-> 5421\u001b[1;33m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5422\u001b[0m                 placement=placement)\n\u001b[0;32m   5423\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5563\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[0;32m   5564\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[1;32m-> 5565\u001b[1;33m                  for ju in join_units]\n\u001b[0m\u001b[0;32m   5566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5567\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   5563\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[0;32m   5564\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[1;32m-> 5565\u001b[1;33m                  for ju in join_units]\n\u001b[0m\u001b[0;32m   5566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5567\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m   5873\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5874\u001b[0m                 values = algos.take_nd(values, indexer, axis=ax,\n\u001b[1;32m-> 5875\u001b[1;33m                                        fill_value=fill_value)\n\u001b[0m\u001b[0;32m   5876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5877\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[0;32m   1650\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv('F:\\\\study\\\\ml\\\\DataSet\\\\Tap4fun\\\\tap_fun_train.csv')\n",
    "test_df = pd.read_csv('F:\\\\study\\\\ml\\\\DataSet\\\\Tap4fun\\\\tap_fun_test.csv')\n",
    "test_df[\"prediction_pay_price\"] = -1\n",
    "test_usid = test_df.user_id\n",
    "data = pd.concat([train_df, test_df])\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "# We're going to be calculating memory usage a lot,\n",
    "# so we'll create a function to save us some time!\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "gl_int = data.select_dtypes(include=['int'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "gl_float = data.select_dtypes(include=['float'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_float.apply(pd.to_numeric,downcast='float')\n",
    "del gl_int, gl_float\n",
    "gc.collect()\n",
    "train_df = data[data.prediction_pay_price != -1]\n",
    "test_df = data[data.prediction_pay_price == -1]\n",
    "test_usid = test_df.user_id\n",
    "\n",
    "data = data[data.pay_price != 0]#取pay_price不为0的用户\n",
    "\n",
    "def process(data):\n",
    "    data['register_hour'] = data['register_time'].map(lambda x : int(x[11:13]))\n",
    "    data['register_time_day'] = data['register_time'].map(lambda x : x[5:10])\n",
    "\n",
    "    data.loc[:,'is_pay_price45'] = data['pay_price'].map(lambda x: 1 if x>0 else 0)\n",
    "    data.loc[:,'is_pay_099'] = data['pay_price'].map(lambda x: 1 if x<1 else 0)\n",
    "\n",
    "    have_pay_price_mean = data.groupby(['register_time_day'])['is_pay_price45'].mean()\n",
    "    have_pay_099_mean = data.groupby(['register_time_day'])['is_pay_099'].mean()\n",
    "    pay_099_ration = data.loc[data['is_pay_price45']>0,:].copy()\n",
    "    pay_099_ration = pay_099_ration.groupby(['register_time_day'])['is_pay_099'].mean()\n",
    "    data['have_pay_price_mean_hour'] = data['register_hour'].map(lambda x : have_pay_price_mean[x])\n",
    "    data['have_pay_099_mean_hour'] = data['register_hour'].map(lambda x : have_pay_099_mean[x])\n",
    "    data['pay_099_ration_hour'] = data['register_hour'].map(lambda x : pay_099_ration[x])\n",
    "    del data['register_hour']\n",
    "    del data['register_time_day']\n",
    "    \n",
    "    import time\n",
    "    a = data['register_time'].apply(lambda x:time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    a /= (3600 * 24)  #这里试试天数会不会更有效果\n",
    "    data['regedit_diff_day'] = (a - min(a))\n",
    "    \n",
    "     #这个特征是顾客在3月双休日点击APP的总次数\n",
    "    new = data[['prediction_pay_price','user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-02-02', '2018-02-09', '2018-02-16', '2018-02-23', '2018-03-02','2018-03-09','2018-03-16']\n",
    "    new['date_week'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_week', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    new = data[['prediction_pay_price','user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-02-13', '2018-03-16']\n",
    "    new['date_week_two'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_week_two', 'user_id']], on='user_id',how='left')\n",
    "\n",
    "    \n",
    "    #这个特征是顾客在3月双休日点击APP的总次数\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-03-10', '2018-02-19']\n",
    "    new['date_holiday'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_holiday', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    #这个特征是顾客在3月双休日点击APP的总次数\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[1])\n",
    "    new['date'] = new['date'].apply(lambda x:int(x.split(':')[0]))\n",
    "    new['date_h_1'] = new.date.apply(lambda x:1 if ((x >= 0) & (x < 4) )else 0)\n",
    "    new['date_h_2'] = new.date.apply(lambda x:1 if ((x >= 4) & (x < 8) )else 0)\n",
    "    new['date_h_3'] = new.date.apply(lambda x:1 if ((x >= 8) & (x < 12) )else 0)\n",
    "    new['date_h_4'] = new.date.apply(lambda x:1 if ((x >= 12) & (x < 16) )else 0)\n",
    "    new['date_h_5'] = new.date.apply(lambda x:1 if ((x >= 16) & (x < 20) )else 0)\n",
    "    new['date_h_6'] = new.date.apply(lambda x:1 if ((x >= 20) & (x < 24) )else 0)\n",
    "    data = pd.merge(data, new[['date_h_2','date_h_3','user_id']], on='user_id',how='left')\n",
    "    \n",
    "    data['register_time'] = pd.to_datetime(data['register_time'])\n",
    "    data['dow'] = data['register_time'].apply(lambda x:x.dayofweek)\n",
    "    data['doy'] = data['register_time'].apply(lambda x:x.dayofyear)\n",
    "#     data['day'] = data['register_time'].apply(lambda x:x.day)\n",
    "    data['month'] = data['register_time'].apply(lambda x:x.month)\n",
    "    data['hour'] = data['register_time'].apply(lambda x:x.hour)\n",
    "    data['minute'] = data['register_time'].apply(lambda x:x.hour*60 + x.minute)\n",
    "    for i in ['dow', 'doy', 'month']:\n",
    "        a = pd.get_dummies(data[i], prefix = i)\n",
    "        data = pd.concat([data, a], axis = 1)\n",
    "        del data[i]\n",
    "        \n",
    "    def get_poly_fea(data):\n",
    "        fea_list = ['ivory_add_value', 'wood_add_value', 'stone_add_value', 'general_acceleration_add_value', 'ivory_reduce_value', 'meat_add_value', 'wood_reduce_value', \n",
    "                'training_acceleration_add_value']\n",
    "        for i in fea_list:\n",
    "    #             data[i + str(i) + 'square'] = data[i] ** 2\n",
    "    #             data[i + str(i) + 'cubic'] = data[i] ** 3\n",
    "            data[i + str(i) + 'sqrt'] = data[i] ** 0.5\n",
    "        return data\n",
    "\n",
    "    data = get_poly_fea(data)\n",
    "\n",
    "    del data['register_time']\n",
    "    \n",
    "    data.loc[:,'wood_reduce_ratio'] = data['wood_reduce_value'] / (data['wood_add_value']+1e-4)\n",
    "    data.loc[:,'stone_reduce_ratio'] = data['stone_reduce_value'] / (data['stone_add_value']+1e-4)\n",
    "    data.loc[:,'ivory_reduce_ratio'] = data['ivory_reduce_value'] / (data['ivory_add_value']+1e-4)\n",
    "    data.loc[:,'meat_reduce_ratio'] = data['meat_reduce_value'] / (data['meat_add_value']+1e-4)\n",
    "    data.loc[:,'magic_reduce_ratio'] = data['magic_reduce_value'] / (data['magic_add_value']+1e-4)\n",
    "    data.loc[:,'infantry_reduce_ratio'] = data['infantry_reduce_value'] / (data['infantry_add_value']+1e-4)\n",
    "    data.loc[:,'cavalry_reduce_ratio'] = data['cavalry_reduce_value'] / (data['cavalry_add_value']+1e-4)\n",
    "    data.loc[:,'shaman_reduce_ratio'] = data['shaman_reduce_value'] / (data['shaman_add_value']+1e-4)\n",
    "    data.loc[:,'wound_infantry_reduce_ratio'] = data['wound_infantry_reduce_value'] / (data['wound_infantry_add_value']+1e-4)\n",
    "    data.loc[:,'wound_cavalry_reduce_ratio'] = data['wound_cavalry_reduce_value'] / (data['wound_cavalry_add_value']+1e-4)\n",
    "    data.loc[:,'wound_shaman_reduce_ratio'] = data['wound_shaman_reduce_value'] / (data['wound_shaman_add_value']+1e-4)\n",
    "    data.loc[:,'general_acceleration_reduce_ratio'] = data['general_acceleration_reduce_value'] / (data['general_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'building_acceleration_reduce_ratio'] = data['building_acceleration_reduce_value'] / (data['building_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'reaserch_acceleration_reduce_ratio'] = data['reaserch_acceleration_reduce_value'] / (data['reaserch_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'training_acceleration_reduce_ratio'] = data['training_acceleration_reduce_value'] / (data['training_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'treatment_acceleraion_reduce_ratio'] = data['treatment_acceleration_reduce_value'] / (data['treatment_acceleraion_add_value']+1e-4)\n",
    "    data.loc[:,'wood_add_sub_reduce'] = np.abs(data['wood_add_value'] - data['wood_reduce_value'])\n",
    "    data.loc[:,'stone_add_sub_reduce'] = np.abs(data['stone_add_value'] - data['stone_reduce_value'])\n",
    "    data.loc[:,'ivory_add_sub_reduce'] = np.abs(data['ivory_add_value'] - data['ivory_reduce_value'])\n",
    "    data.loc[:,'meat_add_sub_reduce'] = np.abs(data['meat_add_value'] - data['meat_reduce_value'])\n",
    "    data.loc[:,'magic_add_sub_reduce'] = np.abs(data['magic_add_value'] - data['magic_reduce_value'])\n",
    "    data.loc[:,'infantry_add_sub_reduce'] = np.abs(data['infantry_add_value'] - data['infantry_reduce_value'])\n",
    "    data.loc[:,'cavalry_add_sub_reduce'] = np.abs(data['cavalry_add_value'] - data['cavalry_reduce_value'])\n",
    "    data.loc[:,'shaman_add_sub_reduce'] = np.abs(data['shaman_add_value'] - data['shaman_reduce_value'])\n",
    "    data.loc[:,'wound_infantry_add_sub_reduce'] = np.abs(data['wound_infantry_add_value'] - data['wound_infantry_reduce_value'])\n",
    "    data.loc[:,'wound_cavalry_add_sub_reduce'] = np.abs(data['wound_cavalry_add_value'] - data['wound_cavalry_reduce_value'])\n",
    "    data.loc[:,'wound_shaman_add_sub_reduce'] = np.abs(data['wound_shaman_add_value'] - data['wound_shaman_reduce_value'])\n",
    "    data.loc[:,'general_acceleration_add_sub_reduce'] = np.abs(data['general_acceleration_add_value'] - data['general_acceleration_reduce_value'])\n",
    "    data.loc[:,'building_acceleration_add_sub_reduce'] = np.abs(data['building_acceleration_add_value'] - data['building_acceleration_reduce_value'])\n",
    "    data.loc[:,'reaserch_acceleration_add_sub_reduce'] = np.abs(data['reaserch_acceleration_add_value'] - data['reaserch_acceleration_reduce_value'])\n",
    "    data.loc[:,'training_acceleration_add_sub_reduce'] = np.abs(data['training_acceleration_add_value'] - data['training_acceleration_reduce_value'])\n",
    "    data.loc[:,'treatment_acceleration_add_sub_reduce'] = np.abs(data['treatment_acceleraion_add_value'] - data['treatment_acceleration_reduce_value'])\n",
    "    log_col = ['wood_add_value','wood_reduce_value','stone_add_value','stone_reduce_value','ivory_add_value',\n",
    "                'ivory_reduce_value','meat_add_value','meat_reduce_value','magic_add_value','magic_reduce_value',\n",
    "                'infantry_add_value','infantry_reduce_value','cavalry_add_value','cavalry_reduce_value','shaman_add_value',\n",
    "                'shaman_reduce_value','wound_infantry_add_value','wound_infantry_reduce_value','wound_cavalry_add_value',\n",
    "                'wound_cavalry_reduce_value','wound_shaman_add_value','wound_shaman_reduce_value',\n",
    "                'general_acceleration_add_value','general_acceleration_reduce_value','building_acceleration_add_value',\n",
    "                'building_acceleration_reduce_value','reaserch_acceleration_add_value','reaserch_acceleration_reduce_value',\n",
    "                'training_acceleration_add_value','training_acceleration_reduce_value','treatment_acceleraion_add_value',\n",
    "                'treatment_acceleration_reduce_value']\n",
    "    for col in log_col:\n",
    "        data[col] = data[col].map(lambda x : np.log1p(x))\n",
    "    # 物资消耗统计\n",
    "    ratio_col = ['wood_reduce_ratio','stone_reduce_ratio','ivory_reduce_ratio','meat_reduce_ratio','magic_reduce_ratio',\\\n",
    "                'infantry_reduce_ratio','cavalry_reduce_ratio','shaman_reduce_ratio','wound_infantry_reduce_ratio',\\\n",
    "                'wound_cavalry_reduce_ratio','wound_shaman_reduce_ratio','general_acceleration_reduce_ratio',\\\n",
    "                'building_acceleration_reduce_ratio','reaserch_acceleration_reduce_ratio','training_acceleration_reduce_ratio',\\\n",
    "                'treatment_acceleraion_reduce_ratio']\n",
    "    data.loc[:,'ratio_max'] = data[ratio_col].max(1)\n",
    "    data.loc[:,'ratio_min'] = data[ratio_col].min(1)\n",
    "    data.loc[:,'ratio_mean'] = data[ratio_col].mean(1)\n",
    "    data.loc[:,'ratio_sum'] = data[ratio_col].sum(1)\n",
    "    data.loc[:,'ratio_std'] = data[ratio_col].std(1)\n",
    "    data.loc[:,'ratio_median'] = data[ratio_col].median(1)\n",
    "    # data.loc[:,'ratio_mode'] = data[ratio_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "    # 物资生产统计\n",
    "    add_col = ['wood_add_value','stone_add_value','ivory_add_value','meat_add_value','magic_add_value',\\\n",
    "                'infantry_add_value','cavalry_add_value','shaman_add_value','wound_infantry_add_value',\\\n",
    "                'wound_cavalry_add_value','wound_shaman_add_value','general_acceleration_add_value',\\\n",
    "                'building_acceleration_add_value','reaserch_acceleration_add_value','training_acceleration_add_value',\\\n",
    "                'treatment_acceleraion_add_value']\n",
    "    data.loc[:,'add_max'] = data[add_col].max(1)\n",
    "    data.loc[:,'add_min'] = data[add_col].min(1)\n",
    "    data.loc[:,'add_mean'] = data[add_col].mean(1)\n",
    "    data.loc[:,'add_sum'] = data[add_col].sum(1)\n",
    "    data.loc[:,'add_std'] = data[add_col].std(1)\n",
    "    data.loc[:,'add_median'] = data[add_col].median(1)\n",
    "    # data.loc[:,'add_mode'] = data[add_col].mode(1)\n",
    "    \n",
    "    reduce_col = ['wood_reduce_value','stone_reduce_value','ivory_reduce_value','meat_reduce_value','magic_reduce_value',\\\n",
    "                'infantry_reduce_value','cavalry_reduce_value','shaman_reduce_value','wound_infantry_reduce_value',\\\n",
    "                'wound_cavalry_reduce_value','wound_shaman_reduce_value','general_acceleration_add_value',\\\n",
    "                'building_acceleration_reduce_value','reaserch_acceleration_reduce_value','training_acceleration_reduce_value',\\\n",
    "                'treatment_acceleration_reduce_value']\n",
    "    data.loc[:,'reduce_max'] = data[reduce_col].max(1)\n",
    "    data.loc[:,'reduce_min'] = data[reduce_col].min(1)\n",
    "    data.loc[:,'reduce_mean'] = data[reduce_col].mean(1)\n",
    "    data.loc[:,'reduce_sum'] = data[reduce_col].sum(1)\n",
    "    data.loc[:,'reduce_std'] = data[reduce_col].std(1)\n",
    "    data.loc[:,'reduce_median'] = data[reduce_col].median(1)\n",
    "    # data.loc[:,'reduce_mode'] = data[reduce_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "    # 建筑等级统计\n",
    "    bd_col = ['bd_training_hut_level','bd_healing_lodge_level','bd_stronghold_level','bd_outpost_portal_level',\n",
    "            'bd_barrack_level','bd_healing_spring_level','bd_dolmen_level','bd_guest_cavern_level','bd_warehouse_level',\n",
    "            'bd_watchtower_level','bd_magic_coin_tree_level','bd_hall_of_war_level','bd_market_level','bd_hero_gacha_level',\n",
    "            'bd_hero_strengthen_level','bd_hero_pve_level']\n",
    "    data.loc[:,'bd_max'] = data[bd_col].max(1)\n",
    "    data.loc[:,'bd_min'] = data[bd_col].min(1)\n",
    "    data.loc[:,'bd_mean'] = data[bd_col].mean(1)\n",
    "    data.loc[:,'bd_sum'] = data[bd_col].sum(1)\n",
    "    data.loc[:,'bd_std'] = data[bd_col].std(1)\n",
    "    data.loc[:,'bd_median'] = data[bd_col].median(1)\n",
    "    # data.loc[:,'bd_mode'] = data[bd_col].mode(1)\n",
    "    \n",
    "    # 科研 tier 统计\n",
    "    tier_col = ['sr_infantry_tier_2_level','sr_cavalry_tier_2_level','sr_shaman_tier_2_level',\n",
    "                'sr_infantry_tier_3_level','sr_cavalry_tier_3_level','sr_shaman_tier_3_level',\n",
    "                'sr_infantry_tier_4_level','sr_cavalry_tier_4_level','sr_shaman_tier_4_level']\n",
    "\n",
    "    data.loc[:,'infantry_tier_sum'] = data[[tier_col[0],tier_col[3],tier_col[6]]].sum(1)\n",
    "    data.loc[:,'cavalry_tier_sum'] = data[[tier_col[1],tier_col[4],tier_col[7]]].sum(1)\n",
    "    data.loc[:,'shaman_tier_sum'] = data[[tier_col[2],tier_col[5],tier_col[8]]].sum(1)\n",
    "\n",
    "\n",
    "    data.loc[:,'sr_tier_sum'] = data[tier_col].sum(1)\n",
    "    data.loc[:,'sr_tier_max'] = data[tier_col].max(1)\n",
    "    data.loc[:,'sr_tier_min'] = data[tier_col].min(1)\n",
    "    data.loc[:,'sr_tier_mean'] = data[tier_col].mean(1)\n",
    "    data.loc[:,'sr_tier_std'] = data[tier_col].std(1)\n",
    "    data.loc[:,'sr_tier_median'] = data[tier_col].median(1)\n",
    "    # data.loc[:,'sr_tier_mode'] = data[tier_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "    # 攻击\n",
    "    atk_col = ['sr_infantry_atk_level','sr_cavalry_atk_level','sr_shaman_atk_level','sr_troop_attack_level']\n",
    "    data.loc[:,'atk_sum'] = data[atk_col].sum(1)\n",
    "    data.loc[:,'atk_mean'] = data[atk_col].mean(1)\n",
    "    data.loc[:,'atk_max'] = data[atk_col].max(1)\n",
    "    data.loc[:,'atk_std'] = data[atk_col].std(1)\n",
    "    data.loc[:,'atk_min'] = data[atk_col].min(1)\n",
    "    data.loc[:,'atk_median'] = data[atk_col].median(1)\n",
    "    # data.loc[:,'atk_mode'] = data[atk_col].mode(1)\n",
    "    \n",
    "     # 防御\n",
    "    def_col = ['sr_infantry_def_level','sr_cavalry_def_level','sr_shaman_def_level','sr_troop_defense_level']\n",
    "    data.loc[:,'def_sum'] = data[def_col].sum(1)\n",
    "    data.loc[:,'def_mean'] = data[def_col].mean(1)\n",
    "    data.loc[:,'def_max'] = data[def_col].max(1)\n",
    "    data.loc[:,'def_min'] = data[def_col].min(1)\n",
    "    data.loc[:,'def_std'] = data[def_col].std(1)\n",
    "    # data.loc[:,'def_mode'] = data[def_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 生命力\n",
    "    hp_col = ['sr_infantry_hp_level','sr_cavalry_hp_level','sr_shaman_hp_level']\n",
    "    data.loc[:,'hp_sum'] = data[hp_col].sum(1)\n",
    "    data.loc[:,'hp_mean'] = data[hp_col].mean(1)\n",
    "    data.loc[:,'hp_max'] = data[hp_col].max(1)\n",
    "    # data.loc[:,'hp_mode'] = data[hp_col].mode(1)\n",
    "    \n",
    "    # 各种 level 统计\n",
    "    level_col = ['sr_construction_speed_level','sr_hide_storage_level','sr_troop_consumption_level',\n",
    "                'sr_rss_a_prod_levell','sr_rss_b_prod_level','sr_rss_c_prod_level',\n",
    "                'sr_rss_d_prod_level','sr_rss_a_gather_level','sr_rss_b_gather_level',\n",
    "                'sr_rss_c_gather_level','sr_rss_d_gather_level','sr_troop_load_level','sr_rss_e_gather_level',\n",
    "                'sr_rss_e_prod_level','sr_outpost_durability_level','sr_outpost_tier_2_level',\n",
    "                'sr_healing_space_level','sr_gathering_hunter_buff_level','sr_healing_speed_level',\n",
    "                'sr_outpost_tier_3_level','sr_alliance_march_speed_level','sr_pvp_march_speed_level',\n",
    "                'sr_gathering_march_speed_level','sr_outpost_tier_4_level','sr_guest_troop_capacity_level',\n",
    "                'sr_march_size_level','sr_rss_help_bonus_level',]\n",
    "    data.loc[:,'same_level_sum'] = data[level_col].sum(1)\n",
    "    data.loc[:,'same_level_mean'] = data[level_col].mean(1)\n",
    "    data.loc[:,'same_level_max'] = data[level_col].max(1)\n",
    "    data.loc[:,'same_level_std'] = data[level_col].std(1)\n",
    "    data.loc[:,'same_level_min'] = data[level_col].min(1)\n",
    "    data.loc[:,'same_level_median'] = data[level_col].median(1)\n",
    "    # data.loc[:,'same_level_mode'] = data[level_col].mode(1)\n",
    "    \n",
    "     # pvp\n",
    "\n",
    "    data.loc[:,'pvp_lanch_ratio'] = data['pvp_lanch_count'] / (data['pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pvp_win_ratio'] = data['pvp_win_count'] / (data['pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pvp_win_lanch_ratio'] = data['pvp_win_count'] / (data['pvp_lanch_count'] + 1e-4)\n",
    "\n",
    "    # pve\n",
    "    data.loc[:,'pve_lanch_ratio'] = data['pve_lanch_count'] / (data['pve_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_win_ratio'] = data['pve_win_count'] / (data['pve_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_win_lanch_ratio'] = data['pve_win_count'] / (data['pve_lanch_count'] + 1e-4)\n",
    "\n",
    "    data.loc[:,'pve_pvp_battle_count'] = data['pvp_battle_count'] + data['pve_battle_count']\n",
    "    data.loc[:,'pve_pvp_lanch_count'] = data['pvp_lanch_count'] + data['pve_lanch_count']\n",
    "    data.loc[:,'pve_pvp_win_count'] = data['pvp_win_count'] + data['pve_win_count']\n",
    "\n",
    "    data.loc[:,'pve_pvp_lanch_'] = data['pve_pvp_lanch_count'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_pvp_win_ratio'] = data['pve_pvp_win_count'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_pvp_win_lanch_ratio'] = data['pve_pvp_win_count'] / (data['pve_pvp_lanch_count'] + 1e-4)\n",
    "\n",
    "    # 时间、消费\n",
    "    data.loc[:,'pay_mean_count'] = data['pay_price'] / (data['pay_count'] + 1e-4)\n",
    "    data.loc[:,'pay_mean_online_minutes'] = data['pay_price'] / (data['avg_online_minutes'] + 1e-4)\n",
    "    data.loc[:,'pay_count_online_minutes'] = data['avg_online_minutes'] / (data['pay_count'] + 1e-4)\n",
    "\n",
    "    data.loc[:,'time_lanch_per'] = data['avg_online_minutes'] / (data['pve_pvp_lanch_count'] + 1e-4)\n",
    "    data.loc[:,'money_lanch_per'] = data['pay_price'] / (data['pve_pvp_lanch_count'] + 1e-4)\n",
    "    \n",
    "    data.loc[:,'time_battle_per'] = data['avg_online_minutes'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'money_battle_per'] = data['pay_price'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "\n",
    "    # data.loc[:,'mean_pay_price'] = data['pay_price'] / 7\n",
    "\n",
    "    data.loc[:,'time_win_per'] = data['avg_online_minutes'] / (data['pve_pvp_win_count'] + 1e-4)\n",
    "    data.loc[:,'money_win_per'] = data['pay_price'] / (data['pve_pvp_win_count'] + 1e-4)\n",
    "    data.loc[:,'pay_count_win_per'] = data['pay_count'] / (data['pve_pvp_win_count'] + 1e-4)\n",
    "    return data\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "045f96a52c3bd6adb239cb4d34d4d607fecfa218"
   },
   "outputs": [],
   "source": [
    "data = process(data)\n",
    "\n",
    "def split(data):\n",
    "    train = data[data.prediction_pay_price != -1]\n",
    "    test = data[data.prediction_pay_price == -1]\n",
    "    test_usid = test.user_id\n",
    "    del test['user_id']\n",
    "    del test['prediction_pay_price']\n",
    "    test_X = test.values.astype(np.float32)\n",
    "\n",
    "    del train['user_id']\n",
    "    # y = np.log1p(train['prediction_pay_price'].values)\n",
    "    \n",
    "    train = train.loc[train['prediction_pay_price']<16000,:]\n",
    "    y = train['prediction_pay_price'].values.astype(np.float32)\n",
    "    \n",
    "    del train['prediction_pay_price']\n",
    "    X = train.values.astype(np.float32)\n",
    "    col = train.columns\n",
    "    return X, y, test_X, train, test, col, test_usid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8489c18ccc7729fa70ff61743a655856e1b5aad"
   },
   "outputs": [],
   "source": [
    "X, y, test_X, train, test, col, test_with_pay_usid  = split(data)\n",
    "\n",
    "#用模型选择特征\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, ElasticNetCV, RidgeCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "xgb_regressor = xgb.XGBRegressor()\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.005,0.003,  0.001, 0.0005, 0.0001])\n",
    "sfm = SelectFromModel(xgb_regressor)\n",
    "sfm.fit(X, y)\n",
    "X = sfm.transform(X)\n",
    "test_X = sfm.transform(test_X)\n",
    "\n",
    "def rmsel(y_true,y_pre):\n",
    "    return mean_squared_error(y_true,y_pre)**0.5\n",
    "\n",
    "#用10折交叉验证预测结果\n",
    "kf = KFold(n_splits=10,random_state=24,shuffle=True)\n",
    "best_rmse = []\n",
    "pred_list = []\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_val = X[val_index]\n",
    "    y_val = y[val_index]\n",
    "\n",
    "    # regr = LinearRegression()\n",
    "#     regr = Ridge(alpha=1.0, max_iter=100, tol=0.001, random_state=24)\n",
    "#     regr = RandomForestRegressor(n_estimators=120,max_depth=8, random_state=0)\n",
    "    regr = GradientBoostingRegressor(n_estimators=100, subsample=0.9)\n",
    "    regr.fit(X_train,y_train)\n",
    "    predi = regr.predict(X_val)\n",
    "    predi = np.where(predi<0,0,predi)\n",
    "\n",
    "    # rmse = rmsel(np.expm1(y_val), np.expm1(predi))\n",
    "    rmse = rmsel(y_val, predi)\n",
    "\n",
    "    print(\"cv: \",rmse)\n",
    "\n",
    "    predi = regr.predict(test_X)\n",
    "    predi = np.where(predi<0,0,predi)\n",
    "\n",
    "    pred_list.append(predi)\n",
    "    best_rmse.append(rmse)\n",
    "\n",
    "pred = np.mean(np.array(pred_list),axis=0)\n",
    "meanrmse = np.mean(best_rmse)\n",
    "stdrmse = np.std(best_rmse)\n",
    "print('10 flod mean rmse, std rmse:',(meanrmse,stdrmse))\n",
    "\n",
    "\n",
    "# test_with_pay = pd.DataFrame()\n",
    "# test_with_pay['user_id'] = test_with_pay_usid\n",
    "# pred[pred < 1] = 0.99\n",
    "# test_with_pay['prediction_pay_price'] = pred\n",
    "# test_with_pay.describe()\n",
    "# sub = pd.DataFrame()\n",
    "# sub['user_id'] = test_usid.values\n",
    "# # sub['prediction_pay_price'] = preds\n",
    "# sub['prediction_pay_price'] = 0\n",
    "# sub.loc[sub.user_id.isin(test_with_pay.user_id), 'prediction_pay_price'] = test_with_pay['prediction_pay_price'].values * 1.432\n",
    "# print(sub.head(), '\\n')\n",
    "# print(sub.describe())\n",
    "# sub.to_csv('gbdt_xiaoyu_16000.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30aef9be0a8025d314b17b71289418b17e4dd0cd"
   },
   "outputs": [],
   "source": [
    "# sub[sub.prediction_pay_price != 0].sort_values('prediction_pay_price', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db735a8c8194b435f9619622208a860ed0f92127"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

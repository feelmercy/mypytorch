{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "def encode_onehot(df,column_name):\n",
    "    feature_df=pd.get_dummies(df[column_name], prefix=column_name)\n",
    "    all = pd.concat([df.drop([column_name], axis=1),feature_df], axis=1)\n",
    "    return all\n",
    "\n",
    "def encode_count(df,column_name):\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(df[column_name].values))\n",
    "    df[column_name] = lbl.transform(list(df[column_name].values))\n",
    "    return df\n",
    "\n",
    "def merge_count(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].count()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_nunique(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].nunique()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_median(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].median()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_mean(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].mean()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_sum(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].sum()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_max(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].max()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_min(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].min()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_std(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].std()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def feat_count(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].count()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_count\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_nunique(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].nunique()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_nunique\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_mean(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].mean()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_mean\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_std(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].std()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_std\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_median(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].median()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_median\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_max(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].max()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_max\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_min(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].min()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_min\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_sum(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].sum()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_sum\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feat_var(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].var()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_var\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feat_quantile(df, df_feature, fe,value,n,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].quantile(n)).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_quantile\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_skew(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].skew()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_skew\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def action_feats(df, df_features,fe=\"userid\"):\n",
    "    a = pd.get_dummies(df_features, columns=['actionType']).groupby(fe).sum()\n",
    "    a = a[[i for i in a.columns if 'actionType' in i]].reset_index()\n",
    "    df = df.merge(a, on=fe, how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "# 分组排序\n",
    "def rank(data, feat1, feat2, ascending):\n",
    "    data.sort_values([feat1,feat2],inplace=True,ascending=ascending)\n",
    "    data['rank'] = range(data.shape[0])\n",
    "    min_rank = data.groupby(feat1,as_index=False)['rank'].agg({'min_rank':'min'})\n",
    "    data = pd.merge(data,min_rank,on=feat1,how='left')\n",
    "    data['rank'] = data['rank'] - data['min_rank']\n",
    "    del data['min_rank']\n",
    "    return data\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "#encoding=utf8\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from com_util import *\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "path=\"F:\\\\study\\\\ml\\\\DataSet\\\\Hbc_csv\\\\\"\n",
    "userProfile_train=pd.read_csv(path+\"userProfile_train.csv\")\n",
    "userProfile_test=pd.read_csv(path+\"userProfile_test.csv\")\n",
    "userComment_train=pd.read_csv(path+\"userComment_train.csv\")\n",
    "userComment_test=pd.read_csv(path+\"userComment_test.csv\")\n",
    "orderHistory_train=pd.read_csv(path+\"orderHistory_train.csv\")\n",
    "orderHistory_test=pd.read_csv(path+\"orderHistory_test.csv\")\n",
    "orderFuture_train=pd.read_csv(path+\"orderFuture_train.csv\")\n",
    "orderFuture_test=pd.read_csv(path+\"orderFuture_test.csv\")\n",
    "action_train=pd.read_csv(path+\"action_train.csv\")\n",
    "action_test=pd.read_csv(path+\"action_test.csv\")\n",
    "#对日期做一些处理\n",
    "def get_date(timestamp) :\n",
    "    time_local = time.localtime(timestamp)\n",
    "    #dt = time.strftime(\"%Y-%m-%d %H\",time_local)\n",
    "    dt = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_local)\n",
    "    return dt\n",
    "\n",
    "orderFuture_test[\"orderType\"]=-1\n",
    "data=pd.concat([orderFuture_train,orderFuture_test])\n",
    "user_profile=pd.concat([userProfile_train,userProfile_test]).fillna(-1)\n",
    "user_comment=pd.concat([userComment_train,userComment_test])\n",
    "\n",
    "order_history=pd.concat([orderHistory_train,orderHistory_test])\n",
    "order_history[\"date\"]=order_history[\"orderTime\"].apply(get_date)\n",
    "order_history[\"date\"]=pd.to_datetime(order_history[\"date\"])\n",
    "order_history[\"weekday\"]=order_history[\"date\"].dt.weekday\n",
    "order_history[\"hour\"]=order_history[\"date\"].dt.hour\n",
    "order_history[\"month\"]=order_history[\"date\"].dt.month\n",
    "order_history[\"day\"]=order_history[\"date\"].dt.day\n",
    "order_history[\"minute\"]=order_history[\"date\"].dt.minute\n",
    "order_history[\"second\"]=order_history[\"date\"].dt.second\n",
    "order_history['tm_hour']=order_history['hour']+order_history['minute']/60.0\n",
    "order_history['tm_hour_sin'] = order_history['tm_hour'].map(lambda x: math.sin((x-12)/24*2*math.pi))\n",
    "order_history['tm_hour_cos'] = order_history['tm_hour'].map(lambda x: math.cos((x-12)/24*2*math.pi))\n",
    "\n",
    "action=pd.concat([action_train,action_test])\n",
    "\n",
    "#增加历史订单\n",
    "order_history_action=order_history[[\"userid\",\"orderTime\",\"orderType\"]].copy()\n",
    "order_history_action.columns=[\"userid\",\"actionTime\",\"actionType\"]\n",
    "order_history_action[\"actionType\"]=order_history_action[\"actionType\"].apply(lambda x:x+10)\n",
    "action=pd.concat([action,order_history_action])\n",
    "\n",
    "action[\"date\"]=action[\"actionTime\"].apply(get_date)\n",
    "action[\"date\"]=pd.to_datetime(action[\"date\"])\n",
    "action[\"weekday\"]=action[\"date\"].dt.weekday\n",
    "action[\"hour\"]=action[\"date\"].dt.hour\n",
    "action[\"month\"]=action[\"date\"].dt.month\n",
    "action[\"day\"]=action[\"date\"].dt.day\n",
    "action[\"minute\"]=action[\"date\"].dt.minute\n",
    "action[\"second\"]=action[\"date\"].dt.second\n",
    "action['tm_hour']=action['hour']+action['minute']/60.0\n",
    "action['tm_hour_sin'] = action['tm_hour'].map(lambda x: math.sin((x-12)/24*2*math.pi))\n",
    "action['tm_hour_cos'] = action['tm_hour'].map(lambda x: math.cos((x-12)/24*2*math.pi))\n",
    "\n",
    "action=action.sort_values([\"userid\",\"actionTime\"])\n",
    "action[\"date\"]=action[\"actionTime\"].apply(get_date)\n",
    "action[\"actionTime_gap\"]=action[\"actionTime\"]-action[\"actionTime\"].shift(1)\n",
    "action[\"actionType_gap\"]=action[\"actionType\"].shift(1)-action[\"actionType\"]\n",
    "action[\"actionTime_long\"]=action[\"actionTime\"].shift(-1)-action[\"actionTime\"]\n",
    "action[\"actionTime_gap_2\"]=action[\"actionTime_gap\"]-action[\"actionTime_gap\"].shift(1)\n",
    "action[\"actionTime_long_2\"]=action[\"actionTime_long\"]-action[\"actionTime_long\"].shift(1)\n",
    "#action[\"user_id\"]=action[\"userid\"].shift(1) #上移，获取下次userid\n",
    "#action[\"action_type\"]=action[\"actionType\"].shift(1) #上移，获取下次type\n",
    "\n",
    "order_history_time=order_history[[\"userid\",\"orderTime\"]].copy()\n",
    "this_time=action.drop_duplicates(\"userid\",keep=\"last\")[[\"userid\",\"actionTime\"]].copy()\n",
    "this_time.columns=[\"userid\",\"orderTime\"]\n",
    "order_history_time=pd.concat([order_history_time,this_time])\n",
    "order_history_time=order_history_time.drop_duplicates()\n",
    "order_history_time=order_history_time.sort_values([\"userid\",\"orderTime\"])\n",
    "order_history_time[\"orderTime_gap\"]=order_history_time[\"orderTime\"]-order_history_time[\"orderTime\"].shift(1)\n",
    "\n",
    "user_profile=encode_count(user_profile,\"gender\")\n",
    "user_profile=encode_count(user_profile,\"province\")\n",
    "user_profile=encode_count(user_profile,\"age\")\n",
    "\n",
    "data=data.merge(user_profile,on=\"userid\",how=\"left\")\n",
    "#######################################################################################################\n",
    "order_history_diff=order_history.sort_values([\"userid\",\"orderTime\"])\n",
    "order_history_diff[\"order_diff\"]=order_history_diff[\"orderid\"]-order_history_diff[\"orderid\"].shift(1)\n",
    "order_history_diff[\"order_diff\"]=order_history_diff[\"order_diff\"].apply(lambda x:1 if x<0 else 0)\n",
    "data=feat_sum(data,order_history_diff,[\"userid\"],\"order_diff\")\n",
    "\n",
    "#历史表\n",
    "data=feat_count(data,order_history,[\"userid\"],\"orderid\",\"history_count\")\n",
    "data=feat_max(data,order_history,[\"userid\"],\"orderTime\")\n",
    "data=feat_min(data,order_history,[\"userid\"],\"orderTime\")\n",
    "data=feat_sum(data,order_history,[\"userid\"],\"orderType\")\n",
    "\n",
    "for i in [\"日本\",\"美国\",\"澳大利亚\",\"新加坡\",\"泰国\"]:\n",
    "    order_history_select=order_history[order_history.country==i]\n",
    "    print order_history_select.shape\n",
    "    data=feat_count(data,order_history_select,[\"userid\"],\"orderid\",\"%s_count\"%i)\n",
    "\n",
    "for i in [\"亚洲\",\"欧洲\",\"大洋洲\",\"北美洲\"]:\n",
    "    order_history_select=order_history[order_history.continent==i]\n",
    "    print order_history_select.shape\n",
    "    data=feat_count(data,order_history_select,[\"userid\"],\"orderid\")\n",
    "\n",
    "#评论表\n",
    "data=feat_min(data,user_comment,[\"userid\"],\"rating\")\n",
    "data=feat_count(data,user_comment,[\"userid\"],\"rating\")\n",
    "\n",
    "#此用户的评论是否在历史表中\n",
    "kk=order_history[[\"orderid\"]].copy()\n",
    "kk[\"not_in_history\"]=0\n",
    "user_comment_orderid=user_comment.merge(kk,on=\"orderid\",how=\"left\").fillna(1)\n",
    "user_comment_orderid=user_comment_orderid[user_comment_orderid.not_in_history==1]\n",
    "data=feat_sum(data,user_comment_orderid,[\"userid\"],\"not_in_history\")\n",
    "data=feat_mean(data,user_comment_orderid,[\"userid\"],\"orderid\",\"orderid_no\")\n",
    "data=feat_max(data,order_history,[\"userid\"],\"orderid\",\"orderid_max\")\n",
    "data[\"ooxx\"]=data[\"orderid_no\"]-data[\"orderid_max\"]\n",
    "\n",
    "order_history[\"uo_rt\"]=(order_history[\"userid\"])/(order_history[\"orderid\"])\n",
    "data=feat_std(data,order_history,[\"userid\"],\"uo_rt\")\n",
    "#######################################################################################################\n",
    "#行为表\n",
    "action_last=pd.DataFrame(action.groupby([\"userid\"]).actionTime.max()).reset_index()\n",
    "action_last.columns=[\"userid\",\"actionTime_last\"]\n",
    "action=action.merge(action_last,on=\"userid\",how=\"left\")\n",
    "action[\"actionTime_last_dif\"]=action[\"actionTime_last\"]-action[\"actionTime\"]\n",
    "\n",
    "action_567=action[(action.actionType>=5)&(action.actionType<=7)]\n",
    "for i in [600,1800,3600,36000,100000,100000000]:\n",
    "    action_select=action_567[action_567.actionTime_last_dif<i].copy()\n",
    "    data=action_feats(data,action_select)\n",
    "\n",
    "#用户5,6,7操作的平均时长\n",
    "for i in range(5,8):\n",
    "    action_select = action_567[action_567.actionType == i].copy()\n",
    "    data = feat_mean(data, action_select, [\"userid\"], \"actionTime_long\", \"action_user_onlytype_mean_%s\" % i)\n",
    "    #data = feat_quantile(data, action_select, [\"userid\"], \"actionTime_long\",0.75, \"action_user_onlytype_quantile_0.75_%s\" % i)\n",
    "    #data = feat_quantile(data, action_select, [\"userid\"], \"actionTime_long\",0.25, \"action_user_onlytype_quantile_0.25_%s\" % i)\n",
    "    data = feat_median(data, action_select, [\"userid\"], \"actionTime_long\", \"action_user_onlytype_median_%s\" % i)\n",
    "    data = feat_max(data, action_select, [\"userid\"], \"actionTime_long\", \"action_user_onlytype_max_%s\" % i)\n",
    "    data = feat_min(data, action_select, [\"userid\"], \"actionTime_long\", \"action_user_onlytype_min_%s\" % i)\n",
    "    data = feat_std(data, action_select, [\"userid\"], \"actionTime_long\", \"action_user_onlytype_std_%s\" % i)\n",
    "\n",
    "    data = feat_mean(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_user_onlytype_mean_%s\" % i)\n",
    "    #data = feat_quantile(data, action_select, [\"userid\"], \"actionTime_gap\",0.75, \"gap_action_user_onlytype_quantile_0.75_%s\" % i)\n",
    "    #data = feat_quantile(data, action_select, [\"userid\"], \"actionTime_gap\",0.25, \"gap_action_user_onlytype_quantile_0.25_%s\" % i)\n",
    "    data = feat_median(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_user_onlytype_median_%s\" % i)\n",
    "    data = feat_max(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_ction_user_onlytype_max_%s\" % i)\n",
    "    data = feat_min(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_user_onlytype_min_%s\" % i)\n",
    "    data = feat_std(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_user_onlytype_std_%s\" % i)\n",
    "\n",
    "    data = feat_max(data, action_select, [\"userid\"], \"actionTime\", \"actionType_max_%s\" % i)\n",
    "    data = feat_min(data, action_select, [\"userid\"], \"actionTime\", \"actionType_min_%s\" % i)\n",
    "\n",
    "for i in [10]:\n",
    "    action_select = action[action.actionType == i].copy()\n",
    "    data = feat_mean(data, action_select, [\"userid\"], \"actionTime_long\", \"action_newtype_mean_%s\" % i)\n",
    "    data = feat_max(data, action_select, [\"userid\"], \"actionTime_long\", \"action_newtype_max_%s\" % i)\n",
    "    data = feat_min(data, action_select, [\"userid\"], \"actionTime_long\", \"action_newtype_min_%s\" % i)\n",
    "    data = feat_std(data, action_select, [\"userid\"], \"actionTime_long\", \"action_newtype_std_%s\" % i)\n",
    "\n",
    "    data = feat_mean(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_newtype_mean_%s\" % i)\n",
    "    data = feat_max(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_newtype_max_%s\" % i)\n",
    "    data = feat_min(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_newtype_min_%s\" % i)\n",
    "    data = feat_std(data, action_select, [\"userid\"], \"actionTime_gap\", \"gap_action_newtype_std_%s\" % i)\n",
    "\n",
    "for a,b in [(6,5),(7,5)]:\n",
    "    data[\"max_%s-max_%s\"%(a,b)] = data[\"actionType_max_%s\"%a] - data[\"actionType_max_%s\"%b]\n",
    "    data[\"min_%s-min_%s\"%(a,b)] = data[\"actionType_min_%s\"%a] - data[\"actionType_min_%s\"%b]\n",
    "    data[\"%s_%s_rt\"%(a,b)] = data[\"max_%s-max_%s\"%(a,b)] / data[\"min_%s-min_%s\"%(a,b)]\n",
    "    data[\"%s_%s_dif\"%(a,b)] = data[\"max_%s-max_%s\"%(a,b)] - data[\"min_%s-min_%s\"%(a,b)]\n",
    "\n",
    "#所有actionType占比\n",
    "type_prob=pd.DataFrame(action.groupby([\"userid\",\"actionType\"]).actionTime.count()).reset_index()\n",
    "type_prob.columns=[\"userid\",\"actionType\",\"type_count\"]\n",
    "type_prob=feat_count(type_prob,action,[\"userid\"],\"actionType\",\"all_count\")\n",
    "type_prob[\"type_rt\"]=type_prob[\"type_count\"]/type_prob[\"all_count\"]\n",
    "action_user_type = pd.pivot_table(type_prob, index=[\"userid\"], columns=[\"actionType\"],values=\"type_rt\", fill_value=0).reset_index()\n",
    "data = data.merge(action_user_type, on=\"userid\", how=\"left\")\n",
    "\n",
    "#到最近的每种type的时间距离\n",
    "action_last=pd.DataFrame(action.groupby([\"userid\",\"actionType\"]).actionTime.max()).reset_index()\n",
    "action_last.columns=[\"userid\",\"actionType\",\"type_actionTime_last\"]\n",
    "action_last[\"actionType\"]=action_last[\"actionType\"].apply(lambda x:\"action_last_\"+str(x))\n",
    "action_last=feat_max(action_last,action,[\"userid\"],\"actionTime\",\"user_last_time\")\n",
    "action_last[\"before_type_time_gap\"]=action_last[\"user_last_time\"]-action_last[\"type_actionTime_last\"]\n",
    "action_user_type = pd.pivot_table(action_last, index=[\"userid\"], columns=[\"actionType\"], values=\"before_type_time_gap\",fill_value=100000).reset_index()\n",
    "data = data.merge(action_user_type, on=\"userid\", how=\"left\")\n",
    "\n",
    "data[\"action_last_5_6\"]=data[\"action_last_5\"]-data[\"action_last_6\"]\n",
    "data[\"action_last_1_5\"]=data[\"action_last_1\"]-data[\"action_last_5\"]\n",
    "data[\"action_last_1_7\"]=data[\"action_last_1\"]-data[\"action_last_7\"]\n",
    "\n",
    "#最后一次type5，6的持续时间\n",
    "action_56=action[(action.actionType>=1)&(action.actionType<=6)]\n",
    "action_56=action_56.sort_values(\"actionTime\")\n",
    "action_56=action_56.drop_duplicates([\"userid\",\"actionType\"],keep=\"last\")\n",
    "action_56[\"actionType\"]=action_56[\"actionType\"].apply(lambda x:\"action_long_\"+str(x))\n",
    "action_user_type = pd.pivot_table(action_56, index=[\"userid\"], columns=[\"actionType\"], values=\"actionTime_long\",fill_value=100000).reset_index()\n",
    "data = data.merge(action_user_type, on=\"userid\", how=\"left\")\n",
    "action_56[\"actionType\"]=action_56[\"actionType\"].apply(lambda x:x.replace(\"action_long_\",\"action_gap_\"))\n",
    "action_user_type = pd.pivot_table(action_56, index=[\"userid\"], columns=[\"actionType\"], values=\"actionTime_gap\",fill_value=100000).reset_index()\n",
    "data = data.merge(action_user_type, on=\"userid\", how=\"left\")\n",
    "#data[\"action_long_5_rt\"]=data[\"action_long_5\"]/data[\"action_user_onlytype_mean_5\"]\n",
    "data[\"action_long_6_rt\"]=data[\"action_long_6\"]/data[\"action_user_onlytype_mean_6\"]\n",
    "\n",
    "#第一次，最后一次操作实间\n",
    "data=feat_max(data,action,[\"userid\"],\"actionTime\",\"last_time\")\n",
    "data=feat_min(data,action,[\"userid\"],\"actionTime\",\"early_time\")\n",
    "data=feat_count(data,action,[\"userid\"],\"actionTime\",\"action_count\")\n",
    "\n",
    "last_time_dt=pd.to_datetime(data[\"last_time\"].apply(get_date))\n",
    "data[\"month\"]=last_time_dt.dt.month\n",
    "data[\"day\"]=last_time_dt.dt.day\n",
    "data[\"weekday\"]=last_time_dt.dt.weekday\n",
    "data[\"hour\"]=last_time_dt.dt.hour\n",
    "data[\"minute\"]=last_time_dt.dt.minute\n",
    "data[\"second\"]=last_time_dt.dt.second\n",
    "data['tm_hour']=data['hour']+data['minute']/60.0\n",
    "data['tm_hour_sin'] = data['tm_hour'].map(lambda x: math.sin((x-12)/24*2*math.pi))\n",
    "data['tm_hour_cos'] = data['tm_hour'].map(lambda x: math.cos((x-12)/24*2*math.pi))\n",
    "\n",
    "data['tm_day']=data['day']+data['hour']/24.0\n",
    "data['tm_day_sin'] = data['tm_day'].map(lambda x: math.sin((x-30)/30*2*math.pi))\n",
    "data['tm_day_cos'] = data['tm_day'].map(lambda x: math.cos((x-30)/30*2*math.pi))\n",
    "\n",
    "data=feat_count(data,order_history,[\"userid\",\"month\"],\"orderid\")\n",
    "data=feat_count(data,order_history,[\"userid\",\"day\"],\"orderid\")\n",
    "data=feat_count(data,order_history,[\"userid\",\"weekday\"],\"orderid\")\n",
    "data=feat_count(data,order_history,[\"userid\",\"hour\"],\"orderid\")\n",
    "data=feat_count(data,order_history,[\"userid\",\"minute\"],\"orderid\")\n",
    "data=feat_count(data,order_history,[\"userid\",\"second\"],\"orderid\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for i in [2]:\n",
    "    action_tail = pd.DataFrame(action.groupby(\"userid\").tail(i)).reset_index()\n",
    "    data=feat_min(data,action_tail,[\"userid\"],\"actionTime\",\"action_time_%s\"%i)\n",
    "    last_time_dt=pd.to_datetime(data[\"action_time_%s\"%i].apply(get_date))\n",
    "    data[\"hour_%s\"%i]=last_time_dt.dt.hour\n",
    "    data[\"minute_%s\"%i]=last_time_dt.dt.minute\n",
    "    data[\"second_%s\"%i]=last_time_dt.dt.second\n",
    "    del data[\"action_time_%s\"%i]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "for i in range(5,8):\n",
    "    action_select = action_567[action_567.actionType == i].copy()\n",
    "    data=feat_mean(data,action_select,[\"userid\"],\"tm_hour_sin\",\"tm_hour_sin_mean_%s\"%i)\n",
    "    data=feat_mean(data,action_select,[\"userid\"],\"tm_hour_cos\",\"tm_hour_cos_mean_%s\"%i)\n",
    "    data['tm_hour_std'] = list(map(lambda x, y: x * x + y * y, data['tm_hour_sin_mean_%s'%i],data['tm_hour_cos_mean_%s'%i]))\n",
    "    data['tm_hour_mean'] = list(map(lambda x, y: math.atan(x / y) if y > 0 else math.atan(x / y) + math.pi,data['tm_hour_sin_mean_%s'%i], data['tm_hour_cos_mean_%s'%i]))\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "#for a,b in [(6,7)]:\n",
    "    #action_select=action[(action.userid==action.user_id)&(action.actionType==a)&(action.action_type==b)&(action.actionTime_long<1000)]\n",
    "    #data=feat_count(data,action_select,[\"userid\"],\"actionTime_long\")\n",
    "#######################################################################################################\n",
    "#字符串级别匹配\n",
    "import os\n",
    "import re\n",
    "#是否购买精品\n",
    "if not os.path.exists(\"../input/string_match.csv\"):\n",
    "    action_str = pd.DataFrame(action.groupby(\"userid\").actionType.apply(lambda x: \"\".join([str(i) for i in list(x)]))).reset_index()\n",
    "    action_str.columns = [\"userid\", \"action_str\"]\n",
    "    all_str=\"\".join(list(action_str[\"action_str\"]))\n",
    "    for i in [1,2,3,4,5,6,7,8]:\n",
    "        action_str[\"last_%s_str\"%i] = action_str[\"action_str\"].apply(lambda x:x[-i:])\n",
    "        action_str_last=action_str[[\"last_%s_str\"%i]].drop_duplicates()\n",
    "        action_str_last[\"last_%s_search_rt\"%i]=action_str_last[\"last_%s_str\"%i].apply(lambda x:len(re.findall(x+\"8\",all_str))/float(len(re.findall(x,all_str))))\n",
    "        #action_str_last[\"last_%s_search_rt\"%i]=action_str_last[\"last_%s_str\"%i].apply(lambda x:len(re.findall(x+(\"8\" if x[-1]==\"7\" else (\"78\" if x[-1]==\"6\" else (\"678\" if x[-1]==\"5\" else \"5678\"))),all_str))/float(len(re.findall(x,all_str))))\n",
    "        action_str=action_str.merge(action_str_last,on=\"last_%s_str\"%i,how=\"left\")\n",
    "\n",
    "        del action_str[\"last_%s_str\"%i]\n",
    "    del action_str[\"action_str\"]\n",
    "    print action_str\n",
    "    action_str.to_csv(\"../input/string_match.csv\",index=None)\n",
    "else:\n",
    "    action_str=pd.read_csv(\"../input/string_match.csv\")\n",
    "data=data.merge(action_str,on=\"userid\",how=\"left\")\n",
    "\n",
    "#\n",
    "###\n",
    "for i in [1]:\n",
    "    action_tail=pd.DataFrame(action.groupby(\"userid\").tail(i)).reset_index()\n",
    "    data = feat_mean(data, action, [\"userid\"], \"actionTime_long\", \"last_type_long\")\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "#github\n",
    "X_train=pd.read_csv(\"../input/data_train.csv\")\n",
    "del X_train[\"futureOrderType\"]\n",
    "X_test=pd.read_csv(\"../input/data_test.csv\")\n",
    "X=pd.concat([X_train,X_test])\n",
    "data=data.merge(X,on=\"userid\",how=\"left\")\n",
    "\n",
    "print data.head()\n",
    "#################################\n",
    "valid=data[data.orderType==-1].copy()\n",
    "del valid[\"orderType\"]\n",
    "data=data[data.orderType!=-1].copy()\n",
    "data.to_csv(\"plantsgo_model_2_train.csv\",index=None)\n",
    "valid.to_csv(\"plantsgo_model_2_test.csv\",index=None)\n",
    "\n",
    "def stacking(clf,train_x,train_y,test_x,clf_name,class_num=1):\n",
    "    train=np.zeros((train_x.shape[0],class_num))\n",
    "    test=np.zeros((test_x.shape[0],class_num))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],class_num))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf):\n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "        test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "\n",
    "        params = {\n",
    "                  'boosting_type': 'gbdt',\n",
    "                  'objective': 'binary',\n",
    "                  'metric': 'auc',\n",
    "                  'min_child_weight': 1.5,\n",
    "                  'num_leaves': 2**5,\n",
    "                  'lambda_l2': 10,\n",
    "                  'subsample': 0.7,\n",
    "                  'colsample_bytree': 0.5,\n",
    "                  'colsample_bylevel': 0.5,\n",
    "                  'learning_rate': 0.01,\n",
    "                  'seed': 2017,\n",
    "                  'nthread': 12,\n",
    "                  'silent': True,\n",
    "                  }\n",
    "\n",
    "\n",
    "        num_round = 15000\n",
    "        early_stopping_rounds = 100\n",
    "        if test_matrix:\n",
    "            model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                              early_stopping_rounds=early_stopping_rounds\n",
    "                              )\n",
    "            pre= model.predict(te_x,num_iteration=model.best_iteration).reshape((te_x.shape[0],1))\n",
    "            train[test_index]=pre\n",
    "            test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape((test_x.shape[0],1))\n",
    "            cv_scores.append(roc_auc_score(te_y, pre))\n",
    "\n",
    "        print \"%s now score is:\"%clf_name,cv_scores\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print \"%s_score_list:\"%clf_name,cv_scores\n",
    "    print \"%s_score_mean:\"%clf_name,np.mean(cv_scores)\n",
    "    with open(\"score_cv.txt\", \"a\") as f:\n",
    "        f.write(\"%s now score is:\" % clf_name + str(cv_scores) + \"\\n\")\n",
    "        f.write(\"%s_score_mean:\"%clf_name+str(np.mean(cv_scores))+\"\\n\")\n",
    "    return train.reshape(-1,class_num),test.reshape(-1,class_num),np.mean(cv_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

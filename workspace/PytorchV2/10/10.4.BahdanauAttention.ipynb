{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c780cc1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T03:11:56.949708Z",
     "start_time": "2023-09-21T03:11:53.493116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib auto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07b30a0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:14:29.277839Z",
     "start_time": "2023-09-21T09:14:29.273934Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_mask(X,valid_len,value=0):\n",
    "    maxlen=X.size(1)\n",
    "    mask=torch.arange((maxlen),dtype=torch.float32,device=X.device)[None,:]<valid_len[:,None]\n",
    "    X[~mask]=value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fbc82e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:14:29.774376Z",
     "start_time": "2023-09-21T09:14:29.769494Z"
    }
   },
   "outputs": [],
   "source": [
    "def masked_softmax(X,valid_lens):\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X,dim=-1)\n",
    "    else:\n",
    "        shape=X.shape\n",
    "        if valid_lens.dim()==1:\n",
    "            valid_lens=torch.repeat_interleave(valid_lens,shape[1])\n",
    "        else:\n",
    "            valid_lens=valid_lens.reshape(-1)\n",
    "        X=sequence_mask(X.reshape(-1,shape[-1]),valid_lens,value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68934ccc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:14:30.433398Z",
     "start_time": "2023-09-21T09:14:30.385558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4692, 0.5308, 0.0000, 0.0000],\n",
       "         [0.5610, 0.4390, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3405, 0.3220, 0.3375, 0.0000],\n",
       "         [0.3044, 0.4306, 0.2650, 0.0000]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061396b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2893ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T03:16:44.244619Z",
     "start_time": "2023-09-21T03:16:44.240713Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def init_state(self,enc_all_outputs,*args):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3c87a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T03:18:49.860224Z",
     "start_time": "2023-09-21T03:18:49.856305Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(Decoder):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "168db6c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:15:26.601031Z",
     "start_time": "2023-09-21T09:15:26.594198Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self,key_size,query_size,num_hiddens,dropout,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_k=nn.Linear(key_size,num_hiddens,bias=False)\n",
    "        self.W_q=nn.Linear(query_size,num_hiddens,bias=False)\n",
    "        self.W_v=nn.Linear(num_hiddens,1,bias=False)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,queries,keys,values,valid_lens):\n",
    "        queries,keys=self.W_q(queries),self.W_k(keys)\n",
    "        features=queries.unsqueeze(2)+keys.unsqueeze(1)\n",
    "        features=torch.tanh(features)\n",
    "        scores=self.W_v(features).squeeze(-1)\n",
    "        self.attention_weights=masked_softmax(scores,valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights),values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94c50e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:25:14.264670Z",
     "start_time": "2023-09-21T09:25:14.254907Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention=AdditiveAttention(num_hiddens,num_hiddens,num_hiddens,dropout)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn=nn.GRU(embed_size+num_hiddens,num_hiddens,num_layers,dropout=dropout)\n",
    "        self.dense=nn.Linear(num_hiddens,vocab_size)\n",
    "        \n",
    "    def init_state(self,enc_outputs,enc_valid_lens,*args):\n",
    "        outputs,hidden_state=enc_outputs\n",
    "        return (outputs.permute(1,0,2),hidden_state,enc_valid_lens)\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        enc_outputs,hidden_state,enc_valid_lens=state\n",
    "        X=self.embedding(X).permute(1,0,2)\n",
    "        outputs,self._attention_weights=[],[]\n",
    "        for x in X:\n",
    "            query=torch.unsqueeze(hidden_state[-1],dim=1)\n",
    "            context=self.attention(query,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "            x=torch.cat((context,torch.unsqueeze(x,dim=1)),dim=-1)\n",
    "            out,hidden_state=self.rnn(x.permute(1,0,2),hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        outputs=self.dense(torch.cat(outputs,dim=0))\n",
    "        return outputs.permute(1,0,2),[enc_outputs,hidden_state,enc_valid_lens]\n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ef576e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:25:14.786029Z",
     "start_time": "2023-09-21T09:25:14.782123Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30977a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:25:15.011775Z",
     "start_time": "2023-09-21T09:25:15.006893Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn=nn.GRU(embed_size,num_hiddens,num_layers,dropout=dropout)\n",
    "#         8,16,2,0\n",
    "\n",
    "    def forward(self,X ,*args):\n",
    "        X=self.embedding(X) #4,7,8\n",
    "        X=X.permute(1,0,2) #7,4,8\n",
    "        output,state=self.rnn(X) \n",
    "        #output:step,batch,num_hiddens=7,4,16\n",
    "        #state :layers,batch,num_hiddnes=2,4,16\n",
    "        return output,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a05e1763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:25:15.246224Z",
     "start_time": "2023-09-21T09:25:15.239389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqEncoder(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): GRU(8, 16, num_layers=2)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,num_layers=2)\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6073ee1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:25:15.446357Z",
     "start_time": "2023-09-21T09:25:15.439532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqAttentionDecoder(\n",
       "  (attention): AdditiveAttention(\n",
       "    (W_k): Linear(in_features=16, out_features=16, bias=False)\n",
       "    (W_q): Linear(in_features=16, out_features=16, bias=False)\n",
       "    (W_v): Linear(in_features=16, out_features=1, bias=False)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): GRU(24, 16, num_layers=2)\n",
       "  (dense): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder=Seq2SeqAttentionDecoder(vocab_size=10,embed_size=8,num_hiddens=16,num_layers=2)\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ce534e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:25:15.641623Z",
     "start_time": "2023-09-21T09:25:15.630882Z"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.zeros((4,7),dtype=torch.long)\n",
    "state=decoder.init_state(encoder(X),None)\n",
    "output,state=decoder(X,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb9ed584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T09:25:15.843723Z",
     "start_time": "2023-09-21T09:25:15.838842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613899a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b011ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

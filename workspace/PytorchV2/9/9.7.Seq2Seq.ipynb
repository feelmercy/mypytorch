{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:34:42.822455Z",
     "start_time": "2023-08-11T02:34:42.818550Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:34:46.267017Z",
     "start_time": "2023-08-11T02:34:46.263123Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def forward(self,X,*args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:34:55.563345Z",
     "start_time": "2023-08-11T02:34:55.558464Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def init_state(self,enc_outputs,*args):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        raise NotImplementedError\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:35:05.524836Z",
     "start_time": "2023-08-11T02:35:05.518964Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn=nn.GRU(embed_size,num_hiddens,num_layers,dropout=dropout)\n",
    "    \n",
    "    def forward(self,X,*args):\n",
    "#         (batch,step,embed)\n",
    "        X=self.embedding(X)\n",
    "#         (step,batch,embed)    \n",
    "        X=X.permute(1,0,2)\n",
    "#         output:(step,batch,hidden)\n",
    "#         state :(layers,batch,hidden)\n",
    "        output,state=self.rnn(X)\n",
    "        return output,state        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:35:14.029637Z",
     "start_time": "2023-08-11T02:35:14.022787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqEncoder(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): GRU(8, 16, num_layers=2)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=Seq2SeqEncoder(vocab_size=10,embed_size=8,num_hiddens=16,num_layers=2)\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:35:15.463231Z",
     "start_time": "2023-08-11T02:35:15.455818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.zeros((4,7,),dtype=torch.long)\n",
    "output,state=encoder(X)\n",
    "output.shape,state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:37:10.829022Z",
     "start_time": "2023-08-11T02:37:10.823178Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn=nn.GRU(embed_size+num_hiddens,num_hiddens,num_layers,dropout=dropout)\n",
    "        self.dense=nn.Linear(num_hiddens,vocab_size)\n",
    "    \n",
    "    # only use state to decode,not output\n",
    "    def init_state(self,enc_outputs,*args):\n",
    "        return enc_outputs[1]\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        X=self.embedding(X).permute(1,0,2)\n",
    "#         上下文和上一步状态相关\n",
    "        context=state[-1].repeat(X.shape[0],1,1)\n",
    "#         输入需要X,上下文\n",
    "        X_and_context=torch.cat((X,context),2)\n",
    "        output,statee=self.rnn(X_and_context,state)\n",
    "#         output经过两次permute,shape和X最开始一样了\n",
    "        output=self.dense(output).permute(1,0,2)\n",
    "        return output,state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:37:46.412954Z",
     "start_time": "2023-08-11T02:37:46.406103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqDecoder(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): GRU(24, 16, num_layers=2)\n",
       "  (dense): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder=Seq2SeqDecoder(vocab_size=10,embed_size=8,num_hiddens=16,num_layers=2)\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:37:47.634891Z",
     "start_time": "2023-08-11T02:37:47.629995Z"
    }
   },
   "outputs": [],
   "source": [
    "state=decoder.init_state(encoder(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:37:48.981945Z",
     "start_time": "2023-08-11T02:37:48.974302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output,state=decoder(X,state)\n",
    "output.shape,state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:37:50.898369Z",
     "start_time": "2023-08-11T02:37:50.894462Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_mask(X,valid_len,value=0):\n",
    "    maxlen=X.size(1)\n",
    "    mask=torch.arange((maxlen),dtype=torch.float32,device=X.device)[None,:]<valid_len[:,None]\n",
    "    X[~mask]=value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:38:34.113848Z",
     "start_time": "2023-08-11T02:38:34.109956Z"
    }
   },
   "outputs": [],
   "source": [
    "X=torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:38:35.293998Z",
     "start_time": "2023-08-11T02:38:35.269589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:38:37.700383Z",
     "start_time": "2023-08-11T02:38:37.637588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(X,torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:39:26.185603Z",
     "start_time": "2023-08-11T02:39:26.181682Z"
    }
   },
   "outputs": [],
   "source": [
    "a=torch.arange(3)\n",
    "b=torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:39:28.138909Z",
     "start_time": "2023-08-11T02:39:28.134022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([1, 2, 3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:39:34.322262Z",
     "start_time": "2023-08-11T02:39:34.317380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:39:34.762587Z",
     "start_time": "2023-08-11T02:39:34.757705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:39:35.330245Z",
     "start_time": "2023-08-11T02:39:35.325351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:39:35.803222Z",
     "start_time": "2023-08-11T02:39:35.797350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True,  True, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None,:]<b[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T06:31:09.330944Z",
     "start_time": "2023-08-08T06:31:09.326076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True,  True, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(0)<b.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:43:21.825786Z",
     "start_time": "2023-08-11T02:43:21.820918Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self,pred,label,valid_len):\n",
    "        weights=torch.ones_like(label)\n",
    "        weights=sequence_mask(weights,valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss=super().forward(pred.permute(0,2,1),label)\n",
    "        weighted_loss=(unweighted_loss*weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:43:22.631054Z",
     "start_time": "2023-08-11T02:43:22.556853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3,4,10),torch.ones((3,4),dtype=torch.long),torch.tensor([4,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T02:43:24.394125Z",
     "start_time": "2023-08-11T02:43:24.389265Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_clipping(net,theta):\n",
    "    if isinstance(net,nn.Module):\n",
    "        params=[p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params=net.params\n",
    "    norm=torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta/norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T03:02:44.801302Z",
     "start_time": "2023-08-11T03:02:44.776894Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_seq2seq(net,data_iter,lr,num_epochs,tgt_vocab,device):\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) ==nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m)==nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    \n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss=MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    train_l,train_num=0,0\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X,X_valid_len,Y,Y_valid_len=[x.to(device) for x in batch]\n",
    "            bos=torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],device=device).reshape(-1,1)\n",
    "            dec_input=torch.cat([bos,Y[:,:-1]],1)\n",
    "            Y_hat,_=net(X,dec_input,X_valid_len)\n",
    "            l=loss(Y_hat,Y,Y_valid_len)\n",
    "            l.sum().backward()\n",
    "            grad_clipping(net,1)\n",
    "            num_tokens=Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                train_l+=l.sum()\n",
    "                train_num+=num_tokens\n",
    "        if(epoch+1)%10==0:\n",
    "            print(\"epoch : \",epoch+1,\" train loss : \",(train_l.item()/train_num.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T07:44:55.117081Z",
     "start_time": "2023-08-11T07:44:55.099220Z"
    }
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,tokens=None,min_freq=0,reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens=[]\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens=[]\n",
    "        counter=count_corpus(tokens)\n",
    "        self._token_freqs=sorted(counter.items(),key=lambda x:x[1],reverse=True)\n",
    "        self.idx_to_token=['<unk>']+reserved_tokens\n",
    "        self.token_to_idx={token:idx for idx,token in enumerate(self.idx_to_token)}\n",
    "        self.idx_to_token,self.token_to_idx=[],dict()\n",
    "        for token,freq in self._token_freqs:\n",
    "            if freq<min_freq:\n",
    "                break;\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token]=len(self.idx_to_token)-1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,tokens):\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            return self.token_to_idx.get(tokens,self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0;\n",
    "    \n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:11:54.949200Z",
     "start_time": "2023-08-11T08:11:54.945286Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_corpus(tokens):\n",
    "    if len(tokens)==0 or isinstance(tokens[0],list):\n",
    "        tokens=[token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:11:55.791614Z",
     "start_time": "2023-08-11T08:11:55.785754Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_nmt(text):\n",
    "    def no_space(char,prev_char):\n",
    "        return char in set(',.!?') and prev_char !=' '\n",
    "    \n",
    "    text=text.replace('\\u202f', ' ').replace('\\xa0',' ').lower()\n",
    "    out=[' '+char if i>0 and no_space(char,text[i-1]) else char for i,char in enumerate(text) ]\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:11:56.741818Z",
     "start_time": "2023-08-11T08:11:56.736936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abd', 'aaa', 'bbb']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"abd,aaa,bbb\"\n",
    "b=a.split(',')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:11:57.477639Z",
     "start_time": "2023-08-11T08:11:57.473532Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_nmt(text,num_examples=None):\n",
    "    source,target=[],[]\n",
    "    for i ,line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts=line.split('\\t')\n",
    "        if len(parts)==2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source,target \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:11:58.222536Z",
     "start_time": "2023-08-11T08:11:58.217667Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "#     \"\"\"将机器翻译的⽂本序列转换成⼩批量\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "#     print(\"lines : \",lines)\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:00.271817Z",
     "start_time": "2023-08-11T08:12:00.265958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3])\n",
    "b=torch.tensor([0])\n",
    "(a!=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:02.966673Z",
     "start_time": "2023-08-11T08:12:02.961668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a!=b).type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:03.646574Z",
     "start_time": "2023-08-11T08:12:03.641692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a!=b).type(torch.int32).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:04.244224Z",
     "start_time": "2023-08-11T08:12:04.233470Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-4d063fc83a15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "(a!=b).type(torch.int32).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:04.766548Z",
     "start_time": "2023-08-11T08:12:04.761680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a!=b).type(torch.int32).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:05.782674Z",
     "start_time": "2023-08-11T08:12:05.776809Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "#     \"\"\"返回翻译数据集的迭代器和词表\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "#     print(\"source : \",source)\n",
    "#     print(\"target : \",target)\n",
    "    src_vocab = Vocab(source, min_freq=2,reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = Vocab(target, min_freq=2,reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = load_array(data_arrays, batch_size)\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:09.156283Z",
     "start_time": "2023-08-11T08:12:09.153353Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:10.677846Z",
     "start_time": "2023-08-11T08:12:10.674019Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data_nmt():\n",
    "    data_dir=r'F:\\study\\ml\\DataSet\\fra-eng'\n",
    "    with open(os.path.join(data_dir,'fra.txt'),'r',encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:11.334131Z",
     "start_time": "2023-08-11T08:12:11.329249Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_nmt(text,num_examples=None):\n",
    "    source,target=[],[]\n",
    "    for i ,line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts=line.split('\\t')\n",
    "        if len(parts)==2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source,target \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:12.268741Z",
     "start_time": "2023-08-11T08:12:12.264836Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_pad(line,num_steps,padding_token):\n",
    "    if len(line)>num_steps:\n",
    "        return line[:num_steps]\n",
    "    return line + [padding_token]*(num_steps-len(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:17.069184Z",
     "start_time": "2023-08-11T08:12:17.065278Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:17.863445Z",
     "start_time": "2023-08-11T08:12:17.858563Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:19.925254Z",
     "start_time": "2023-08-11T08:12:19.921244Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size,num_hiddens,num_layers,dropout=32,32,2,0.1\n",
    "batch_size,num_steps=64,10\n",
    "lr,num_epochs,device=0.005,1000,'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:27.785703Z",
     "start_time": "2023-08-11T08:12:20.833556Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, src_vocab, tgt_vocab =load_data_nmt(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:38.111849Z",
     "start_time": "2023-08-11T08:12:38.105991Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:12:39.172829Z",
     "start_time": "2023-08-11T08:12:39.168924Z"
    }
   },
   "outputs": [],
   "source": [
    "net = EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T08:13:46.522772Z",
     "start_time": "2023-08-11T08:12:42.313419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  10  train loss :  0.3442362562964593\n",
      "epoch :  20  train loss :  0.27560905253570034\n",
      "epoch :  30  train loss :  0.22989249152321337\n",
      "epoch :  40  train loss :  0.19750518082453053\n",
      "epoch :  50  train loss :  0.17358705619131456\n",
      "epoch :  60  train loss :  0.15516358058652843\n",
      "epoch :  70  train loss :  0.14060047131748826\n",
      "epoch :  80  train loss :  0.12888933399660113\n",
      "epoch :  90  train loss :  0.11929560823661102\n",
      "epoch :  100  train loss :  0.11134738116197183\n",
      "epoch :  110  train loss :  0.10461761652617727\n",
      "epoch :  120  train loss :  0.09886663732394366\n",
      "epoch :  130  train loss :  0.09392931859990068\n",
      "epoch :  140  train loss :  0.08962580277672982\n",
      "epoch :  150  train loss :  0.08582737309272301\n",
      "epoch :  160  train loss :  0.08246004346390845\n",
      "epoch :  170  train loss :  0.07948245154135598\n",
      "epoch :  180  train loss :  0.07681742339647453\n",
      "epoch :  190  train loss :  0.07440999248414464\n",
      "epoch :  200  train loss :  0.07221411476672535\n",
      "epoch :  210  train loss :  0.07022963155087936\n",
      "epoch :  220  train loss :  0.06841125965864632\n",
      "epoch :  230  train loss :  0.06674849538808941\n",
      "epoch :  240  train loss :  0.06523588416348787\n",
      "epoch :  250  train loss :  0.06382392532276995\n",
      "epoch :  260  train loss :  0.06251158543810943\n",
      "epoch :  270  train loss :  0.061313108363400565\n",
      "epoch :  280  train loss :  0.06017362311892187\n",
      "epoch :  290  train loss :  0.05911183772293724\n",
      "epoch :  300  train loss :  0.0581158151163928\n",
      "epoch :  310  train loss :  0.05719203352958251\n",
      "epoch :  320  train loss :  0.0563173726877323\n",
      "epoch :  330  train loss :  0.05550062130993029\n",
      "epoch :  340  train loss :  0.054727127059744085\n",
      "epoch :  350  train loss :  0.054006168084898276\n",
      "epoch :  360  train loss :  0.053328017511900104\n",
      "epoch :  370  train loss :  0.05268169936001142\n",
      "epoch :  380  train loss :  0.05206409325838069\n",
      "epoch :  390  train loss :  0.05147563174631837\n",
      "epoch :  400  train loss :  0.050917705123973006\n",
      "epoch :  410  train loss :  0.05039465067846101\n",
      "epoch :  420  train loss :  0.04989340338908451\n",
      "epoch :  430  train loss :  0.04941990190090803\n",
      "epoch :  440  train loss :  0.048969392427799116\n",
      "epoch :  450  train loss :  0.0485368169666145\n",
      "epoch :  460  train loss :  0.04811665366975403\n",
      "epoch :  470  train loss :  0.047708178555672096\n",
      "epoch :  480  train loss :  0.047316197896289776\n",
      "epoch :  490  train loss :  0.04694001059931015\n",
      "epoch :  500  train loss :  0.04657933538732394\n",
      "epoch :  510  train loss :  0.04623291817215932\n",
      "epoch :  520  train loss :  0.04589936621599253\n",
      "epoch :  530  train loss :  0.0455756473622922\n",
      "epoch :  540  train loss :  0.0452704075695169\n",
      "epoch :  550  train loss :  0.044972046610470905\n",
      "epoch :  560  train loss :  0.04468748253409345\n",
      "epoch :  570  train loss :  0.0444087044501414\n",
      "epoch :  580  train loss :  0.04413933124308591\n",
      "epoch :  590  train loss :  0.043883140533407604\n",
      "epoch :  600  train loss :  0.04363115687760824\n",
      "epoch :  610  train loss :  0.043383045598270865\n",
      "epoch :  620  train loss :  0.043144308418837395\n",
      "epoch :  630  train loss :  0.04291607573682838\n",
      "epoch :  640  train loss :  0.0426992353699017\n",
      "epoch :  650  train loss :  0.04248623619387264\n",
      "epoch :  660  train loss :  0.042276681315798835\n",
      "epoch :  670  train loss :  0.042073427427124936\n",
      "epoch :  680  train loss :  0.041886683150372826\n",
      "epoch :  690  train loss :  0.04169699279614887\n",
      "epoch :  700  train loss :  0.041511473354012965\n",
      "epoch :  710  train loss :  0.04133017675202231\n",
      "epoch :  720  train loss :  0.04115311807185707\n",
      "epoch :  730  train loss :  0.04098266049397603\n",
      "epoch :  740  train loss :  0.04081705357426088\n",
      "epoch :  750  train loss :  0.04065108649582681\n",
      "epoch :  760  train loss :  0.040488931969926284\n",
      "epoch :  770  train loss :  0.040329738849765255\n",
      "epoch :  780  train loss :  0.040175395656524615\n",
      "epoch :  790  train loss :  0.04002441048364731\n",
      "epoch :  800  train loss :  0.03988111230315923\n",
      "epoch :  810  train loss :  0.03974329192749087\n",
      "epoch :  820  train loss :  0.03960485207569946\n",
      "epoch :  830  train loss :  0.03947000590389728\n",
      "epoch :  840  train loss :  0.039341863536543335\n",
      "epoch :  850  train loss :  0.03921440612630028\n",
      "epoch :  860  train loss :  0.039089497011136586\n",
      "epoch :  870  train loss :  0.03896759287365316\n",
      "epoch :  880  train loss :  0.038845437484439464\n",
      "epoch :  890  train loss :  0.03872705401808479\n",
      "epoch :  900  train loss :  0.03861335596526691\n",
      "epoch :  910  train loss :  0.038500161626812156\n",
      "epoch :  920  train loss :  0.038389893159530855\n",
      "epoch :  930  train loss :  0.03828483896208794\n",
      "epoch :  940  train loss :  0.038180634765299835\n",
      "epoch :  950  train loss :  0.038080496870109545\n",
      "epoch :  960  train loss :  0.03798868242004108\n",
      "epoch :  970  train loss :  0.03789750377623703\n",
      "epoch :  980  train loss :  0.03780601827732267\n",
      "epoch :  990  train loss :  0.0377156924783832\n",
      "epoch :  1000  train loss :  0.03762961842233959\n"
     ]
    }
   ],
   "source": [
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net,src_sentence,src_vocab,tgt_vocab,num_steps,device,\n",
    "                    save_attention_weights=False):\n",
    "    net.eval()\n",
    "    src_tokens=src_vocab[src_sentence.lower().split(' ')]+[src_vocab['<eos>']]\n",
    "    enc_valid_len=torch.tensor([len(src_tokens)],device=device)\n",
    "    src_tokens=truncate_pad(src_tokens,num_steps,src_vocab['<pad>'])\n",
    "\n",
    "    enc_X=torch.unsqueeze(torch.tensor(src_tokens,dtype=torch.long,device=device),dim=0)\n",
    "    enc_outputs=net.encoder(enc_X,enc_valid_len)\n",
    "    dec_state=net.decoder.init_state(enc_outputs,enc_valid_len)\n",
    "\n",
    "    dec_X=torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']],dtype=torch.long,device=device),dim=0)\n",
    "    output_seq,attention_weight_seq=[],[]\n",
    "    for _ in range(num_steps):\n",
    "        Y,dec_state=net.decoder(dec_X,dec_state)\n",
    "        dec_X=Y.argmax(dim=2)\n",
    "        pred=dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        if pred==tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)),attention_weight_seq \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq,label_seq,k):\n",
    "    pred_tokens,label_tokens=pred_seq.split(' '),label_seq.split(' ')\n",
    "    len_pred,len_label=len(pred_tokens),len(label_tokens)\n",
    "    score=math.exp(min(0,1-len_label/len_pred))\n",
    "    for n in range(1,k+1):\n",
    "        num_matches,label_subs=0,collections.defaultdict(int)\n",
    "        for i in range(len_label-n+1):\n",
    "            label_subs[' '.join(label_tokens[i:i+n])]+=1\n",
    "        for i in range(len_pred-n+1):\n",
    "            if label_subs[' '.join(pred_tokens[i:i+n])]>0:\n",
    "                num_matches+=1\n",
    "                label_subs[' '.join(pred_tokens[i:i+n])]-=1\n",
    "        score *=math.pow(num_matches/(len_pred-n+1),math.pow(0.5,n))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => entrez ! va, bleu 0.0000\n",
      "i lost . => j'ai j'ai j'ai j'ai j'ai j'ai j'ai j'ai j'ai j'ai, bleu 0.0000\n",
      "he's calm . => je je je je je je je je je je, bleu 0.0000\n",
      "i'm home . => je j'ai comment je j'ai comment je j'ai comment je, bleu 0.0000\n"
     ]
    }
   ],
   "source": [
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

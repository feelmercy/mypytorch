{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3213cfd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:47.529828Z",
     "start_time": "2023-02-13T13:05:46.560792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn,optim\n",
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6733ddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:48.064514Z",
     "start_time": "2023-02-13T13:05:48.058514Z"
    }
   },
   "outputs": [],
   "source": [
    "content_path=r'E:\\Study\\ml\\dataset\\LM\\13\\rainier.jpg'\n",
    "style_path=r'E:\\Study\\ml\\dataset\\LM\\13\\autumn-oak.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5370b13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:48.687817Z",
     "start_time": "2023-02-13T13:05:48.588729Z"
    }
   },
   "outputs": [],
   "source": [
    "content_img=plt.imread(content_path)\n",
    "style_img=plt.imread(style_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01061f52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:49.289073Z",
     "start_time": "2023-02-13T13:05:49.092070Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(12)\n",
    "plt.subplot(121)\n",
    "plt.imshow(content_img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(style_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c37e57e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:52.763756Z",
     "start_time": "2023-02-13T13:05:52.747754Z"
    }
   },
   "outputs": [],
   "source": [
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345a8e14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:53.229521Z",
     "start_time": "2023-02-13T13:05:53.218521Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(img,image_shape):\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean,std=rgb_std)\n",
    "    ])\n",
    "    return transforms(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b34be0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:53.952758Z",
     "start_time": "2023-02-13T13:05:53.946795Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess(img):\n",
    "    img=img[0].to(rgb_std.device)\n",
    "    img=torch.clamp(img.permute(1,2,0)*rgb_std+rgb_mean,0,1)\n",
    "    return torchvision.transforms.ToPILImage()(img.permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68687546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:56.297322Z",
     "start_time": "2023-02-13T13:05:55.634552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "pretrained_net=torchvision.models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda1b0cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:05:59.242809Z",
     "start_time": "2023-02-13T13:05:59.227891Z"
    }
   },
   "outputs": [],
   "source": [
    "style_layers,content_layers=[0,5,10,19,28],[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8193e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:06:03.596210Z",
     "start_time": "2023-02-13T13:06:03.580128Z"
    }
   },
   "outputs": [],
   "source": [
    "net=nn.Sequential(*[pretrained_net.features[i] for i in \n",
    "                   range(max(content_layers+style_layers)+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba14779a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:06:23.673306Z",
     "start_time": "2023-02-13T13:06:23.660296Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(X,content_layers,style_layers):\n",
    "    contents=[]\n",
    "    styles=[]\n",
    "    for i in range(len(net)):\n",
    "        X=net[i](X)\n",
    "        if i in style_layers:\n",
    "            styles.append(X)\n",
    "        if i in content_layers:\n",
    "            contents.append(X)\n",
    "    return contents,styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b8ea1b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:06:24.290075Z",
     "start_time": "2023-02-13T13:06:24.286079Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_contents(image_shape,device):\n",
    "    content_X=preprocess(content_img,image_shape).to(device)\n",
    "    contents_Y,_=extract_features(content_X,content_layers,style_layers)\n",
    "    return content_X,contents_Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e527f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:06:27.089262Z",
     "start_time": "2023-02-13T13:06:27.084264Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_styles(image_shape,device):\n",
    "    style_X=preprocess(style_img,image_shape).to(device)\n",
    "    _,styles_Y=extract_features(style_X,content_layers,style_layers)\n",
    "    return style_X,styles_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9a2ed",
   "metadata": {},
   "source": [
    "#### content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4944d70f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:06:37.915977Z",
     "start_time": "2023-02-13T13:06:37.910980Z"
    }
   },
   "outputs": [],
   "source": [
    "def content_loss(Y_hat,Y):\n",
    "    return torch.square(Y_hat-Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699994d",
   "metadata": {},
   "source": [
    "#### style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e019e702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:06:48.717268Z",
     "start_time": "2023-02-13T13:06:48.700644Z"
    }
   },
   "outputs": [],
   "source": [
    "def style_loss(Y_hat,gram_Y):\n",
    "    return torch.square(gram(Y_hat)-gram_Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f491b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:21:33.068260Z",
     "start_time": "2023-02-13T13:21:33.051261Z"
    }
   },
   "outputs": [],
   "source": [
    "def tv_loss(Y_hat):\n",
    "    return 0.5 * (torch.abs(Y_hat[:,:,1:,:]-Y_hat[:,:,-1,:]).mean()+\n",
    "                 torch.abs(Y_hat[:,:,:,1:]-Y_hat[:,:,:,:-1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b91d4f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:22:11.992048Z",
     "start_time": "2023-02-13T13:22:11.974949Z"
    }
   },
   "outputs": [],
   "source": [
    "content_weight,style_weight,tv_weight=1,1e3,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31f1b5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:28:30.960869Z",
     "start_time": "2023-02-13T13:28:30.944865Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(X,contents_Y_hat,styles_Y_hat,contents_Y,styles_Y_gram):\n",
    "    contents_l=[content_loss(Y_hat,Y) * content_weight for Y_hat,Y in\n",
    "               zip(contents_Y_hat,contents_Y)]\n",
    "    styles_l=[style_loss(Y_hat,Y) * style_weight for Y_hat,Y in \n",
    "             zip(styles_Y_hat,styles_Y_gram)]\n",
    "    tv_l=tv_loss(X) * tv_weight\n",
    "    l=sum(10*styles_l+contents_l+[tv_l])\n",
    "    return contents_l,styles_l,tv_l,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "983b0a95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:31:27.987529Z",
     "start_time": "2023-02-13T13:31:27.980525Z"
    }
   },
   "outputs": [],
   "source": [
    "class SynthesizedImage(nn.Module):\n",
    "    def __init__(self,img_shape,**kwargs):\n",
    "        super(SynthesizedImage,self).__init__(**kwargs)\n",
    "        self.weight=nn.Parameter(torch.rand(*img_shape))\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1d7e096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T13:52:40.785113Z",
     "start_time": "2023-02-13T13:52:40.776113Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_inits(X,device,lr,styles_Y):\n",
    "    gen_img=SynthesizedImage(X.shape).to(device)\n",
    "    gen_img.weight.data.copy_(X.data)\n",
    "    trainer=torch.optim.Adam(gen_img.parameters(),lr=lr)\n",
    "    styles_Y_gram=[gram(Y) for Y in styles_Y]\n",
    "    return gen_img(),styles_Y_gram,trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12d2b506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T14:21:11.146239Z",
     "start_time": "2023-02-13T14:21:11.128345Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(X,contents_Y,styles_Y,device,lr,num_epochs,lr_decay_epoch):\n",
    "    X,styles_Y_gram,trainer=get_inits(X,device,lr,styles_Y)\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR(trainer,lr_decay_epoch,0.8)\n",
    "    for epoch in range(num_epochs):\n",
    "        trainer.zero_grad()\n",
    "        contents_Y_hat,styles_Y_hat=extract_features(\n",
    "        X,content_layers,style_layers)\n",
    "        contents_l,styles_l,tv_l,l=compute_loss(\n",
    "        X,contents_Y_hat,styles_Y_hat,contens_Y,style_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        scheduler.step()\n",
    "        if(epoch+1) %10 ==0:\n",
    "            printf(\"contens loss: %.2f\",float(sum(contents_l)))\n",
    "            printf(\"styles loss: %.2f\",float(sum(styles_l)))\n",
    "            printf(\"tv loss: %.2f\",float(sum(tv_l)))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7988e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad12e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549cb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e24eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54408289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e85d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:23.832138Z",
     "start_time": "2022-04-19T06:35:22.378953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch import optim,nn\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm,skew,kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:24.472719Z",
     "start_time": "2022-04-19T06:35:24.404711Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r'F:\\study\\ml\\HonyiLee2020\\hw1\\data\\train.csv',encoding='big5')\n",
    "test_data=pd.read_csv(r'F:\\study\\ml\\HonyiLee2020\\hw1\\data\\test.csv',encoding='big5',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:24.719250Z",
     "start_time": "2022-04-19T06:35:24.711249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape:  (4320, 27)\n",
      "test_data shape:  (4320, 11)\n"
     ]
    }
   ],
   "source": [
    "print('train_data shape: ',train_data.shape)\n",
    "print('test_data shape: ',test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:24.991285Z",
     "start_time": "2022-04-19T06:35:24.971783Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data1=train_data.replace('NR',0)\n",
    "train_data1=train_data1.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:25.363332Z",
     "start_time": "2022-04-19T06:35:25.202312Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_data={}\n",
    "for m in range(12):\n",
    "    month_data=np.empty((18,20*24))\n",
    "    for d in range(20):\n",
    "        month_data[:,d*24:(d+1)*24]=train_data1.iloc[m*20*18+d*18:m*20*18+(d+1)*18,:]\n",
    "    year_data[m]=month_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:25.427840Z",
     "start_time": "2022-04-19T06:35:25.418339Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_data1=year_data.copy()\n",
    "month_list=[]\n",
    "for i in range(12):\n",
    "    month_list.append(pd.DataFrame(year_data1[i]))\n",
    "all_train_data1=pd.concat(month_list,axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:25.680873Z",
     "start_time": "2022-04-19T06:35:25.674872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.527413194444442"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data1.iloc[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:25.879398Z",
     "start_time": "2022-04-19T06:35:25.874897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 5760)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data1.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:26.129430Z",
     "start_time": "2022-04-19T06:35:26.082424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " month :  0\n",
      " month :  1\n",
      " month :  2\n",
      "---------- pm2.5 start : \n",
      " -3.1 \n",
      "\n",
      "-8.0\n",
      "-7.2\n",
      "-6.8\n",
      "-6.5\n",
      "-7.1\n",
      "-7.4\n",
      "-8.1\n",
      "-8.3\n",
      "-8.4\n",
      "-9.3\n",
      "-10.6\n",
      "-11.2\n",
      "-12.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-12.3\n",
      "pm2.5 end ------------\n",
      "correct value between  : 19.0 18.0\n",
      " month :  3\n",
      " month :  4\n",
      " month :  5\n",
      " month :  6\n",
      " month :  7\n",
      " month :  8\n",
      " month :  9\n",
      " month :  10\n",
      " month :  11\n"
     ]
    }
   ],
   "source": [
    "# temperature always >=0 in Taiwan and change fast when temperature <=0 ,so correct it\n",
    "for idx in [0]:\t\n",
    "\tfor m in range(12):\n",
    "\t\tprint(' month : ',m)\n",
    "\t\ti=0\n",
    "\t\twhile i<480:\n",
    "\t\t\tif year_data1[m][idx,i] <=0:\n",
    "\t\t\t\tprint('---------- pm2.5 start : \\n',year_data1[m][idx,i],'\\n')\n",
    "\t\t\t\tfor k in range(30):\n",
    "\t\t\t\t\tif k+i+1>479:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif year_data1[m][idx,k+i+1] >0:\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tprint(year_data1[m][idx,k+i+1])\n",
    "\t\t\t\ti=i+k\n",
    "\t\t\t\tprint('pm2.5 end ------------')\n",
    "\t\t\t\tprint('correct value between  :',year_data1[m][idx,i-1-k],year_data1[m][idx,i+1])\n",
    "\t\t\t\t## add correct to mean value\n",
    "\t\t\t\tyear_data1[m][idx,i-k:i+1]=(year_data1[m][idx,i-1-k]+year_data1[m][idx,i+1])/2\n",
    "\t\t\ti=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:26.419966Z",
     "start_time": "2022-04-19T06:35:26.303952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " month :  0\n",
      " month :  1\n",
      " month :  2\n",
      " month :  3\n",
      " month :  4\n",
      " month :  5\n",
      " month :  6\n",
      " month :  7\n",
      " month :  8\n",
      " month :  9\n",
      " month :  10\n",
      " month :  11\n",
      "---------- pm2.5 start : \n",
      " -0.2 \n",
      "\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "pm2.5 end ------------\n",
      "correct value between  : 1.8 1.8\n",
      " month :  0\n",
      " month :  1\n",
      " month :  2\n",
      " month :  3\n",
      " month :  4\n",
      " month :  5\n",
      " month :  6\n",
      " month :  7\n",
      " month :  8\n",
      " month :  9\n",
      " month :  10\n",
      " month :  11\n",
      "---------- pm2.5 start : \n",
      " -0.12 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.34 0.26\n",
      " month :  0\n",
      " month :  1\n",
      " month :  2\n",
      " month :  3\n",
      " month :  4\n",
      " month :  5\n",
      " month :  6\n",
      " month :  7\n",
      " month :  8\n",
      "---------- pm2.5 start : \n",
      " -0.1 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 2.5 0.3\n",
      "---------- pm2.5 start : \n",
      " -0.1 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.1 1.0\n",
      " month :  9\n",
      " month :  10\n",
      " month :  11\n",
      "---------- pm2.5 start : \n",
      " -1.1 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 6.6 1.2\n",
      " month :  0\n",
      " month :  1\n",
      " month :  2\n",
      " month :  3\n",
      " month :  4\n",
      "---------- pm2.5 start : \n",
      " -0.9 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 17.0 17.0\n",
      " month :  5\n",
      " month :  6\n",
      " month :  7\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 7.4 10.0\n",
      " month :  8\n",
      " month :  9\n",
      " month :  10\n",
      "---------- pm2.5 start : \n",
      " -2.4 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 7.0 6.6\n",
      " month :  11\n",
      " month :  0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 46.0 51.0\n",
      " month :  1\n",
      " month :  2\n",
      " month :  3\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 6.0 16.0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 13.0 16.0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 55.0 48.0\n",
      " month :  4\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 5.0 1.0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 2.0 4.0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 5.0 0.0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 10.0 9.0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 9.0 2.0\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.0 4.0\n",
      " month :  5\n",
      " month :  6\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 20.0 27.0\n",
      " month :  7\n",
      " month :  8\n",
      " month :  9\n",
      " month :  10\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 18.0 18.0\n",
      " month :  11\n",
      "---------- pm2.5 start : \n",
      " -1.0 \n",
      "\n",
      "-1.0\n",
      "pm2.5 end ------------\n",
      "correct value between  : 13.0 7.0\n",
      " month :  0\n",
      "---------- pm2.5 start : \n",
      " -0.9 \n",
      "\n",
      "-0.9\n",
      "-0.9\n",
      "pm2.5 end ------------\n",
      "correct value between  : 1.6 1.3\n",
      "---------- pm2.5 start : \n",
      " -0.1 \n",
      "\n",
      "-0.1\n",
      "-0.2\n",
      "-0.3\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.1 1.2\n",
      " month :  1\n",
      "---------- pm2.5 start : \n",
      " -0.2 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 1.1 0.3\n",
      " month :  2\n",
      " month :  3\n",
      " month :  4\n",
      " month :  5\n",
      "---------- pm2.5 start : \n",
      " -1.5 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.2 2.3\n",
      "---------- pm2.5 start : \n",
      " -0.3 \n",
      "\n",
      "-0.2\n",
      "pm2.5 end ------------\n",
      "correct value between  : 3.8 2.2\n",
      " month :  6\n",
      " month :  7\n",
      "---------- pm2.5 start : \n",
      " -1.6 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 4.3 2.7\n",
      " month :  8\n",
      " month :  9\n",
      " month :  10\n",
      "---------- pm2.5 start : \n",
      " -0.2 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.0 0.9\n",
      "---------- pm2.5 start : \n",
      " -0.1 \n",
      "\n",
      "-0.3\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.2 0.3\n",
      " month :  11\n",
      "---------- pm2.5 start : \n",
      " -0.2 \n",
      "\n",
      "-0.4\n",
      "-0.5\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.0 1.2\n",
      "---------- pm2.5 start : \n",
      " -0.3 \n",
      "\n",
      "-0.3\n",
      "pm2.5 end ------------\n",
      "correct value between  : 0.0 1.1\n",
      "---------- pm2.5 start : \n",
      " -0.2 \n",
      "\n",
      "pm2.5 end ------------\n",
      "correct value between  : 2.8 2.0\n",
      " month :  0\n",
      " month :  1\n",
      " month :  2\n",
      " month :  3\n",
      " month :  4\n",
      " month :  5\n",
      " month :  6\n",
      " month :  7\n",
      " month :  8\n",
      " month :  9\n",
      " month :  10\n",
      " month :  11\n",
      "---------- pm2.5 start : \n",
      " -0.2 \n",
      "\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "-0.2\n",
      "pm2.5 end ------------\n",
      "correct value between  : 2.0 1.9\n"
     ]
    }
   ],
   "source": [
    "for idx in [1,2,4,6,9,12,13]:\t\n",
    "\tfor m in range(12):\n",
    "\t\tprint(' month : ',m)\n",
    "\t\ti=0\n",
    "\t\twhile i<480:\n",
    "\t\t\tif year_data1[m][idx,i] <0:\n",
    "\t\t\t\tprint('---------- pm2.5 start : \\n',year_data1[m][idx,i],'\\n')\n",
    "\t\t\t\tfor k in range(30):\n",
    "\t\t\t\t\tif k+i+1>479:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif year_data1[m][idx,k+i+1] >=0:\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tprint(year_data1[m][idx,k+i+1])\n",
    "\t\t\t\ti=i+k\n",
    "\t\t\t\tprint('pm2.5 end ------------')\n",
    "\t\t\t\tprint('correct value between  :',year_data1[m][idx,i-1-k],year_data1[m][idx,i+1])\n",
    "\t\t\t\t## add correct to mean value\n",
    "\t\t\t\tyear_data1[m][idx,i-k:i+1]=(year_data1[m][idx,i-1-k]+year_data1[m][idx,i+1])/2\n",
    "\t\t\ti=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:26.430468Z",
     "start_time": "2022-04-19T06:35:26.425467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14.00\n",
       "1     1.80\n",
       "2     0.51\n",
       "3     0.20\n",
       "4     0.90\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data1.iloc[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:26.881025Z",
     "start_time": "2022-04-19T06:35:26.780512Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_data2=all_train_data1.apply(lambda x : (x-x.mean())/x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:27.020043Z",
     "start_time": "2022-04-19T06:35:26.955534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.527413</td>\n",
       "      <td>1.702396</td>\n",
       "      <td>0.388363</td>\n",
       "      <td>0.140427</td>\n",
       "      <td>2.135729</td>\n",
       "      <td>10.125990</td>\n",
       "      <td>12.247726</td>\n",
       "      <td>31.905469</td>\n",
       "      <td>42.709201</td>\n",
       "      <td>21.414236</td>\n",
       "      <td>0.200625</td>\n",
       "      <td>73.229167</td>\n",
       "      <td>2.763125</td>\n",
       "      <td>1.839653</td>\n",
       "      <td>156.329271</td>\n",
       "      <td>158.482795</td>\n",
       "      <td>2.297240</td>\n",
       "      <td>1.712760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.290152</td>\n",
       "      <td>0.125265</td>\n",
       "      <td>0.323573</td>\n",
       "      <td>0.104645</td>\n",
       "      <td>2.282155</td>\n",
       "      <td>6.187555</td>\n",
       "      <td>7.577133</td>\n",
       "      <td>18.703486</td>\n",
       "      <td>26.222292</td>\n",
       "      <td>16.662537</td>\n",
       "      <td>2.045443</td>\n",
       "      <td>13.361351</td>\n",
       "      <td>1.816940</td>\n",
       "      <td>0.181839</td>\n",
       "      <td>95.745881</td>\n",
       "      <td>94.697432</td>\n",
       "      <td>1.065408</td>\n",
       "      <td>1.062683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12.300000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>-1.600000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>121.500000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>29.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.570000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  5760.000000  5760.000000  5760.000000  5760.000000  5760.000000   \n",
       "mean     22.527413     1.702396     0.388363     0.140427     2.135729   \n",
       "std       6.290152     0.125265     0.323573     0.104645     2.282155   \n",
       "min     -12.300000    -0.200000    -0.120000     0.000000    -1.100000   \n",
       "25%      18.000000     1.600000     0.250000     0.070000     1.100000   \n",
       "50%      23.000000     1.700000     0.340000     0.110000     1.600000   \n",
       "75%      27.000000     1.800000     0.450000     0.180000     2.300000   \n",
       "max      36.000000     2.000000     7.570000     1.300000    31.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  5760.000000  5760.000000  5760.000000  5760.000000  5760.000000   \n",
       "mean     10.125990    12.247726    31.905469    42.709201    21.414236   \n",
       "std       6.187555     7.577133    18.703486    26.222292    16.662537   \n",
       "min       0.000000    -2.400000     0.000000     0.000000    -1.000000   \n",
       "25%       5.900000     7.300000    18.000000    24.000000     9.000000   \n",
       "50%       8.600000    10.000000    29.000000    38.000000    18.000000   \n",
       "75%      13.000000    15.000000    42.000000    57.000000    29.250000   \n",
       "max      46.000000    71.000000   231.000000   181.000000   112.000000   \n",
       "\n",
       "                10           11           12           13           14  \\\n",
       "count  5760.000000  5760.000000  5760.000000  5760.000000  5760.000000   \n",
       "mean      0.200625    73.229167     2.763125     1.839653   156.329271   \n",
       "std       2.045443    13.361351     1.816940     0.181839    95.745881   \n",
       "min       0.000000    29.000000    -1.600000    -0.200000     0.100000   \n",
       "25%       0.000000    64.000000     1.600000     1.700000    72.000000   \n",
       "50%       0.000000    75.000000     2.300000     1.800000   119.000000   \n",
       "75%       0.000000    84.000000     3.400000     1.900000   213.000000   \n",
       "max      74.000000    99.000000    22.000000     3.000000   360.000000   \n",
       "\n",
       "                15           16           17  \n",
       "count  5760.000000  5760.000000  5760.000000  \n",
       "mean    158.482795     2.297240     1.712760  \n",
       "std      94.697432     1.065408     1.062683  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%      76.000000     1.500000     0.900000  \n",
       "50%     121.500000     2.100000     1.500000  \n",
       "75%     219.000000     2.900000     2.300000  \n",
       "max     360.000000     7.700000     7.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:27.234070Z",
     "start_time": "2022-04-19T06:35:27.225569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.527413194444442"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data1.iloc[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:27.439096Z",
     "start_time": "2022-04-19T06:35:27.433595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1.355677\n",
       "1   -1.355677\n",
       "2   -1.355677\n",
       "3   -1.514655\n",
       "4   -1.673634\n",
       "5   -1.673634\n",
       "6   -1.673634\n",
       "7   -1.673634\n",
       "8   -1.196698\n",
       "9   -0.878741\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data2.iloc[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:27.732633Z",
     "start_time": "2022-04-19T06:35:27.651623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "      <td>5.760000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.921351e-15</td>\n",
       "      <td>-4.132630e-13</td>\n",
       "      <td>-1.393800e-14</td>\n",
       "      <td>5.078230e-15</td>\n",
       "      <td>3.474738e-15</td>\n",
       "      <td>-2.338465e-15</td>\n",
       "      <td>-3.438183e-15</td>\n",
       "      <td>-4.431640e-16</td>\n",
       "      <td>2.485088e-16</td>\n",
       "      <td>1.855094e-16</td>\n",
       "      <td>4.130921e-15</td>\n",
       "      <td>-9.199431e-16</td>\n",
       "      <td>5.026265e-16</td>\n",
       "      <td>3.205650e-13</td>\n",
       "      <td>8.574353e-17</td>\n",
       "      <td>5.245418e-16</td>\n",
       "      <td>5.851993e-16</td>\n",
       "      <td>1.040902e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.536816e+00</td>\n",
       "      <td>-1.518692e+01</td>\n",
       "      <td>-1.571093e+00</td>\n",
       "      <td>-1.341940e+00</td>\n",
       "      <td>-1.417839e+00</td>\n",
       "      <td>-1.636509e+00</td>\n",
       "      <td>-1.933149e+00</td>\n",
       "      <td>-1.705857e+00</td>\n",
       "      <td>-1.628736e+00</td>\n",
       "      <td>-1.345187e+00</td>\n",
       "      <td>-9.808389e-02</td>\n",
       "      <td>-3.310232e+00</td>\n",
       "      <td>-2.401358e+00</td>\n",
       "      <td>-1.121682e+01</td>\n",
       "      <td>-1.631707e+00</td>\n",
       "      <td>-1.673570e+00</td>\n",
       "      <td>-2.156207e+00</td>\n",
       "      <td>-1.611732e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.197621e-01</td>\n",
       "      <td>-8.174309e-01</td>\n",
       "      <td>-4.276098e-01</td>\n",
       "      <td>-6.730107e-01</td>\n",
       "      <td>-4.538382e-01</td>\n",
       "      <td>-6.829821e-01</td>\n",
       "      <td>-6.529812e-01</td>\n",
       "      <td>-7.434694e-01</td>\n",
       "      <td>-7.134846e-01</td>\n",
       "      <td>-7.450388e-01</td>\n",
       "      <td>-9.808389e-02</td>\n",
       "      <td>-6.907360e-01</td>\n",
       "      <td>-6.401558e-01</td>\n",
       "      <td>-7.680033e-01</td>\n",
       "      <td>-8.807613e-01</td>\n",
       "      <td>-8.710141e-01</td>\n",
       "      <td>-7.482952e-01</td>\n",
       "      <td>-7.648192e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.513122e-02</td>\n",
       "      <td>-1.912605e-02</td>\n",
       "      <td>-1.494652e-01</td>\n",
       "      <td>-2.907653e-01</td>\n",
       "      <td>-2.347470e-01</td>\n",
       "      <td>-2.466224e-01</td>\n",
       "      <td>-2.966459e-01</td>\n",
       "      <td>-1.553437e-01</td>\n",
       "      <td>-1.795877e-01</td>\n",
       "      <td>-2.049049e-01</td>\n",
       "      <td>-9.808389e-02</td>\n",
       "      <td>1.325340e-01</td>\n",
       "      <td>-2.548928e-01</td>\n",
       "      <td>-2.180656e-01</td>\n",
       "      <td>-3.898786e-01</td>\n",
       "      <td>-3.905364e-01</td>\n",
       "      <td>-1.851306e-01</td>\n",
       "      <td>-2.002106e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.110459e-01</td>\n",
       "      <td>7.791788e-01</td>\n",
       "      <td>1.904894e-01</td>\n",
       "      <td>3.781641e-01</td>\n",
       "      <td>7.198057e-02</td>\n",
       "      <td>4.644824e-01</td>\n",
       "      <td>3.632342e-01</td>\n",
       "      <td>5.397139e-01</td>\n",
       "      <td>5.449866e-01</td>\n",
       "      <td>4.702623e-01</td>\n",
       "      <td>-9.808389e-02</td>\n",
       "      <td>8.061186e-01</td>\n",
       "      <td>3.505206e-01</td>\n",
       "      <td>3.318721e-01</td>\n",
       "      <td>5.918869e-01</td>\n",
       "      <td>6.390586e-01</td>\n",
       "      <td>5.657555e-01</td>\n",
       "      <td>5.526008e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.141854e+00</td>\n",
       "      <td>2.375788e+00</td>\n",
       "      <td>2.219482e+01</td>\n",
       "      <td>1.108104e+01</td>\n",
       "      <td>1.264781e+01</td>\n",
       "      <td>5.797768e+00</td>\n",
       "      <td>7.753892e+00</td>\n",
       "      <td>1.064478e+01</td>\n",
       "      <td>5.273788e+00</td>\n",
       "      <td>5.436493e+00</td>\n",
       "      <td>3.607990e+01</td>\n",
       "      <td>1.928760e+00</td>\n",
       "      <td>1.058751e+01</td>\n",
       "      <td>6.381187e+00</td>\n",
       "      <td>2.127201e+00</td>\n",
       "      <td>2.128011e+00</td>\n",
       "      <td>5.071072e+00</td>\n",
       "      <td>4.975368e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  5.760000e+03  5.760000e+03  5.760000e+03  5.760000e+03  5.760000e+03   \n",
       "mean   2.921351e-15 -4.132630e-13 -1.393800e-14  5.078230e-15  3.474738e-15   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -5.536816e+00 -1.518692e+01 -1.571093e+00 -1.341940e+00 -1.417839e+00   \n",
       "25%   -7.197621e-01 -8.174309e-01 -4.276098e-01 -6.730107e-01 -4.538382e-01   \n",
       "50%    7.513122e-02 -1.912605e-02 -1.494652e-01 -2.907653e-01 -2.347470e-01   \n",
       "75%    7.110459e-01  7.791788e-01  1.904894e-01  3.781641e-01  7.198057e-02   \n",
       "max    2.141854e+00  2.375788e+00  2.219482e+01  1.108104e+01  1.264781e+01   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  5.760000e+03  5.760000e+03  5.760000e+03  5.760000e+03  5.760000e+03   \n",
       "mean  -2.338465e-15 -3.438183e-15 -4.431640e-16  2.485088e-16  1.855094e-16   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.636509e+00 -1.933149e+00 -1.705857e+00 -1.628736e+00 -1.345187e+00   \n",
       "25%   -6.829821e-01 -6.529812e-01 -7.434694e-01 -7.134846e-01 -7.450388e-01   \n",
       "50%   -2.466224e-01 -2.966459e-01 -1.553437e-01 -1.795877e-01 -2.049049e-01   \n",
       "75%    4.644824e-01  3.632342e-01  5.397139e-01  5.449866e-01  4.702623e-01   \n",
       "max    5.797768e+00  7.753892e+00  1.064478e+01  5.273788e+00  5.436493e+00   \n",
       "\n",
       "                 10            11            12            13            14  \\\n",
       "count  5.760000e+03  5.760000e+03  5.760000e+03  5.760000e+03  5.760000e+03   \n",
       "mean   4.130921e-15 -9.199431e-16  5.026265e-16  3.205650e-13  8.574353e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -9.808389e-02 -3.310232e+00 -2.401358e+00 -1.121682e+01 -1.631707e+00   \n",
       "25%   -9.808389e-02 -6.907360e-01 -6.401558e-01 -7.680033e-01 -8.807613e-01   \n",
       "50%   -9.808389e-02  1.325340e-01 -2.548928e-01 -2.180656e-01 -3.898786e-01   \n",
       "75%   -9.808389e-02  8.061186e-01  3.505206e-01  3.318721e-01  5.918869e-01   \n",
       "max    3.607990e+01  1.928760e+00  1.058751e+01  6.381187e+00  2.127201e+00   \n",
       "\n",
       "                 15            16            17  \n",
       "count  5.760000e+03  5.760000e+03  5.760000e+03  \n",
       "mean   5.245418e-16  5.851993e-16  1.040902e-15  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -1.673570e+00 -2.156207e+00 -1.611732e+00  \n",
       "25%   -8.710141e-01 -7.482952e-01 -7.648192e-01  \n",
       "50%   -3.905364e-01 -1.851306e-01 -2.002106e-01  \n",
       "75%    6.390586e-01  5.657555e-01  5.526008e-01  \n",
       "max    2.128011e+00  5.071072e+00  4.975368e+00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:27.974164Z",
     "start_time": "2022-04-19T06:35:27.876651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:28.093679Z",
     "start_time": "2022-04-19T06:35:28.090679Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.empty((471*12,18*9))\n",
    "y=np.empty((471*12,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:29.256327Z",
     "start_time": "2022-04-19T06:35:28.327709Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in range(12):\n",
    "    for g in range(471):\n",
    "        x[m*471+g:m*471+g+1,:]=all_train_data2.iloc[m*480+g:m*480+g+9,:].values.reshape(1,-1)\n",
    "        y[m*471+g:g*471+g+1,:]=all_train_data2.iloc[m*480+g+9:m*480+g+9+1,9].values.reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:29.263828Z",
     "start_time": "2022-04-19T06:35:29.257827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5152735 , -0.09808389])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[9,9:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:29.964417Z",
     "start_time": "2022-04-19T06:35:29.959916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5152735 ],\n",
       "       [1.17543706]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:30.201447Z",
     "start_time": "2022-04-19T06:35:30.194946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    1.175437\n",
       "Name: 9, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data2.iloc[10:11,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:30.448478Z",
     "start_time": "2022-04-19T06:35:30.443477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17958771,  0.5152735 , -0.09808389, -1.28947789,  6.73487971,\n",
       "        0.88180982, -1.15231349, -0.05789803, -1.40532054, -1.32942776])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:30.952042Z",
     "start_time": "2022-04-19T06:35:30.946041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5152735])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:31.999675Z",
     "start_time": "2022-04-19T06:35:31.979672Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=10,shuffle=True,random_state=42)\n",
    "\n",
    "def rmse(y,y_hat):\n",
    "    return np.sqrt(mean_squared_error(y,y_hat))\n",
    "\n",
    "def cv_rmse(model,x=x,y=y):\n",
    "    rmse=np.sqrt( -cross_val_score(model,x,y,scoring='neg_mean_squared_error',cv=kf) )\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:32.502739Z",
     "start_time": "2022-04-19T06:35:32.493238Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:32.950296Z",
     "start_time": "2022-04-19T06:35:32.945795Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge=make_pipeline(RobustScaler(),RidgeCV(alphas=alphas_alt,cv=kf))\n",
    "lasso=make_pipeline(RobustScaler(),LassoCV(max_iter=1e7,alphas=alphas2,random_state=42,cv=kf))\n",
    "elasticnet=make_pipeline(RobustScaler(),ElasticNetCV(max_iter=1e7,alphas=e_alphas,cv=kf,l1_ratio=e_l1ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:35:54.545538Z",
     "start_time": "2022-04-19T06:35:33.379850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: 0.3776 (0.0248)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score=cv_rmse(ridge)\n",
    "print(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:36:21.642479Z",
     "start_time": "2022-04-19T06:35:54.547538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO: 0.3759 (0.0246)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = cv_rmse(lasso)\n",
    "print(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:15:29.808948Z",
     "start_time": "2022-04-19T02:13:39.873988Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elastic net: 0.3760 (0.0247)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = cv_rmse(elasticnet)\n",
    "print(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:16:07.405222Z",
     "start_time": "2022-04-19T02:16:07.399722Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ linear model with  kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:36:56.615420Z",
     "start_time": "2022-04-19T06:36:56.524908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_k_fold_data(net,num_epochs,lr,train_features,train_labels,test_features,test_labels,batch_size,montum,wd):\n",
    "#     net=nn.Linear(train_features.shape[1],1)\n",
    "#     net=nn.Sequential(nn.Linear(train_features.shape[1],1))\n",
    "    loss=nn.MSELoss()\n",
    "#     optimizer=optim.SGD(net.parameters(),lr=lr,momentum=montum,weight_decay=wd)\n",
    "    optimizer=optim.Adam(net.parameters(),lr=lr,weight_decay=wd)\n",
    "    dataset=torch.utils.data.TensorDataset(train_features,train_labels)\n",
    "    data_iter=torch.utils.data.DataLoader(dataset,batch_size,shuffle=True)\n",
    "    train_l,test_l=[],[]\n",
    "    \n",
    "    min_test_loss=1000\n",
    "    early_stop_cnt=0\n",
    "    train_loss,test_loss=0,0\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        net.train()\n",
    "        for x,y in data_iter:\n",
    "            l=loss(net(x),y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "#         if (e+1) %1000==0 and test_features is not  None:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss=loss(net(test_features),test_labels).item()\n",
    "            if test_loss<min_test_loss:\n",
    "                min_test_loss=test_loss\n",
    "#                 test_l.append(test_loss)\n",
    "                train_loss=loss(net(train_features),train_labels).item()\n",
    "                test_l.append(test_loss)\n",
    "                train_l.append(train_loss)\n",
    "                print('epoch = %d train_loss : %f , test loss : %f' % (e+1,train_loss,test_loss))\n",
    "                early_stop_cnt=0\n",
    "            else:\n",
    "                early_stop_cnt+=1\n",
    "        if early_stop_cnt > 500:\n",
    "            \n",
    "            break\n",
    "                \n",
    "#             net.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 train_l.append(loss(net(train_features),train_labels).item())\n",
    "#                 test_l.append(loss(net(test_features),test_labels).item())\n",
    "# #                 print('epoch ',(e+1),'train loss : ',train_l[-1],'test loss : ',test_l[-1])\n",
    "\n",
    "    return train_l,test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:36:57.208495Z",
     "start_time": "2022-04-19T06:36:57.164490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kfold_data(k,j,x,y,random_state=13):\n",
    "    assert k>=1, 'k must >=1'\n",
    "    fold_size = x.shape[0] // k\n",
    "    x_train,y_train=None,None\n",
    "    row_list=list(range(x.shape[0]))\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(row_list)\n",
    "    for i in range(k):\n",
    "        idx=slice(fold_size*i,fold_size*(i+1))\n",
    "        x_part,y_part=x[row_list[idx],:],y[row_list[idx],:]\n",
    "        if i==j:\n",
    "            x_val,y_val=x_part,y_part\n",
    "        elif x_train is None:\n",
    "            x_train,y_train=x_part,y_part\n",
    "        else:\n",
    "            x_train=torch.cat((x_train,x_part))\n",
    "            y_train=torch.cat((y_train,y_part))\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:37:00.324891Z",
     "start_time": "2022-04-19T06:37:00.289886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_kfold(net1,num_epochs,lr,k,x_data,y_data,batch_size,montum,wd):\n",
    "    train_sum_l,test_sum_l=[],[]\n",
    "    train_l,test_l=[],[]\n",
    "#     net=nn.Linear(x_data.shape[1])\n",
    "    for j in range(k):\n",
    "        net1=get_net()\n",
    "#         for p in net1.parameters():\n",
    "#             torch.nn.init.normal_(p)\n",
    "        net=net1\n",
    "        data=get_kfold_data(k,j,x_data,y_data)\n",
    "        train_l,test_l=train_k_fold_data(net,num_epochs,lr,*data,batch_size,montum,wd)\n",
    "        train_sum_l.append(train_l[-1])\n",
    "        test_sum_l.append(test_l[-1])\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        print('fold %d,train loss mean : %f,test loss : %f' % (j+1,train_l[-1],test_l[-1]))\n",
    "        print('-------------------------------------------------------------------------')\n",
    "    print('%d fold ,total train loss mean : %f,total test loss mean : %f ' % (k,np.mean(train_sum_l),np.mean(test_sum_l)))\n",
    "    print('-----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:37:01.429031Z",
     "start_time": "2022-04-19T06:37:01.403028Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x11=torch.Tensor(x)\n",
    "y11=torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:40:26.132525Z",
     "start_time": "2022-04-19T06:37:17.701097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 train_loss : 0.371675 , test loss : 0.412721\n",
      "epoch = 2 train_loss : 0.296409 , test loss : 0.353679\n",
      "epoch = 3 train_loss : 0.249648 , test loss : 0.307367\n",
      "epoch = 4 train_loss : 0.219696 , test loss : 0.278300\n",
      "epoch = 5 train_loss : 0.196267 , test loss : 0.251929\n",
      "epoch = 6 train_loss : 0.174817 , test loss : 0.233486\n",
      "epoch = 7 train_loss : 0.160651 , test loss : 0.218536\n",
      "epoch = 8 train_loss : 0.147730 , test loss : 0.207120\n",
      "epoch = 9 train_loss : 0.138490 , test loss : 0.199888\n",
      "epoch = 10 train_loss : 0.131910 , test loss : 0.198027\n",
      "epoch = 11 train_loss : 0.121803 , test loss : 0.188596\n",
      "epoch = 12 train_loss : 0.111415 , test loss : 0.180612\n",
      "epoch = 13 train_loss : 0.107153 , test loss : 0.179309\n",
      "epoch = 15 train_loss : 0.095786 , test loss : 0.172003\n",
      "epoch = 16 train_loss : 0.091395 , test loss : 0.171546\n",
      "epoch = 17 train_loss : 0.088295 , test loss : 0.168564\n",
      "epoch = 19 train_loss : 0.081826 , test loss : 0.168512\n",
      "--------------------------------------------------------------------------\n",
      "fold 1,train loss mean : 0.081826,test loss : 0.168512\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.371323 , test loss : 0.415935\n",
      "epoch = 2 train_loss : 0.303225 , test loss : 0.336922\n",
      "epoch = 3 train_loss : 0.255769 , test loss : 0.291131\n",
      "epoch = 4 train_loss : 0.220042 , test loss : 0.265990\n",
      "epoch = 5 train_loss : 0.194838 , test loss : 0.247145\n",
      "epoch = 6 train_loss : 0.180403 , test loss : 0.241195\n",
      "epoch = 7 train_loss : 0.159666 , test loss : 0.225254\n",
      "epoch = 8 train_loss : 0.149096 , test loss : 0.214997\n",
      "epoch = 9 train_loss : 0.138767 , test loss : 0.213922\n",
      "epoch = 10 train_loss : 0.130684 , test loss : 0.203717\n",
      "epoch = 11 train_loss : 0.121400 , test loss : 0.203276\n",
      "epoch = 12 train_loss : 0.114194 , test loss : 0.191661\n",
      "epoch = 14 train_loss : 0.103470 , test loss : 0.189025\n",
      "epoch = 15 train_loss : 0.097692 , test loss : 0.188237\n",
      "epoch = 16 train_loss : 0.098623 , test loss : 0.187107\n",
      "epoch = 17 train_loss : 0.094845 , test loss : 0.187076\n",
      "epoch = 18 train_loss : 0.086630 , test loss : 0.177636\n",
      "epoch = 20 train_loss : 0.081203 , test loss : 0.177362\n",
      "epoch = 21 train_loss : 0.077804 , test loss : 0.173541\n",
      "epoch = 27 train_loss : 0.065430 , test loss : 0.172749\n",
      "--------------------------------------------------------------------------\n",
      "fold 2,train loss mean : 0.065430,test loss : 0.172749\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.355759 , test loss : 0.343336\n",
      "epoch = 2 train_loss : 0.293178 , test loss : 0.291095\n",
      "epoch = 3 train_loss : 0.251642 , test loss : 0.253445\n",
      "epoch = 4 train_loss : 0.221634 , test loss : 0.230264\n",
      "epoch = 5 train_loss : 0.200116 , test loss : 0.211423\n",
      "epoch = 6 train_loss : 0.179293 , test loss : 0.195981\n",
      "epoch = 7 train_loss : 0.169125 , test loss : 0.184456\n",
      "epoch = 8 train_loss : 0.162267 , test loss : 0.184208\n",
      "epoch = 9 train_loss : 0.142242 , test loss : 0.166780\n",
      "epoch = 10 train_loss : 0.131806 , test loss : 0.165840\n",
      "epoch = 11 train_loss : 0.127148 , test loss : 0.164689\n",
      "epoch = 12 train_loss : 0.118528 , test loss : 0.156146\n",
      "epoch = 13 train_loss : 0.111186 , test loss : 0.152997\n",
      "epoch = 15 train_loss : 0.100413 , test loss : 0.151959\n",
      "epoch = 17 train_loss : 0.094194 , test loss : 0.148894\n",
      "epoch = 18 train_loss : 0.089396 , test loss : 0.148544\n",
      "epoch = 23 train_loss : 0.076692 , test loss : 0.148104\n",
      "--------------------------------------------------------------------------\n",
      "fold 3,train loss mean : 0.076692,test loss : 0.148104\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.364573 , test loss : 0.362325\n",
      "epoch = 2 train_loss : 0.277525 , test loss : 0.289434\n",
      "epoch = 3 train_loss : 0.235907 , test loss : 0.257217\n",
      "epoch = 4 train_loss : 0.211531 , test loss : 0.238520\n",
      "epoch = 5 train_loss : 0.190548 , test loss : 0.219850\n",
      "epoch = 6 train_loss : 0.174710 , test loss : 0.204553\n",
      "epoch = 7 train_loss : 0.157040 , test loss : 0.193134\n",
      "epoch = 8 train_loss : 0.144877 , test loss : 0.183913\n",
      "epoch = 9 train_loss : 0.138048 , test loss : 0.180440\n",
      "epoch = 10 train_loss : 0.129059 , test loss : 0.174776\n",
      "epoch = 11 train_loss : 0.121490 , test loss : 0.163220\n",
      "epoch = 12 train_loss : 0.112296 , test loss : 0.162283\n",
      "epoch = 13 train_loss : 0.109015 , test loss : 0.161193\n",
      "epoch = 14 train_loss : 0.103075 , test loss : 0.159395\n",
      "epoch = 15 train_loss : 0.099910 , test loss : 0.158843\n",
      "epoch = 17 train_loss : 0.093069 , test loss : 0.155214\n",
      "epoch = 18 train_loss : 0.090448 , test loss : 0.153145\n",
      "epoch = 19 train_loss : 0.087668 , test loss : 0.153011\n",
      "epoch = 20 train_loss : 0.084408 , test loss : 0.152145\n",
      "epoch = 23 train_loss : 0.075640 , test loss : 0.151111\n",
      "--------------------------------------------------------------------------\n",
      "fold 4,train loss mean : 0.075640,test loss : 0.151111\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.353184 , test loss : 0.358957\n",
      "epoch = 2 train_loss : 0.280381 , test loss : 0.286391\n",
      "epoch = 3 train_loss : 0.238384 , test loss : 0.255704\n",
      "epoch = 4 train_loss : 0.211676 , test loss : 0.237165\n",
      "epoch = 5 train_loss : 0.190156 , test loss : 0.220239\n",
      "epoch = 6 train_loss : 0.170638 , test loss : 0.204306\n",
      "epoch = 7 train_loss : 0.158485 , test loss : 0.196658\n",
      "epoch = 8 train_loss : 0.144784 , test loss : 0.188145\n",
      "epoch = 9 train_loss : 0.132341 , test loss : 0.181813\n",
      "epoch = 10 train_loss : 0.126282 , test loss : 0.177991\n",
      "epoch = 11 train_loss : 0.115721 , test loss : 0.171184\n",
      "epoch = 13 train_loss : 0.104863 , test loss : 0.167673\n",
      "epoch = 14 train_loss : 0.102590 , test loss : 0.166003\n",
      "epoch = 15 train_loss : 0.098255 , test loss : 0.164547\n",
      "epoch = 16 train_loss : 0.093225 , test loss : 0.161170\n",
      "epoch = 20 train_loss : 0.080386 , test loss : 0.160104\n",
      "--------------------------------------------------------------------------\n",
      "fold 5,train loss mean : 0.080386,test loss : 0.160104\n",
      "-------------------------------------------------------------------------\n",
      "5 fold ,total train loss mean : 0.075995,total test loss mean : 0.160116 \n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_net():\n",
    "    return nn.Sequential(nn.Linear(x11.shape[1],128),nn.ReLU(),nn.Linear(128,1))\n",
    "net1=get_net()\n",
    "train_kfold(net1,1000,0.001,5,x11,y11,256,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:24:04.955364Z",
     "start_time": "2022-04-19T02:24:04.944362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15*16+22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change y to original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:42:03.660910Z",
     "start_time": "2022-04-19T06:42:02.723791Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in range(12):\n",
    "    for g in range(471):\n",
    "        x[m*471+g:m*471+g+1,:]=all_train_data2.iloc[m*480+g:m*480+g+9,:].values.reshape(1,-1)\n",
    "        y[m*471+g:g*471+g+1,:]=all_train_data1.iloc[m*480+g+9:m*480+g+9+1,9].values.reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T06:43:05.318239Z",
     "start_time": "2022-04-19T06:43:05.306238Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x11=torch.Tensor(x)\n",
    "y11=torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T07:18:04.373285Z",
     "start_time": "2022-04-19T07:08:48.356180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 train_loss : 647.951294 , test loss : 670.700134\n",
      "epoch = 2 train_loss : 595.591187 , test loss : 617.502502\n",
      "epoch = 3 train_loss : 570.112244 , test loss : 594.117126\n",
      "epoch = 4 train_loss : 552.577759 , test loss : 578.583496\n",
      "epoch = 5 train_loss : 536.613098 , test loss : 564.098938\n",
      "epoch = 6 train_loss : 523.258240 , test loss : 551.711365\n",
      "epoch = 7 train_loss : 510.037598 , test loss : 539.422058\n",
      "epoch = 8 train_loss : 496.708099 , test loss : 526.543091\n",
      "epoch = 9 train_loss : 483.165009 , test loss : 512.768433\n",
      "epoch = 10 train_loss : 468.881012 , test loss : 499.001923\n",
      "epoch = 11 train_loss : 453.585754 , test loss : 482.989655\n",
      "epoch = 12 train_loss : 437.246033 , test loss : 466.660889\n",
      "epoch = 13 train_loss : 419.737183 , test loss : 449.412598\n",
      "epoch = 14 train_loss : 401.414154 , test loss : 429.527161\n",
      "epoch = 15 train_loss : 381.792816 , test loss : 410.705444\n",
      "epoch = 16 train_loss : 361.618958 , test loss : 390.580658\n",
      "epoch = 17 train_loss : 340.656891 , test loss : 369.052246\n",
      "epoch = 18 train_loss : 319.107239 , test loss : 346.294067\n",
      "epoch = 19 train_loss : 296.978912 , test loss : 324.405762\n",
      "epoch = 20 train_loss : 275.130890 , test loss : 301.215332\n",
      "epoch = 21 train_loss : 253.412292 , test loss : 279.127594\n",
      "epoch = 22 train_loss : 232.973557 , test loss : 258.535828\n",
      "epoch = 23 train_loss : 211.283234 , test loss : 236.319550\n",
      "epoch = 24 train_loss : 191.492874 , test loss : 214.873810\n",
      "epoch = 25 train_loss : 172.291931 , test loss : 195.165848\n",
      "epoch = 26 train_loss : 154.925629 , test loss : 177.499481\n",
      "epoch = 27 train_loss : 138.703827 , test loss : 159.249176\n",
      "epoch = 28 train_loss : 123.607155 , test loss : 143.563400\n",
      "epoch = 29 train_loss : 110.716164 , test loss : 128.470322\n",
      "epoch = 30 train_loss : 98.192307 , test loss : 115.919365\n",
      "epoch = 31 train_loss : 87.844292 , test loss : 104.830971\n",
      "epoch = 32 train_loss : 79.014503 , test loss : 95.335808\n",
      "epoch = 33 train_loss : 71.334435 , test loss : 86.769455\n",
      "epoch = 34 train_loss : 64.789940 , test loss : 79.300301\n",
      "epoch = 35 train_loss : 59.468510 , test loss : 73.913788\n",
      "epoch = 36 train_loss : 54.790913 , test loss : 68.508301\n",
      "epoch = 37 train_loss : 51.341648 , test loss : 64.717400\n",
      "epoch = 38 train_loss : 48.760529 , test loss : 61.668659\n",
      "epoch = 39 train_loss : 46.615913 , test loss : 59.420769\n",
      "epoch = 40 train_loss : 44.763988 , test loss : 57.449043\n",
      "epoch = 41 train_loss : 43.577538 , test loss : 55.709866\n",
      "epoch = 42 train_loss : 42.384155 , test loss : 54.789703\n",
      "epoch = 43 train_loss : 41.989498 , test loss : 54.062057\n",
      "epoch = 44 train_loss : 41.344894 , test loss : 53.288708\n",
      "epoch = 45 train_loss : 40.772198 , test loss : 52.844505\n",
      "epoch = 46 train_loss : 40.033913 , test loss : 52.052414\n",
      "epoch = 48 train_loss : 39.543419 , test loss : 51.287434\n",
      "epoch = 50 train_loss : 39.310581 , test loss : 51.163177\n",
      "epoch = 52 train_loss : 38.879848 , test loss : 50.933338\n",
      "epoch = 53 train_loss : 38.929974 , test loss : 50.912613\n",
      "epoch = 54 train_loss : 38.825211 , test loss : 50.774342\n",
      "epoch = 56 train_loss : 38.247574 , test loss : 50.416550\n",
      "epoch = 59 train_loss : 38.207024 , test loss : 50.398483\n",
      "epoch = 60 train_loss : 37.970596 , test loss : 50.035789\n",
      "epoch = 63 train_loss : 37.788071 , test loss : 49.925793\n",
      "epoch = 65 train_loss : 37.651886 , test loss : 49.778069\n",
      "epoch = 66 train_loss : 37.661358 , test loss : 49.706734\n",
      "epoch = 68 train_loss : 37.529675 , test loss : 49.555428\n",
      "epoch = 72 train_loss : 37.377945 , test loss : 49.436993\n",
      "epoch = 78 train_loss : 37.078762 , test loss : 49.382809\n",
      "epoch = 81 train_loss : 36.998615 , test loss : 49.067867\n",
      "epoch = 88 train_loss : 36.823963 , test loss : 49.022400\n",
      "epoch = 100 train_loss : 36.481956 , test loss : 48.735016\n",
      "epoch = 121 train_loss : 36.456085 , test loss : 48.673557\n",
      "epoch = 149 train_loss : 36.168549 , test loss : 48.569813\n",
      "epoch = 171 train_loss : 36.082848 , test loss : 48.529579\n",
      "epoch = 183 train_loss : 36.260975 , test loss : 48.511417\n",
      "epoch = 209 train_loss : 36.021446 , test loss : 48.437740\n",
      "epoch = 213 train_loss : 36.003815 , test loss : 48.426136\n",
      "epoch = 258 train_loss : 36.109123 , test loss : 48.411152\n",
      "epoch = 329 train_loss : 36.008301 , test loss : 48.390018\n",
      "--------------------------------------------------------------------------\n",
      "fold 1,train loss mean : 36.008301,test loss : 48.390018\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 654.041321 , test loss : 633.828918\n",
      "epoch = 2 train_loss : 598.049500 , test loss : 581.948059\n",
      "epoch = 3 train_loss : 568.483276 , test loss : 558.043518\n",
      "epoch = 4 train_loss : 549.834106 , test loss : 543.414001\n",
      "epoch = 5 train_loss : 533.353027 , test loss : 530.259705\n",
      "epoch = 6 train_loss : 518.778442 , test loss : 518.442505\n",
      "epoch = 7 train_loss : 504.939636 , test loss : 506.523285\n",
      "epoch = 8 train_loss : 490.628265 , test loss : 494.303223\n",
      "epoch = 9 train_loss : 476.135345 , test loss : 481.078552\n",
      "epoch = 10 train_loss : 460.838989 , test loss : 467.449005\n",
      "epoch = 11 train_loss : 444.917786 , test loss : 451.983551\n",
      "epoch = 12 train_loss : 428.162720 , test loss : 435.560577\n",
      "epoch = 13 train_loss : 410.112793 , test loss : 418.378571\n",
      "epoch = 14 train_loss : 391.255371 , test loss : 400.313538\n",
      "epoch = 15 train_loss : 371.817932 , test loss : 380.369751\n",
      "epoch = 16 train_loss : 351.299713 , test loss : 360.421844\n",
      "epoch = 17 train_loss : 330.053772 , test loss : 338.045624\n",
      "epoch = 18 train_loss : 309.213348 , test loss : 316.597656\n",
      "epoch = 19 train_loss : 287.059631 , test loss : 294.894165\n",
      "epoch = 20 train_loss : 265.513794 , test loss : 271.082581\n",
      "epoch = 21 train_loss : 244.535400 , test loss : 251.136429\n",
      "epoch = 22 train_loss : 223.594467 , test loss : 228.175018\n",
      "epoch = 23 train_loss : 202.698395 , test loss : 206.660721\n",
      "epoch = 24 train_loss : 183.431641 , test loss : 186.490494\n",
      "epoch = 25 train_loss : 167.033844 , test loss : 170.236115\n",
      "epoch = 26 train_loss : 148.774918 , test loss : 149.823608\n",
      "epoch = 27 train_loss : 133.050034 , test loss : 135.063690\n",
      "epoch = 28 train_loss : 118.908653 , test loss : 119.208435\n",
      "epoch = 29 train_loss : 106.091202 , test loss : 105.712265\n",
      "epoch = 30 train_loss : 94.795746 , test loss : 94.700905\n",
      "epoch = 31 train_loss : 85.573822 , test loss : 85.337898\n",
      "epoch = 32 train_loss : 76.736359 , test loss : 75.396782\n",
      "epoch = 33 train_loss : 69.629929 , test loss : 68.300201\n",
      "epoch = 34 train_loss : 63.563629 , test loss : 62.037956\n",
      "epoch = 35 train_loss : 58.776825 , test loss : 56.934238\n",
      "epoch = 36 train_loss : 54.783943 , test loss : 52.738213\n",
      "epoch = 37 train_loss : 51.794334 , test loss : 49.810417\n",
      "epoch = 38 train_loss : 49.391762 , test loss : 46.705982\n",
      "epoch = 39 train_loss : 47.308071 , test loss : 44.933071\n",
      "epoch = 40 train_loss : 45.582367 , test loss : 43.126160\n",
      "epoch = 41 train_loss : 44.629814 , test loss : 42.494095\n",
      "epoch = 42 train_loss : 43.436783 , test loss : 41.013172\n",
      "epoch = 43 train_loss : 42.872108 , test loss : 40.408169\n",
      "epoch = 44 train_loss : 42.121109 , test loss : 39.618145\n",
      "epoch = 46 train_loss : 41.341721 , test loss : 38.464809\n",
      "epoch = 47 train_loss : 41.078945 , test loss : 38.409645\n",
      "epoch = 48 train_loss : 40.898739 , test loss : 38.081699\n",
      "epoch = 49 train_loss : 40.703773 , test loss : 37.946659\n",
      "epoch = 51 train_loss : 40.213985 , test loss : 37.857246\n",
      "epoch = 53 train_loss : 39.977825 , test loss : 37.809002\n",
      "epoch = 55 train_loss : 39.637074 , test loss : 37.433113\n",
      "epoch = 58 train_loss : 39.580219 , test loss : 37.412235\n",
      "epoch = 60 train_loss : 39.253014 , test loss : 37.404091\n",
      "epoch = 61 train_loss : 39.239410 , test loss : 37.380215\n",
      "epoch = 62 train_loss : 39.302860 , test loss : 37.367706\n",
      "epoch = 65 train_loss : 39.215866 , test loss : 36.696369\n",
      "epoch = 69 train_loss : 38.687618 , test loss : 36.586990\n",
      "epoch = 87 train_loss : 38.119801 , test loss : 36.208416\n",
      "epoch = 91 train_loss : 37.920509 , test loss : 36.123192\n",
      "epoch = 106 train_loss : 37.712517 , test loss : 36.096352\n",
      "epoch = 128 train_loss : 37.644676 , test loss : 35.946220\n",
      "epoch = 151 train_loss : 37.508213 , test loss : 35.815872\n",
      "--------------------------------------------------------------------------\n",
      "fold 2,train loss mean : 37.508213,test loss : 35.815872\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 657.172119 , test loss : 642.408203\n",
      "epoch = 2 train_loss : 601.739624 , test loss : 589.338074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3 train_loss : 574.041565 , test loss : 568.368225\n",
      "epoch = 4 train_loss : 555.870178 , test loss : 556.936096\n",
      "epoch = 5 train_loss : 540.556030 , test loss : 547.863464\n",
      "epoch = 6 train_loss : 526.341370 , test loss : 535.954041\n",
      "epoch = 7 train_loss : 512.665710 , test loss : 525.398865\n",
      "epoch = 8 train_loss : 499.021301 , test loss : 513.670959\n",
      "epoch = 9 train_loss : 485.039978 , test loss : 501.140442\n",
      "epoch = 10 train_loss : 470.489777 , test loss : 487.778473\n",
      "epoch = 11 train_loss : 454.877686 , test loss : 474.482300\n",
      "epoch = 12 train_loss : 438.478363 , test loss : 458.559082\n",
      "epoch = 13 train_loss : 421.073364 , test loss : 443.254578\n",
      "epoch = 14 train_loss : 402.579926 , test loss : 425.741272\n",
      "epoch = 15 train_loss : 383.007904 , test loss : 407.902618\n",
      "epoch = 16 train_loss : 362.541656 , test loss : 388.541412\n",
      "epoch = 17 train_loss : 341.669250 , test loss : 369.038849\n",
      "epoch = 18 train_loss : 320.644531 , test loss : 349.121094\n",
      "epoch = 19 train_loss : 299.204681 , test loss : 330.129150\n",
      "epoch = 20 train_loss : 277.560394 , test loss : 306.816376\n",
      "epoch = 21 train_loss : 255.108261 , test loss : 285.421112\n",
      "epoch = 22 train_loss : 233.676407 , test loss : 264.095490\n",
      "epoch = 23 train_loss : 213.211227 , test loss : 244.272156\n",
      "epoch = 24 train_loss : 193.040405 , test loss : 222.759918\n",
      "epoch = 25 train_loss : 174.461548 , test loss : 201.820602\n",
      "epoch = 26 train_loss : 156.348495 , test loss : 185.238342\n",
      "epoch = 27 train_loss : 140.325943 , test loss : 169.663055\n",
      "epoch = 28 train_loss : 124.980614 , test loss : 153.002396\n",
      "epoch = 29 train_loss : 112.254120 , test loss : 140.048615\n",
      "epoch = 30 train_loss : 99.923294 , test loss : 123.605202\n",
      "epoch = 31 train_loss : 89.261635 , test loss : 115.232826\n",
      "epoch = 32 train_loss : 80.935020 , test loss : 104.681732\n",
      "epoch = 33 train_loss : 72.454880 , test loss : 94.174988\n",
      "epoch = 34 train_loss : 65.756416 , test loss : 88.295609\n",
      "epoch = 35 train_loss : 59.842335 , test loss : 82.179420\n",
      "epoch = 36 train_loss : 55.823814 , test loss : 78.635551\n",
      "epoch = 37 train_loss : 51.702217 , test loss : 72.576706\n",
      "epoch = 38 train_loss : 49.148232 , test loss : 69.927048\n",
      "epoch = 39 train_loss : 46.574585 , test loss : 66.723480\n",
      "epoch = 40 train_loss : 44.734173 , test loss : 64.718040\n",
      "epoch = 41 train_loss : 43.388367 , test loss : 62.393646\n",
      "epoch = 42 train_loss : 42.726162 , test loss : 62.175926\n",
      "epoch = 43 train_loss : 41.447327 , test loss : 60.505428\n",
      "epoch = 44 train_loss : 40.720211 , test loss : 59.236263\n",
      "epoch = 45 train_loss : 40.554657 , test loss : 58.139912\n",
      "epoch = 46 train_loss : 39.896843 , test loss : 57.237312\n",
      "epoch = 47 train_loss : 39.646835 , test loss : 56.564011\n",
      "epoch = 48 train_loss : 39.371220 , test loss : 56.534718\n",
      "epoch = 49 train_loss : 39.000431 , test loss : 56.532185\n",
      "epoch = 50 train_loss : 39.084972 , test loss : 55.153252\n",
      "epoch = 51 train_loss : 39.223717 , test loss : 55.067604\n",
      "epoch = 55 train_loss : 38.429672 , test loss : 54.692730\n",
      "epoch = 56 train_loss : 38.309223 , test loss : 54.155087\n",
      "epoch = 59 train_loss : 37.817287 , test loss : 54.065945\n",
      "epoch = 62 train_loss : 37.639149 , test loss : 53.971558\n",
      "epoch = 65 train_loss : 37.415627 , test loss : 53.890831\n",
      "epoch = 66 train_loss : 37.466549 , test loss : 53.655548\n",
      "epoch = 68 train_loss : 37.359840 , test loss : 52.640324\n",
      "epoch = 71 train_loss : 37.198914 , test loss : 52.534874\n",
      "epoch = 75 train_loss : 36.971634 , test loss : 52.528984\n",
      "epoch = 76 train_loss : 36.965672 , test loss : 52.378506\n",
      "epoch = 79 train_loss : 36.857609 , test loss : 52.344246\n",
      "epoch = 83 train_loss : 36.945354 , test loss : 51.766121\n",
      "epoch = 87 train_loss : 37.142662 , test loss : 50.996952\n",
      "epoch = 97 train_loss : 36.631004 , test loss : 50.826561\n",
      "epoch = 107 train_loss : 36.360336 , test loss : 50.746403\n",
      "epoch = 119 train_loss : 36.234943 , test loss : 50.463097\n",
      "epoch = 131 train_loss : 36.293423 , test loss : 50.356903\n",
      "epoch = 139 train_loss : 36.185211 , test loss : 50.212002\n",
      "epoch = 154 train_loss : 36.486351 , test loss : 50.168133\n",
      "epoch = 192 train_loss : 36.239807 , test loss : 50.160938\n",
      "epoch = 196 train_loss : 36.121346 , test loss : 50.106140\n",
      "epoch = 216 train_loss : 36.166534 , test loss : 50.036053\n",
      "epoch = 218 train_loss : 36.173233 , test loss : 49.904858\n",
      "epoch = 237 train_loss : 36.618050 , test loss : 49.886627\n",
      "epoch = 243 train_loss : 36.529667 , test loss : 49.783146\n",
      "epoch = 283 train_loss : 36.077675 , test loss : 49.680851\n",
      "epoch = 286 train_loss : 36.078465 , test loss : 49.559948\n",
      "epoch = 608 train_loss : 36.122963 , test loss : 49.551109\n",
      "epoch = 1000 train_loss : 36.114471 , test loss : 49.530766\n",
      "--------------------------------------------------------------------------\n",
      "fold 3,train loss mean : 36.114471,test loss : 49.530766\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 654.169495 , test loss : 634.202515\n",
      "epoch = 2 train_loss : 600.673279 , test loss : 581.843201\n",
      "epoch = 3 train_loss : 572.492188 , test loss : 553.832764\n",
      "epoch = 4 train_loss : 554.451233 , test loss : 537.971436\n",
      "epoch = 5 train_loss : 538.881348 , test loss : 525.792725\n",
      "epoch = 6 train_loss : 524.438904 , test loss : 513.508179\n",
      "epoch = 7 train_loss : 511.012329 , test loss : 502.394958\n",
      "epoch = 8 train_loss : 497.656586 , test loss : 491.440094\n",
      "epoch = 9 train_loss : 483.306488 , test loss : 478.840424\n",
      "epoch = 10 train_loss : 468.832489 , test loss : 466.451202\n",
      "epoch = 11 train_loss : 453.311218 , test loss : 453.152283\n",
      "epoch = 12 train_loss : 437.049225 , test loss : 438.026245\n",
      "epoch = 13 train_loss : 419.367615 , test loss : 422.781799\n",
      "epoch = 14 train_loss : 401.201477 , test loss : 406.400238\n",
      "epoch = 15 train_loss : 381.930634 , test loss : 389.409515\n",
      "epoch = 16 train_loss : 361.911224 , test loss : 371.261017\n",
      "epoch = 17 train_loss : 341.301941 , test loss : 351.270142\n",
      "epoch = 18 train_loss : 319.926575 , test loss : 329.756195\n",
      "epoch = 19 train_loss : 298.621765 , test loss : 311.355286\n",
      "epoch = 20 train_loss : 276.496552 , test loss : 287.975250\n",
      "epoch = 21 train_loss : 255.268066 , test loss : 267.049866\n",
      "epoch = 22 train_loss : 233.969910 , test loss : 245.825943\n",
      "epoch = 23 train_loss : 215.240952 , test loss : 225.665497\n",
      "epoch = 24 train_loss : 193.830780 , test loss : 204.467850\n",
      "epoch = 25 train_loss : 174.690979 , test loss : 184.117615\n",
      "epoch = 26 train_loss : 157.213715 , test loss : 166.906845\n",
      "epoch = 27 train_loss : 141.230698 , test loss : 149.372940\n",
      "epoch = 28 train_loss : 126.254471 , test loss : 133.318161\n",
      "epoch = 29 train_loss : 112.427765 , test loss : 119.336975\n",
      "epoch = 30 train_loss : 100.427765 , test loss : 106.501373\n",
      "epoch = 31 train_loss : 90.095062 , test loss : 95.203690\n",
      "epoch = 32 train_loss : 81.430031 , test loss : 86.147354\n",
      "epoch = 33 train_loss : 73.292961 , test loss : 77.562775\n",
      "epoch = 34 train_loss : 66.925774 , test loss : 70.668724\n",
      "epoch = 35 train_loss : 61.375774 , test loss : 64.412209\n",
      "epoch = 36 train_loss : 56.781368 , test loss : 60.023037\n",
      "epoch = 37 train_loss : 53.315907 , test loss : 55.875050\n",
      "epoch = 38 train_loss : 50.352448 , test loss : 53.055416\n",
      "epoch = 39 train_loss : 48.213791 , test loss : 50.367268\n",
      "epoch = 40 train_loss : 46.263554 , test loss : 48.374554\n",
      "epoch = 41 train_loss : 44.774948 , test loss : 46.586430\n",
      "epoch = 42 train_loss : 43.575378 , test loss : 45.332108\n",
      "epoch = 43 train_loss : 42.836918 , test loss : 44.727921\n",
      "epoch = 44 train_loss : 42.415020 , test loss : 44.246616\n",
      "epoch = 45 train_loss : 41.604202 , test loss : 43.117855\n",
      "epoch = 46 train_loss : 41.309628 , test loss : 42.580151\n",
      "epoch = 48 train_loss : 40.716942 , test loss : 41.913822\n",
      "epoch = 49 train_loss : 40.482761 , test loss : 41.737991\n",
      "epoch = 50 train_loss : 40.422539 , test loss : 41.699886\n",
      "epoch = 52 train_loss : 40.097786 , test loss : 41.269493\n",
      "epoch = 53 train_loss : 40.058102 , test loss : 40.836288\n",
      "epoch = 54 train_loss : 39.983658 , test loss : 40.762154\n",
      "epoch = 56 train_loss : 39.549976 , test loss : 40.422218\n",
      "epoch = 59 train_loss : 39.399071 , test loss : 39.614960\n",
      "epoch = 61 train_loss : 39.213310 , test loss : 39.521015\n",
      "epoch = 65 train_loss : 38.839706 , test loss : 39.081654\n",
      "epoch = 68 train_loss : 38.921360 , test loss : 39.016140\n",
      "epoch = 70 train_loss : 38.854816 , test loss : 38.959225\n",
      "epoch = 71 train_loss : 38.819412 , test loss : 38.629837\n",
      "epoch = 73 train_loss : 39.024548 , test loss : 38.626095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 75 train_loss : 38.458519 , test loss : 38.473949\n",
      "epoch = 78 train_loss : 38.471119 , test loss : 38.243309\n",
      "epoch = 80 train_loss : 38.483303 , test loss : 38.006290\n",
      "epoch = 85 train_loss : 38.167805 , test loss : 37.562824\n",
      "epoch = 87 train_loss : 38.073360 , test loss : 37.562386\n",
      "epoch = 94 train_loss : 38.065800 , test loss : 37.557026\n",
      "epoch = 98 train_loss : 38.080540 , test loss : 37.528099\n",
      "epoch = 99 train_loss : 37.961571 , test loss : 37.164780\n",
      "epoch = 104 train_loss : 37.820011 , test loss : 36.914581\n",
      "epoch = 115 train_loss : 37.673161 , test loss : 36.786797\n",
      "epoch = 118 train_loss : 37.825703 , test loss : 36.704365\n",
      "epoch = 142 train_loss : 37.583252 , test loss : 36.246109\n",
      "epoch = 175 train_loss : 37.595676 , test loss : 36.153908\n",
      "epoch = 259 train_loss : 37.354218 , test loss : 36.037594\n",
      "epoch = 345 train_loss : 37.425079 , test loss : 35.991837\n",
      "epoch = 404 train_loss : 37.452896 , test loss : 35.948242\n",
      "--------------------------------------------------------------------------\n",
      "fold 4,train loss mean : 37.452896,test loss : 35.948242\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 654.061890 , test loss : 682.904358\n",
      "epoch = 2 train_loss : 597.518860 , test loss : 629.933289\n",
      "epoch = 3 train_loss : 569.844360 , test loss : 601.751221\n",
      "epoch = 4 train_loss : 552.914795 , test loss : 581.924927\n",
      "epoch = 5 train_loss : 538.351135 , test loss : 565.818115\n",
      "epoch = 6 train_loss : 523.972595 , test loss : 550.389587\n",
      "epoch = 7 train_loss : 510.636566 , test loss : 537.207153\n",
      "epoch = 8 train_loss : 497.361755 , test loss : 524.463135\n",
      "epoch = 9 train_loss : 483.531128 , test loss : 510.421417\n",
      "epoch = 10 train_loss : 469.280396 , test loss : 496.394745\n",
      "epoch = 11 train_loss : 454.203552 , test loss : 482.404358\n",
      "epoch = 12 train_loss : 438.030182 , test loss : 465.302094\n",
      "epoch = 13 train_loss : 420.969910 , test loss : 447.544495\n",
      "epoch = 14 train_loss : 402.699890 , test loss : 429.801819\n",
      "epoch = 15 train_loss : 383.313568 , test loss : 408.890717\n",
      "epoch = 16 train_loss : 363.213043 , test loss : 387.573792\n",
      "epoch = 17 train_loss : 342.313904 , test loss : 366.664764\n",
      "epoch = 18 train_loss : 321.027252 , test loss : 343.559692\n",
      "epoch = 19 train_loss : 299.282227 , test loss : 321.492828\n",
      "epoch = 20 train_loss : 277.444824 , test loss : 299.049988\n",
      "epoch = 21 train_loss : 255.887680 , test loss : 274.681976\n",
      "epoch = 22 train_loss : 235.248550 , test loss : 253.744217\n",
      "epoch = 23 train_loss : 213.530746 , test loss : 230.569275\n",
      "epoch = 24 train_loss : 193.939316 , test loss : 209.514496\n",
      "epoch = 25 train_loss : 174.790787 , test loss : 188.757812\n",
      "epoch = 26 train_loss : 156.961182 , test loss : 169.427521\n",
      "epoch = 27 train_loss : 141.061295 , test loss : 152.320984\n",
      "epoch = 28 train_loss : 125.751183 , test loss : 135.367386\n",
      "epoch = 29 train_loss : 111.942612 , test loss : 119.861694\n",
      "epoch = 30 train_loss : 100.446045 , test loss : 107.334991\n",
      "epoch = 31 train_loss : 89.637253 , test loss : 95.047081\n",
      "epoch = 32 train_loss : 80.332344 , test loss : 85.521881\n",
      "epoch = 33 train_loss : 72.890938 , test loss : 77.019554\n",
      "epoch = 34 train_loss : 66.262299 , test loss : 69.241119\n",
      "epoch = 35 train_loss : 60.826099 , test loss : 62.910061\n",
      "epoch = 36 train_loss : 56.463982 , test loss : 58.275116\n",
      "epoch = 37 train_loss : 53.112148 , test loss : 53.834827\n",
      "epoch = 38 train_loss : 50.592304 , test loss : 52.104973\n",
      "epoch = 39 train_loss : 48.218975 , test loss : 49.043369\n",
      "epoch = 40 train_loss : 46.051048 , test loss : 46.021488\n",
      "epoch = 41 train_loss : 44.845848 , test loss : 44.990852\n",
      "epoch = 42 train_loss : 44.104218 , test loss : 43.845692\n",
      "epoch = 43 train_loss : 43.064137 , test loss : 42.347206\n",
      "epoch = 45 train_loss : 41.759499 , test loss : 40.691765\n",
      "epoch = 46 train_loss : 41.569366 , test loss : 40.042969\n",
      "epoch = 47 train_loss : 41.151524 , test loss : 39.899342\n",
      "epoch = 48 train_loss : 40.878643 , test loss : 39.292938\n",
      "epoch = 51 train_loss : 40.697807 , test loss : 38.922840\n",
      "epoch = 55 train_loss : 40.012669 , test loss : 37.978374\n",
      "epoch = 56 train_loss : 39.977215 , test loss : 37.909882\n",
      "epoch = 57 train_loss : 39.669037 , test loss : 37.753700\n",
      "epoch = 59 train_loss : 39.350964 , test loss : 37.625664\n",
      "epoch = 63 train_loss : 39.379795 , test loss : 36.895954\n",
      "epoch = 74 train_loss : 38.649250 , test loss : 36.745998\n",
      "epoch = 76 train_loss : 38.617367 , test loss : 36.443401\n",
      "epoch = 80 train_loss : 38.392776 , test loss : 36.235550\n",
      "epoch = 89 train_loss : 38.365402 , test loss : 36.182732\n",
      "epoch = 94 train_loss : 37.999043 , test loss : 36.167423\n",
      "epoch = 102 train_loss : 38.503773 , test loss : 36.142906\n",
      "epoch = 103 train_loss : 37.983875 , test loss : 36.078091\n",
      "epoch = 104 train_loss : 37.985836 , test loss : 36.060524\n",
      "epoch = 106 train_loss : 37.788750 , test loss : 35.899315\n",
      "epoch = 112 train_loss : 38.242516 , test loss : 35.671543\n",
      "epoch = 173 train_loss : 38.028370 , test loss : 35.655899\n",
      "epoch = 254 train_loss : 37.643631 , test loss : 35.619987\n",
      "epoch = 301 train_loss : 37.705402 , test loss : 35.418579\n",
      "--------------------------------------------------------------------------\n",
      "fold 5,train loss mean : 37.705402,test loss : 35.418579\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 658.612183 , test loss : 688.527649\n",
      "epoch = 2 train_loss : 601.475464 , test loss : 622.230530\n",
      "epoch = 3 train_loss : 570.783386 , test loss : 580.674561\n",
      "epoch = 4 train_loss : 554.269714 , test loss : 555.795044\n",
      "epoch = 5 train_loss : 539.348206 , test loss : 535.668213\n",
      "epoch = 6 train_loss : 525.469482 , test loss : 518.862305\n",
      "epoch = 7 train_loss : 512.078369 , test loss : 502.559509\n",
      "epoch = 8 train_loss : 498.163269 , test loss : 486.384674\n",
      "epoch = 9 train_loss : 484.383026 , test loss : 472.586792\n",
      "epoch = 10 train_loss : 469.318695 , test loss : 458.518677\n",
      "epoch = 11 train_loss : 453.456451 , test loss : 444.109894\n",
      "epoch = 12 train_loss : 436.629700 , test loss : 428.328674\n",
      "epoch = 13 train_loss : 418.541107 , test loss : 412.739532\n",
      "epoch = 14 train_loss : 399.647583 , test loss : 393.359680\n",
      "epoch = 15 train_loss : 379.718811 , test loss : 376.714691\n",
      "epoch = 16 train_loss : 359.114441 , test loss : 358.339081\n",
      "epoch = 17 train_loss : 338.166901 , test loss : 336.349854\n",
      "epoch = 18 train_loss : 316.404388 , test loss : 317.113159\n",
      "epoch = 19 train_loss : 294.202972 , test loss : 293.470123\n",
      "epoch = 20 train_loss : 272.047821 , test loss : 272.603943\n",
      "epoch = 21 train_loss : 250.269440 , test loss : 250.945129\n",
      "epoch = 22 train_loss : 229.268036 , test loss : 230.521637\n",
      "epoch = 23 train_loss : 208.422424 , test loss : 209.917191\n",
      "epoch = 24 train_loss : 188.611008 , test loss : 189.243744\n",
      "epoch = 25 train_loss : 170.005020 , test loss : 168.568665\n",
      "epoch = 26 train_loss : 153.382385 , test loss : 151.968658\n",
      "epoch = 27 train_loss : 136.812912 , test loss : 137.009659\n",
      "epoch = 28 train_loss : 121.952408 , test loss : 120.171158\n",
      "epoch = 29 train_loss : 108.718239 , test loss : 107.158379\n",
      "epoch = 30 train_loss : 97.397415 , test loss : 95.404816\n",
      "epoch = 31 train_loss : 87.061562 , test loss : 85.132111\n",
      "epoch = 32 train_loss : 78.714943 , test loss : 77.418793\n",
      "epoch = 33 train_loss : 70.960579 , test loss : 68.257416\n",
      "epoch = 34 train_loss : 64.646820 , test loss : 61.098396\n",
      "epoch = 35 train_loss : 59.529198 , test loss : 56.610214\n",
      "epoch = 36 train_loss : 55.499073 , test loss : 52.543602\n",
      "epoch = 37 train_loss : 52.368134 , test loss : 49.256496\n",
      "epoch = 38 train_loss : 49.246643 , test loss : 45.488583\n",
      "epoch = 39 train_loss : 47.786156 , test loss : 43.749615\n",
      "epoch = 40 train_loss : 45.650333 , test loss : 41.540318\n",
      "epoch = 41 train_loss : 44.593300 , test loss : 40.764091\n",
      "epoch = 42 train_loss : 43.214542 , test loss : 39.838486\n",
      "epoch = 43 train_loss : 42.536026 , test loss : 38.824760\n",
      "epoch = 44 train_loss : 42.018536 , test loss : 38.397160\n",
      "epoch = 45 train_loss : 41.580669 , test loss : 37.833328\n",
      "epoch = 49 train_loss : 40.764683 , test loss : 37.107540\n",
      "epoch = 50 train_loss : 40.134331 , test loss : 36.739079\n",
      "epoch = 54 train_loss : 39.616512 , test loss : 36.433796\n",
      "epoch = 63 train_loss : 38.943336 , test loss : 36.265827\n",
      "epoch = 65 train_loss : 38.757164 , test loss : 36.186588\n",
      "epoch = 68 train_loss : 38.713985 , test loss : 36.080143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 86 train_loss : 38.221745 , test loss : 35.841839\n",
      "--------------------------------------------------------------------------\n",
      "fold 6,train loss mean : 38.221745,test loss : 35.841839\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 653.523376 , test loss : 692.618652\n",
      "epoch = 2 train_loss : 597.747742 , test loss : 630.544861\n",
      "epoch = 3 train_loss : 568.851562 , test loss : 597.138245\n",
      "epoch = 4 train_loss : 551.172363 , test loss : 576.410400\n",
      "epoch = 5 train_loss : 535.735779 , test loss : 560.827759\n",
      "epoch = 6 train_loss : 520.988770 , test loss : 545.165955\n",
      "epoch = 7 train_loss : 507.511169 , test loss : 531.558228\n",
      "epoch = 8 train_loss : 493.415131 , test loss : 516.501404\n",
      "epoch = 9 train_loss : 478.926392 , test loss : 503.504272\n",
      "epoch = 10 train_loss : 463.891113 , test loss : 488.369293\n",
      "epoch = 11 train_loss : 447.972656 , test loss : 472.462006\n",
      "epoch = 12 train_loss : 430.980194 , test loss : 455.516541\n",
      "epoch = 13 train_loss : 413.273834 , test loss : 437.573090\n",
      "epoch = 14 train_loss : 394.015961 , test loss : 419.430969\n",
      "epoch = 15 train_loss : 374.048096 , test loss : 400.498871\n",
      "epoch = 16 train_loss : 353.533020 , test loss : 379.024841\n",
      "epoch = 17 train_loss : 332.229034 , test loss : 357.631042\n",
      "epoch = 18 train_loss : 310.503754 , test loss : 335.627411\n",
      "epoch = 19 train_loss : 288.562195 , test loss : 313.178680\n",
      "epoch = 20 train_loss : 267.789886 , test loss : 291.460266\n",
      "epoch = 21 train_loss : 245.365860 , test loss : 269.734222\n",
      "epoch = 22 train_loss : 224.060715 , test loss : 247.090988\n",
      "epoch = 23 train_loss : 203.968002 , test loss : 225.997635\n",
      "epoch = 24 train_loss : 184.293900 , test loss : 205.229141\n",
      "epoch = 25 train_loss : 165.751816 , test loss : 185.537949\n",
      "epoch = 26 train_loss : 148.666107 , test loss : 166.632462\n",
      "epoch = 27 train_loss : 132.834717 , test loss : 149.970078\n",
      "epoch = 28 train_loss : 118.715164 , test loss : 134.336304\n",
      "epoch = 29 train_loss : 105.719849 , test loss : 120.490761\n",
      "epoch = 30 train_loss : 94.374077 , test loss : 107.853416\n",
      "epoch = 31 train_loss : 84.949455 , test loss : 97.162865\n",
      "epoch = 32 train_loss : 76.517883 , test loss : 87.679565\n",
      "epoch = 33 train_loss : 68.918716 , test loss : 79.278481\n",
      "epoch = 34 train_loss : 63.905014 , test loss : 73.100815\n",
      "epoch = 35 train_loss : 58.783493 , test loss : 67.743660\n",
      "epoch = 36 train_loss : 54.784973 , test loss : 63.142693\n",
      "epoch = 37 train_loss : 50.785332 , test loss : 58.765385\n",
      "epoch = 38 train_loss : 48.479298 , test loss : 55.933407\n",
      "epoch = 39 train_loss : 47.106400 , test loss : 53.963356\n",
      "epoch = 40 train_loss : 45.170757 , test loss : 51.625980\n",
      "epoch = 41 train_loss : 43.625504 , test loss : 49.860458\n",
      "epoch = 42 train_loss : 42.770985 , test loss : 49.061977\n",
      "epoch = 43 train_loss : 42.178135 , test loss : 48.446796\n",
      "epoch = 44 train_loss : 41.483200 , test loss : 47.431210\n",
      "epoch = 45 train_loss : 40.994923 , test loss : 46.950272\n",
      "epoch = 46 train_loss : 40.716106 , test loss : 46.750217\n",
      "epoch = 47 train_loss : 40.411182 , test loss : 46.093681\n",
      "epoch = 48 train_loss : 40.071907 , test loss : 45.940845\n",
      "epoch = 49 train_loss : 39.825447 , test loss : 45.418243\n",
      "epoch = 51 train_loss : 39.573677 , test loss : 45.210545\n",
      "epoch = 52 train_loss : 39.314716 , test loss : 45.031837\n",
      "epoch = 54 train_loss : 39.092304 , test loss : 44.383705\n",
      "epoch = 60 train_loss : 38.728867 , test loss : 44.318527\n",
      "epoch = 61 train_loss : 38.545162 , test loss : 44.073242\n",
      "epoch = 64 train_loss : 38.348263 , test loss : 43.718113\n",
      "epoch = 75 train_loss : 37.804153 , test loss : 43.522728\n",
      "epoch = 76 train_loss : 37.875774 , test loss : 43.433590\n",
      "epoch = 80 train_loss : 37.582764 , test loss : 43.372120\n",
      "epoch = 92 train_loss : 37.506985 , test loss : 43.366840\n",
      "epoch = 94 train_loss : 37.419762 , test loss : 43.243889\n",
      "epoch = 105 train_loss : 37.133812 , test loss : 43.163181\n",
      "epoch = 110 train_loss : 37.111595 , test loss : 43.162380\n",
      "epoch = 112 train_loss : 37.083469 , test loss : 42.931515\n",
      "epoch = 114 train_loss : 37.278610 , test loss : 42.886810\n",
      "epoch = 121 train_loss : 37.087326 , test loss : 42.802639\n",
      "epoch = 159 train_loss : 36.858402 , test loss : 42.653973\n",
      "epoch = 171 train_loss : 36.732487 , test loss : 42.648445\n",
      "epoch = 213 train_loss : 37.128502 , test loss : 42.633095\n",
      "epoch = 261 train_loss : 36.905956 , test loss : 42.603237\n",
      "epoch = 291 train_loss : 36.729832 , test loss : 42.550285\n",
      "--------------------------------------------------------------------------\n",
      "fold 7,train loss mean : 36.729832,test loss : 42.550285\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 657.239014 , test loss : 632.477600\n",
      "epoch = 2 train_loss : 602.319031 , test loss : 567.331299\n",
      "epoch = 3 train_loss : 573.878784 , test loss : 535.524658\n",
      "epoch = 4 train_loss : 555.962830 , test loss : 519.942261\n",
      "epoch = 5 train_loss : 539.359985 , test loss : 508.250824\n",
      "epoch = 6 train_loss : 524.898865 , test loss : 498.611450\n",
      "epoch = 7 train_loss : 510.619019 , test loss : 488.429047\n",
      "epoch = 8 train_loss : 496.809875 , test loss : 478.878052\n",
      "epoch = 9 train_loss : 482.559052 , test loss : 467.850616\n",
      "epoch = 10 train_loss : 467.262817 , test loss : 455.784058\n",
      "epoch = 11 train_loss : 451.486145 , test loss : 443.103119\n",
      "epoch = 12 train_loss : 434.509979 , test loss : 428.666077\n",
      "epoch = 13 train_loss : 416.579895 , test loss : 412.443237\n",
      "epoch = 14 train_loss : 397.630127 , test loss : 395.784943\n",
      "epoch = 15 train_loss : 377.757629 , test loss : 377.904083\n",
      "epoch = 16 train_loss : 357.223969 , test loss : 358.308746\n",
      "epoch = 17 train_loss : 335.830444 , test loss : 338.687347\n",
      "epoch = 18 train_loss : 314.310974 , test loss : 317.983917\n",
      "epoch = 19 train_loss : 292.394135 , test loss : 297.276306\n",
      "epoch = 20 train_loss : 270.450195 , test loss : 276.499664\n",
      "epoch = 21 train_loss : 249.832382 , test loss : 254.777405\n",
      "epoch = 22 train_loss : 227.958237 , test loss : 233.288101\n",
      "epoch = 23 train_loss : 207.909760 , test loss : 215.402664\n",
      "epoch = 24 train_loss : 188.231491 , test loss : 193.556091\n",
      "epoch = 25 train_loss : 169.355850 , test loss : 175.643234\n",
      "epoch = 26 train_loss : 152.001373 , test loss : 158.370529\n",
      "epoch = 27 train_loss : 136.266632 , test loss : 140.060074\n",
      "epoch = 28 train_loss : 124.615883 , test loss : 126.655846\n",
      "epoch = 29 train_loss : 108.938622 , test loss : 113.016502\n",
      "epoch = 30 train_loss : 97.217529 , test loss : 99.892723\n",
      "epoch = 31 train_loss : 86.921417 , test loss : 89.073685\n",
      "epoch = 32 train_loss : 78.572884 , test loss : 80.249458\n",
      "epoch = 33 train_loss : 70.991585 , test loss : 72.302246\n",
      "epoch = 34 train_loss : 64.729713 , test loss : 65.702179\n",
      "epoch = 35 train_loss : 59.675098 , test loss : 59.980255\n",
      "epoch = 36 train_loss : 55.801987 , test loss : 55.634922\n",
      "epoch = 37 train_loss : 52.663391 , test loss : 52.954899\n",
      "epoch = 38 train_loss : 49.701809 , test loss : 49.881042\n",
      "epoch = 39 train_loss : 47.923401 , test loss : 46.576134\n",
      "epoch = 40 train_loss : 46.079407 , test loss : 45.264694\n",
      "epoch = 41 train_loss : 44.757843 , test loss : 43.718666\n",
      "epoch = 42 train_loss : 44.356853 , test loss : 42.949173\n",
      "epoch = 43 train_loss : 43.211269 , test loss : 41.390648\n",
      "epoch = 44 train_loss : 42.484249 , test loss : 41.137081\n",
      "epoch = 45 train_loss : 41.741348 , test loss : 40.481297\n",
      "epoch = 46 train_loss : 41.615654 , test loss : 39.548885\n",
      "epoch = 48 train_loss : 41.099312 , test loss : 39.163399\n",
      "epoch = 50 train_loss : 40.514938 , test loss : 38.698967\n",
      "epoch = 51 train_loss : 40.434410 , test loss : 38.562092\n",
      "epoch = 53 train_loss : 40.146225 , test loss : 37.879757\n",
      "epoch = 55 train_loss : 40.090366 , test loss : 37.556496\n",
      "epoch = 59 train_loss : 39.415730 , test loss : 37.512653\n",
      "epoch = 63 train_loss : 39.140011 , test loss : 37.013493\n",
      "epoch = 66 train_loss : 39.058281 , test loss : 36.705357\n",
      "epoch = 68 train_loss : 38.920132 , test loss : 36.676670\n",
      "epoch = 69 train_loss : 38.904625 , test loss : 36.473377\n",
      "epoch = 72 train_loss : 38.784191 , test loss : 36.043503\n",
      "epoch = 84 train_loss : 38.328251 , test loss : 36.007633\n",
      "epoch = 89 train_loss : 38.372688 , test loss : 35.807087\n",
      "epoch = 92 train_loss : 38.179451 , test loss : 35.668224\n",
      "epoch = 94 train_loss : 38.093632 , test loss : 35.634933\n",
      "epoch = 104 train_loss : 37.906857 , test loss : 35.629452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 108 train_loss : 37.919186 , test loss : 35.327553\n",
      "epoch = 114 train_loss : 37.950809 , test loss : 35.117809\n",
      "epoch = 218 train_loss : 37.633755 , test loss : 35.091339\n",
      "epoch = 251 train_loss : 37.625908 , test loss : 34.822033\n",
      "--------------------------------------------------------------------------\n",
      "fold 8,train loss mean : 37.625908,test loss : 34.822033\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 659.456482 , test loss : 645.546265\n",
      "epoch = 2 train_loss : 602.729431 , test loss : 594.427917\n",
      "epoch = 3 train_loss : 572.437561 , test loss : 569.404602\n",
      "epoch = 4 train_loss : 554.675598 , test loss : 555.461365\n",
      "epoch = 5 train_loss : 539.395630 , test loss : 542.482056\n",
      "epoch = 6 train_loss : 525.214600 , test loss : 530.284485\n",
      "epoch = 7 train_loss : 512.018127 , test loss : 517.945740\n",
      "epoch = 8 train_loss : 498.368500 , test loss : 505.842590\n",
      "epoch = 9 train_loss : 484.646606 , test loss : 491.020294\n",
      "epoch = 10 train_loss : 470.312683 , test loss : 476.755157\n",
      "epoch = 11 train_loss : 454.968719 , test loss : 460.986237\n",
      "epoch = 12 train_loss : 438.497192 , test loss : 445.192932\n",
      "epoch = 13 train_loss : 421.021790 , test loss : 427.674805\n",
      "epoch = 14 train_loss : 402.599854 , test loss : 409.845886\n",
      "epoch = 15 train_loss : 383.446655 , test loss : 390.988922\n",
      "epoch = 16 train_loss : 363.060364 , test loss : 371.267517\n",
      "epoch = 17 train_loss : 342.182861 , test loss : 350.601440\n",
      "epoch = 18 train_loss : 320.942200 , test loss : 329.343597\n",
      "epoch = 19 train_loss : 299.235413 , test loss : 308.145599\n",
      "epoch = 20 train_loss : 277.318237 , test loss : 286.551208\n",
      "epoch = 21 train_loss : 255.786728 , test loss : 264.739380\n",
      "epoch = 22 train_loss : 235.247314 , test loss : 243.588470\n",
      "epoch = 23 train_loss : 213.957382 , test loss : 222.226212\n",
      "epoch = 24 train_loss : 194.001175 , test loss : 202.192551\n",
      "epoch = 25 train_loss : 175.426422 , test loss : 182.779266\n",
      "epoch = 26 train_loss : 157.352386 , test loss : 163.954971\n",
      "epoch = 27 train_loss : 141.089142 , test loss : 147.144684\n",
      "epoch = 28 train_loss : 125.834686 , test loss : 131.084091\n",
      "epoch = 29 train_loss : 112.682648 , test loss : 117.291840\n",
      "epoch = 30 train_loss : 100.323532 , test loss : 103.942314\n",
      "epoch = 31 train_loss : 90.133133 , test loss : 93.384018\n",
      "epoch = 32 train_loss : 81.235931 , test loss : 83.355362\n",
      "epoch = 33 train_loss : 73.138405 , test loss : 74.081055\n",
      "epoch = 34 train_loss : 66.778572 , test loss : 67.770790\n",
      "epoch = 35 train_loss : 61.304264 , test loss : 61.856747\n",
      "epoch = 36 train_loss : 56.956871 , test loss : 56.693932\n",
      "epoch = 37 train_loss : 53.072456 , test loss : 52.830051\n",
      "epoch = 38 train_loss : 50.226501 , test loss : 49.366009\n",
      "epoch = 39 train_loss : 47.880283 , test loss : 46.747467\n",
      "epoch = 40 train_loss : 46.606129 , test loss : 45.444660\n",
      "epoch = 41 train_loss : 44.961395 , test loss : 43.229309\n",
      "epoch = 42 train_loss : 43.678757 , test loss : 42.184967\n",
      "epoch = 43 train_loss : 42.979233 , test loss : 40.888206\n",
      "epoch = 44 train_loss : 42.306995 , test loss : 40.231476\n",
      "epoch = 46 train_loss : 41.642323 , test loss : 39.367950\n",
      "epoch = 47 train_loss : 41.049225 , test loss : 39.097065\n",
      "epoch = 50 train_loss : 40.532715 , test loss : 38.027554\n",
      "epoch = 52 train_loss : 40.018188 , test loss : 37.769684\n",
      "epoch = 58 train_loss : 39.467762 , test loss : 37.522610\n",
      "epoch = 59 train_loss : 39.542439 , test loss : 37.371918\n",
      "epoch = 63 train_loss : 39.098309 , test loss : 37.108765\n",
      "epoch = 66 train_loss : 39.069511 , test loss : 37.020580\n",
      "epoch = 68 train_loss : 38.787086 , test loss : 36.878170\n",
      "epoch = 70 train_loss : 38.799992 , test loss : 36.833214\n",
      "epoch = 72 train_loss : 38.751659 , test loss : 36.745670\n",
      "epoch = 73 train_loss : 38.557972 , test loss : 36.606556\n",
      "epoch = 74 train_loss : 38.464111 , test loss : 36.559681\n",
      "epoch = 82 train_loss : 38.305824 , test loss : 36.336418\n",
      "epoch = 85 train_loss : 38.274883 , test loss : 36.270794\n",
      "epoch = 88 train_loss : 38.128941 , test loss : 36.237114\n",
      "epoch = 89 train_loss : 38.137348 , test loss : 36.169598\n",
      "epoch = 93 train_loss : 38.268433 , test loss : 36.076794\n",
      "epoch = 105 train_loss : 37.787880 , test loss : 36.031067\n",
      "epoch = 110 train_loss : 37.808796 , test loss : 35.941216\n",
      "epoch = 113 train_loss : 37.676281 , test loss : 35.933613\n",
      "epoch = 122 train_loss : 37.945541 , test loss : 35.749676\n",
      "epoch = 138 train_loss : 37.620190 , test loss : 35.619801\n",
      "epoch = 201 train_loss : 37.518944 , test loss : 35.604355\n",
      "epoch = 216 train_loss : 37.806126 , test loss : 35.543766\n",
      "epoch = 261 train_loss : 37.454540 , test loss : 35.425758\n",
      "--------------------------------------------------------------------------\n",
      "fold 9,train loss mean : 37.454540,test loss : 35.425758\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 656.332947 , test loss : 631.335754\n",
      "epoch = 2 train_loss : 599.036316 , test loss : 587.374878\n",
      "epoch = 3 train_loss : 569.320374 , test loss : 567.762756\n",
      "epoch = 4 train_loss : 551.622437 , test loss : 551.852478\n",
      "epoch = 5 train_loss : 536.066101 , test loss : 534.766724\n",
      "epoch = 6 train_loss : 521.795593 , test loss : 520.951721\n",
      "epoch = 7 train_loss : 508.008453 , test loss : 506.639099\n",
      "epoch = 8 train_loss : 494.584564 , test loss : 494.166931\n",
      "epoch = 9 train_loss : 480.428070 , test loss : 479.158691\n",
      "epoch = 10 train_loss : 465.885834 , test loss : 463.799408\n",
      "epoch = 11 train_loss : 450.227234 , test loss : 448.848480\n",
      "epoch = 12 train_loss : 433.568817 , test loss : 431.660614\n",
      "epoch = 13 train_loss : 415.957520 , test loss : 415.528046\n",
      "epoch = 14 train_loss : 397.105347 , test loss : 396.583221\n",
      "epoch = 15 train_loss : 377.477692 , test loss : 376.026306\n",
      "epoch = 16 train_loss : 356.879944 , test loss : 356.608734\n",
      "epoch = 17 train_loss : 335.779694 , test loss : 335.955475\n",
      "epoch = 18 train_loss : 314.151276 , test loss : 315.110870\n",
      "epoch = 19 train_loss : 292.067932 , test loss : 290.484741\n",
      "epoch = 20 train_loss : 270.819183 , test loss : 272.379822\n",
      "epoch = 21 train_loss : 249.050095 , test loss : 249.644272\n",
      "epoch = 22 train_loss : 227.494934 , test loss : 227.189789\n",
      "epoch = 23 train_loss : 206.898605 , test loss : 209.363678\n",
      "epoch = 24 train_loss : 187.751846 , test loss : 192.710800\n",
      "epoch = 25 train_loss : 169.799454 , test loss : 172.479141\n",
      "epoch = 26 train_loss : 151.225861 , test loss : 154.925491\n",
      "epoch = 27 train_loss : 135.611572 , test loss : 140.204666\n",
      "epoch = 28 train_loss : 121.465256 , test loss : 123.570839\n",
      "epoch = 29 train_loss : 108.006905 , test loss : 110.390656\n",
      "epoch = 30 train_loss : 96.419289 , test loss : 99.560364\n",
      "epoch = 31 train_loss : 86.267120 , test loss : 89.726196\n",
      "epoch = 32 train_loss : 77.689980 , test loss : 81.116859\n",
      "epoch = 33 train_loss : 70.379074 , test loss : 73.668457\n",
      "epoch = 34 train_loss : 64.405060 , test loss : 67.100380\n",
      "epoch = 35 train_loss : 59.417255 , test loss : 63.038227\n",
      "epoch = 36 train_loss : 55.082817 , test loss : 58.091648\n",
      "epoch = 37 train_loss : 52.425755 , test loss : 55.113956\n",
      "epoch = 38 train_loss : 49.860161 , test loss : 52.496464\n",
      "epoch = 39 train_loss : 47.558994 , test loss : 50.704281\n",
      "epoch = 40 train_loss : 45.566860 , test loss : 48.053959\n",
      "epoch = 41 train_loss : 44.790844 , test loss : 47.228855\n",
      "epoch = 42 train_loss : 43.627235 , test loss : 45.361320\n",
      "epoch = 43 train_loss : 42.725220 , test loss : 44.971874\n",
      "epoch = 44 train_loss : 42.159805 , test loss : 44.302166\n",
      "epoch = 45 train_loss : 41.583927 , test loss : 43.881241\n",
      "epoch = 46 train_loss : 41.129921 , test loss : 43.252636\n",
      "epoch = 47 train_loss : 40.733204 , test loss : 43.079723\n",
      "epoch = 48 train_loss : 40.520245 , test loss : 42.848728\n",
      "epoch = 49 train_loss : 40.259415 , test loss : 42.522888\n",
      "epoch = 50 train_loss : 40.492489 , test loss : 42.277267\n",
      "epoch = 51 train_loss : 40.095211 , test loss : 42.213467\n",
      "epoch = 52 train_loss : 39.865242 , test loss : 42.060089\n",
      "epoch = 53 train_loss : 39.712860 , test loss : 41.888008\n",
      "epoch = 54 train_loss : 39.555843 , test loss : 41.775814\n",
      "epoch = 55 train_loss : 39.613304 , test loss : 41.524250\n",
      "epoch = 57 train_loss : 39.424870 , test loss : 41.254616\n",
      "epoch = 61 train_loss : 39.002308 , test loss : 41.069794\n",
      "epoch = 65 train_loss : 38.765568 , test loss : 40.819794\n",
      "epoch = 70 train_loss : 38.496296 , test loss : 40.405975\n",
      "epoch = 73 train_loss : 38.293686 , test loss : 40.355968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 75 train_loss : 38.328602 , test loss : 40.237507\n",
      "epoch = 79 train_loss : 38.098667 , test loss : 40.084515\n",
      "epoch = 80 train_loss : 38.153839 , test loss : 40.043186\n",
      "epoch = 81 train_loss : 38.179031 , test loss : 40.032951\n",
      "epoch = 82 train_loss : 38.235321 , test loss : 39.957638\n",
      "epoch = 84 train_loss : 37.884460 , test loss : 39.749493\n",
      "epoch = 95 train_loss : 37.798962 , test loss : 39.618126\n",
      "epoch = 98 train_loss : 37.622265 , test loss : 39.347126\n",
      "epoch = 107 train_loss : 37.643524 , test loss : 39.291512\n",
      "epoch = 116 train_loss : 37.549572 , test loss : 39.266895\n",
      "epoch = 117 train_loss : 37.382343 , test loss : 39.229248\n",
      "epoch = 118 train_loss : 37.418674 , test loss : 39.168411\n",
      "epoch = 125 train_loss : 37.308880 , test loss : 38.930038\n",
      "epoch = 143 train_loss : 37.282761 , test loss : 38.818230\n",
      "epoch = 166 train_loss : 37.287483 , test loss : 38.710926\n",
      "epoch = 210 train_loss : 37.120750 , test loss : 38.553799\n",
      "epoch = 333 train_loss : 37.171101 , test loss : 38.524731\n",
      "--------------------------------------------------------------------------\n",
      "fold 10,train loss mean : 37.171101,test loss : 38.524731\n",
      "-------------------------------------------------------------------------\n",
      "10 fold ,total train loss mean : 37.199241,total test loss mean : 39.226812 \n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_net():\n",
    "    return nn.Sequential(nn.Linear(x11.shape[1],128),nn.Linear(128,1))\n",
    "net1=get_net()\n",
    "train_kfold(net1,1000,0.0005,10,x11,y11,256,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T02:29:04.667422Z",
     "start_time": "2022-04-19T02:29:04.637919Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Reg(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,layer_size,output_size):\n",
    "        super().__init__()\n",
    "        self.lstm_net=nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=layer_size)\n",
    "        self.dense=nn.Linear(hidden_size,output_size)\n",
    "        self.state=None\n",
    "    \n",
    "    def forward(self,x,state):\n",
    "        y,self.state=self.lstm_net(x,state)\n",
    "        output=self.dense(y.view(-1,y.shape[-1]))\n",
    "        return output,self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:52:29.730268Z",
     "start_time": "2022-04-19T05:52:29.551246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_k_fold_data_lstm(net,num_epochs,lr,train_features,train_labels,test_features,test_labels,batch_size,montum,wd):\n",
    "#     net=nn.Linear(train_features.shape[1],1)\n",
    "#     net=nn.Sequential(nn.Linear(train_features.shape[1],1))\n",
    "    loss=nn.MSELoss()\n",
    "#     optimizer=optim.SGD(net.parameters(),lr=lr,momentum=montum,weight_decay=wd)\n",
    "    optimizer=optim.Adam(net.parameters(),lr=lr,weight_decay=wd)\n",
    "    dataset=torch.utils.data.TensorDataset(train_features,train_labels)\n",
    "    data_iter=torch.utils.data.DataLoader(dataset,batch_size,shuffle=True,drop_last=True)\n",
    "    train_l,test_l=[],[]\n",
    "    state=None\n",
    "    \n",
    "    min_test_loss=1000\n",
    "    early_stop_cnt=0\n",
    "    train_loss,test_loss=0,0\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        test_loss_list,train_loss_list=[],[]\n",
    "        net.train()\n",
    "        for x,y in data_iter:\n",
    "            if state is not None:\n",
    "                if isinstance(state,tuple):\n",
    "                    state=(state[0].detach(),state[1].detach())\n",
    "                else:\n",
    "                    state=state.detach()\n",
    "                    \n",
    "            x=x.unsqueeze(dim=0)\n",
    "            (output,state)=net(x,state)\n",
    "            \n",
    "            l=loss(output,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(l.item())\n",
    "#       if (e+1) %1000==0 and test_features is not  None:\n",
    "        net.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "#             dataset_train=torch.utils.data.TensorDataset(train_features,train_labels)\n",
    "#             data_iter_train=torch.utils.data.DataLoader(dataset_train,batch_size,shuffle=True,drop_last=True)\n",
    "            \n",
    "            dataset_test=torch.utils.data.TensorDataset(test_features,test_labels)\n",
    "            data_iter_test=torch.utils.data.DataLoader(dataset_test,batch_size,shuffle=True,drop_last=True)\n",
    "            \n",
    "            for xx_test,yy_test in data_iter_test:\n",
    "                xx_test=xx_test.unsqueeze(dim=0)\n",
    "                (output_test,state)=net(xx_test,state)\n",
    "                test_loss_list.append(loss(output_test,yy_test).item())\n",
    "            test_loss=np.mean(test_loss_list)\n",
    "\n",
    "            if test_loss<min_test_loss:\n",
    "                min_test_loss=test_loss\n",
    "#                 test_l.append(test_loss)\n",
    "#                 for xx_train,yy_train in data_iter_train:\n",
    "#                     xx_train=xx_train.unsqueeze(dim=0)\n",
    "#                     (output_train,state)=net(xx_train,state)\n",
    "#                     train_loss_list.append(loss(output_train,yy_train).item())\n",
    "\n",
    "                train_loss=np.mean(train_loss_list)\n",
    "                \n",
    "                test_l.append(test_loss)\n",
    "                train_l.append(train_loss)\n",
    "                print('epoch = %d train_loss : %f , test loss : %f' % (e+1,train_loss,test_loss))\n",
    "                early_stop_cnt=0\n",
    "            else:\n",
    "                early_stop_cnt+=1\n",
    "        if early_stop_cnt > 500:\n",
    "            \n",
    "            break\n",
    "                \n",
    "#             net.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 train_l.append(loss(net(train_features),train_labels).item())\n",
    "#                 test_l.append(loss(net(test_features),test_labels).item())\n",
    "# #                 print('epoch ',(e+1),'train loss : ',train_l[-1],'test loss : ',test_l[-1])\n",
    "\n",
    "    return train_l,test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:52:30.197328Z",
     "start_time": "2022-04-19T05:52:30.155322Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kfold_data(k,j,x,y,random_state=13):\n",
    "    assert k>=1, 'k must >=1'\n",
    "    fold_size = x.shape[0] // k\n",
    "    x_train,y_train=None,None\n",
    "    row_list=list(range(x.shape[0]))\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(row_list)\n",
    "    for i in range(k):\n",
    "        idx=slice(fold_size*i,fold_size*(i+1))\n",
    "        x_part,y_part=x[row_list[idx],:],y[row_list[idx],:]\n",
    "        if i==j:\n",
    "            x_val,y_val=x_part,y_part\n",
    "        elif x_train is None:\n",
    "            x_train,y_train=x_part,y_part\n",
    "        else:\n",
    "            x_train=torch.cat((x_train,x_part))\n",
    "            y_train=torch.cat((y_train,y_part))\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:52:30.727895Z",
     "start_time": "2022-04-19T05:52:30.699391Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_kfold(net1,num_epochs,lr,k,x_data,y_data,batch_size,montum,wd):\n",
    "    train_sum_l,test_sum_l=[],[]\n",
    "    train_l,test_l=[],[]\n",
    "#     net=nn.Linear(x_data.shape[1])\n",
    "    for j in range(k):\n",
    "        net1=get_net()\n",
    "#         for p in net1.parameters():\n",
    "#             torch.nn.init.normal_(p)\n",
    "        net=net1\n",
    "        data=get_kfold_data(k,j,x_data,y_data)\n",
    "        train_l,test_l=train_k_fold_data_lstm(net,num_epochs,lr,*data,batch_size,montum,wd)\n",
    "        train_sum_l.append(train_l[-1])\n",
    "        test_sum_l.append(test_l[-1])\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        print('fold %d,train loss mean : %f,test loss : %f' % (j+1,train_l[-1],test_l[-1]))\n",
    "        print('-------------------------------------------------------------------------')\n",
    "    print('%d fold ,total train loss mean : %f,total test loss mean : %f ' % (k,np.mean(train_sum_l),np.mean(test_sum_l)))\n",
    "    print('-----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-19T05:52:32.034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 train_loss : 0.657967 , test loss : 0.472880\n",
      "epoch = 2 train_loss : 0.372705 , test loss : 0.378358\n",
      "epoch = 3 train_loss : 0.284610 , test loss : 0.305523\n",
      "epoch = 4 train_loss : 0.236541 , test loss : 0.244333\n",
      "epoch = 5 train_loss : 0.205396 , test loss : 0.215072\n",
      "epoch = 7 train_loss : 0.158358 , test loss : 0.206296\n",
      "epoch = 8 train_loss : 0.143984 , test loss : 0.198946\n",
      "epoch = 9 train_loss : 0.135191 , test loss : 0.191424\n",
      "epoch = 10 train_loss : 0.125332 , test loss : 0.189128\n",
      "epoch = 12 train_loss : 0.110688 , test loss : 0.186612\n",
      "epoch = 13 train_loss : 0.104582 , test loss : 0.157756\n",
      "epoch = 452 train_loss : 0.001220 , test loss : 0.156650\n",
      "epoch = 582 train_loss : 0.000930 , test loss : 0.148020\n",
      "--------------------------------------------------------------------------\n",
      "fold 1,train loss mean : 0.000930,test loss : 0.148020\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.656729 , test loss : 0.459039\n",
      "epoch = 2 train_loss : 0.367449 , test loss : 0.340285\n",
      "epoch = 3 train_loss : 0.279367 , test loss : 0.302749\n",
      "epoch = 4 train_loss : 0.235801 , test loss : 0.254034\n",
      "epoch = 6 train_loss : 0.173529 , test loss : 0.239171\n",
      "epoch = 7 train_loss : 0.154770 , test loss : 0.229220\n",
      "epoch = 8 train_loss : 0.142254 , test loss : 0.227942\n",
      "epoch = 9 train_loss : 0.129893 , test loss : 0.220331\n",
      "epoch = 10 train_loss : 0.118909 , test loss : 0.207635\n",
      "epoch = 11 train_loss : 0.114650 , test loss : 0.184862\n",
      "epoch = 14 train_loss : 0.094948 , test loss : 0.180118\n",
      "epoch = 20 train_loss : 0.072654 , test loss : 0.174700\n",
      "epoch = 26 train_loss : 0.056219 , test loss : 0.172753\n",
      "epoch = 31 train_loss : 0.046213 , test loss : 0.168474\n",
      "epoch = 359 train_loss : 0.001316 , test loss : 0.166828\n",
      "epoch = 584 train_loss : 0.000906 , test loss : 0.166677\n",
      "epoch = 597 train_loss : 0.000909 , test loss : 0.164041\n",
      "epoch = 688 train_loss : 0.000740 , test loss : 0.163908\n",
      "epoch = 791 train_loss : 0.000878 , test loss : 0.160571\n",
      "--------------------------------------------------------------------------\n",
      "fold 2,train loss mean : 0.000878,test loss : 0.160571\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.665590 , test loss : 0.425922\n",
      "epoch = 2 train_loss : 0.377164 , test loss : 0.305521\n",
      "epoch = 3 train_loss : 0.289957 , test loss : 0.248282\n",
      "epoch = 4 train_loss : 0.242816 , test loss : 0.203122\n",
      "epoch = 5 train_loss : 0.207331 , test loss : 0.192995\n",
      "epoch = 6 train_loss : 0.180662 , test loss : 0.170548\n",
      "epoch = 7 train_loss : 0.156502 , test loss : 0.165635\n",
      "epoch = 8 train_loss : 0.149934 , test loss : 0.158629\n",
      "epoch = 9 train_loss : 0.132325 , test loss : 0.155430\n",
      "epoch = 10 train_loss : 0.132948 , test loss : 0.154699\n",
      "epoch = 11 train_loss : 0.117004 , test loss : 0.146343\n",
      "epoch = 22 train_loss : 0.072541 , test loss : 0.143486\n",
      "epoch = 353 train_loss : 0.001568 , test loss : 0.142255\n",
      "epoch = 437 train_loss : 0.001676 , test loss : 0.142222\n",
      "epoch = 500 train_loss : 0.001135 , test loss : 0.141251\n",
      "epoch = 659 train_loss : 0.001137 , test loss : 0.140499\n",
      "epoch = 747 train_loss : 0.000621 , test loss : 0.140436\n",
      "epoch = 801 train_loss : 0.000619 , test loss : 0.134484\n",
      "--------------------------------------------------------------------------\n",
      "fold 3,train loss mean : 0.000619,test loss : 0.134484\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.663423 , test loss : 0.436706\n",
      "epoch = 2 train_loss : 0.365132 , test loss : 0.307885\n",
      "epoch = 3 train_loss : 0.281244 , test loss : 0.258136\n",
      "epoch = 4 train_loss : 0.233735 , test loss : 0.224793\n",
      "epoch = 5 train_loss : 0.202345 , test loss : 0.211975\n",
      "epoch = 6 train_loss : 0.173898 , test loss : 0.185784\n",
      "epoch = 7 train_loss : 0.163371 , test loss : 0.180800\n",
      "epoch = 8 train_loss : 0.148513 , test loss : 0.161562\n",
      "epoch = 10 train_loss : 0.127270 , test loss : 0.159108\n",
      "epoch = 12 train_loss : 0.110776 , test loss : 0.156018\n",
      "epoch = 18 train_loss : 0.083253 , test loss : 0.151777\n",
      "epoch = 20 train_loss : 0.080610 , test loss : 0.148885\n",
      "--------------------------------------------------------------------------\n",
      "fold 4,train loss mean : 0.080610,test loss : 0.148885\n",
      "-------------------------------------------------------------------------\n",
      "epoch = 1 train_loss : 0.691454 , test loss : 0.415785\n",
      "epoch = 2 train_loss : 0.374069 , test loss : 0.325631\n",
      "epoch = 3 train_loss : 0.293754 , test loss : 0.246869\n",
      "epoch = 4 train_loss : 0.242870 , test loss : 0.223933\n",
      "epoch = 5 train_loss : 0.210160 , test loss : 0.208029\n",
      "epoch = 6 train_loss : 0.186630 , test loss : 0.196101\n",
      "epoch = 7 train_loss : 0.160481 , test loss : 0.191522\n",
      "epoch = 8 train_loss : 0.148445 , test loss : 0.180246\n",
      "epoch = 9 train_loss : 0.136228 , test loss : 0.168856\n",
      "epoch = 12 train_loss : 0.113248 , test loss : 0.162730\n",
      "epoch = 15 train_loss : 0.098415 , test loss : 0.159553\n",
      "epoch = 21 train_loss : 0.075502 , test loss : 0.153375\n"
     ]
    }
   ],
   "source": [
    "x9=torch.Tensor(x)\n",
    "y9=torch.Tensor(y)\n",
    "def get_net():\n",
    "    return LSTM_Reg(162,256,2,1)\n",
    "net1=get_net()\n",
    "train_kfold(net1,1000,0.001,5,x9,y9,256,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T02:09:54.892316Z",
     "start_time": "2022-04-14T02:09:54.886316Z"
    },
    "collapsed": true
   },
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:22:41.046134Z",
     "start_time": "2022-04-19T05:22:41.020131Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Reg(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,layer_size,output_size):\n",
    "        super().__init__()\n",
    "        self.lstm_net=nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=layer_size)\n",
    "        self.dense=nn.Linear(hidden_size,output_size)\n",
    "        self.state=None\n",
    "    \n",
    "    def forward(self,x,state):\n",
    "        y,self.state=self.lstm_net(x,state)\n",
    "        output=self.dense(y.view(-1,y.shape[-1]))\n",
    "        return output,self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:22:41.659212Z",
     "start_time": "2022-04-19T05:22:41.640210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x9=torch.Tensor(x)\n",
    "y9=torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:22:43.175405Z",
     "start_time": "2022-04-19T05:22:43.047889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([32, 162])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([1, 32, 162])\n",
      "torch.Size([20, 162])\n",
      "torch.Size([20, 1])\n",
      "torch.Size([1, 20, 162])\n"
     ]
    }
   ],
   "source": [
    "a=torch.utils.data.TensorDataset(x9,y9)\n",
    "for x2,y2 in  torch.utils.data.DataLoader(a,32,shuffle=True):\n",
    "    print(x2.shape)\n",
    "    print(y2.shape)\n",
    "    print(x2.unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:22:46.511828Z",
     "start_time": "2022-04-19T05:22:46.447320Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_pytorch(num_epochs,lr,train_features,train_labels,batch_size):\n",
    "    net=LSTM_Reg(162,256,2,1)\n",
    "    loss=nn.MSELoss()\n",
    "    optimizer=optim.Adam(net.parameters(),lr=5e-3)\n",
    "    dataset=torch.utils.data.TensorDataset(train_features,train_labels)\n",
    "    data_iter=torch.utils.data.DataLoader(dataset,batch_size,shuffle=True,drop_last=True)\n",
    "    state=None\n",
    "    for e in range(num_epochs):\n",
    "        l_sum=[]\n",
    "        net.train()\n",
    "        for x,y in data_iter:\n",
    "            if state is not None:\n",
    "                if isinstance(state ,tuple):\n",
    "                    state=(state[0].detach(),state[1].detach())\n",
    "                else:\n",
    "                    state=state.detach()\n",
    "                \n",
    "            x=x.unsqueeze(dim=0)\n",
    "            (output,state)=net(x,state)\n",
    "            l=loss(output,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            l_sum.append(l.item())\n",
    "            \n",
    "        if (e+1) % 50 ==0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                print('epoch %d , train loss mean : %f,train loss std : %f' % ( e+1,np.mean(l_sum),np.std(l_sum) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:45:52.156283Z",
     "start_time": "2022-04-19T05:22:48.880629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 , train loss mean : 0.029320,train loss std : 0.011825\n",
      "epoch 100 , train loss mean : 0.013447,train loss std : 0.005459\n",
      "epoch 150 , train loss mean : 0.009132,train loss std : 0.003357\n",
      "epoch 200 , train loss mean : 0.005775,train loss std : 0.001916\n",
      "epoch 250 , train loss mean : 0.005784,train loss std : 0.002395\n",
      "epoch 300 , train loss mean : 0.004202,train loss std : 0.001604\n",
      "epoch 350 , train loss mean : 0.003526,train loss std : 0.001398\n",
      "epoch 400 , train loss mean : 0.002931,train loss std : 0.001270\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-b815e29b4dec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_pytorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-62-335d897f2fa6>\u001b[0m in \u001b[0;36mtrain_pytorch\u001b[1;34m(num_epochs, lr, train_features, train_labels, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_pytorch(1000,0.001,x9,y9,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T07:23:38.962857Z",
     "start_time": "2022-04-18T07:23:38.953355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5652"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "471*12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compared with linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T09:52:11.150561Z",
     "start_time": "2022-04-18T09:52:11.093554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_pytorch_LInear(num_epochs,lr,train_features,train_labels,batch_size):\n",
    "    net=nn.Sequential(nn.Linear(train_features.shape[1],128),nn.ReLU(),nn.Linear(128,1))\n",
    "    loss=nn.MSELoss()\n",
    "    optimizer=optim.Adam(net.parameters(),lr=lr)\n",
    "    dataset=torch.utils.data.TensorDataset(train_features,train_labels)\n",
    "    data_iter=torch.utils.data.DataLoader(dataset,batch_size,shuffle=True)\n",
    "    for e in range(num_epochs):\n",
    "        l_sum=[]\n",
    "        net.train()\n",
    "        for x,y in data_iter:\n",
    "            l=loss(net(x),y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum.append(l.item())\n",
    "        if (e+1) % 50 ==0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                print('epoch %d , train loss mean : %f,train loss std : %f' % ( e+1,np.mean(l_sum),np.std(l_sum) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T09:56:00.438177Z",
     "start_time": "2022-04-18T09:52:12.158689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 , train loss mean : 0.038039,train loss std : 0.014225\n",
      "epoch 100 , train loss mean : 0.025197,train loss std : 0.010772\n",
      "epoch 150 , train loss mean : 0.014195,train loss std : 0.006786\n",
      "epoch 200 , train loss mean : 0.010302,train loss std : 0.004486\n",
      "epoch 250 , train loss mean : 0.009503,train loss std : 0.004339\n",
      "epoch 300 , train loss mean : 0.008229,train loss std : 0.003656\n",
      "epoch 350 , train loss mean : 0.007078,train loss std : 0.002974\n",
      "epoch 400 , train loss mean : 0.007720,train loss std : 0.003838\n",
      "epoch 450 , train loss mean : 0.005004,train loss std : 0.002256\n",
      "epoch 500 , train loss mean : 0.005787,train loss std : 0.003748\n",
      "epoch 550 , train loss mean : 0.004788,train loss std : 0.002233\n",
      "epoch 600 , train loss mean : 0.004894,train loss std : 0.002861\n",
      "epoch 650 , train loss mean : 0.004689,train loss std : 0.001956\n",
      "epoch 700 , train loss mean : 0.004684,train loss std : 0.002623\n",
      "epoch 750 , train loss mean : 0.006662,train loss std : 0.004670\n",
      "epoch 800 , train loss mean : 0.005225,train loss std : 0.002757\n",
      "epoch 850 , train loss mean : 0.003030,train loss std : 0.001386\n",
      "epoch 900 , train loss mean : 0.002931,train loss std : 0.001191\n",
      "epoch 950 , train loss mean : 0.004297,train loss std : 0.002032\n",
      "epoch 1000 , train loss mean : 0.003782,train loss std : 0.002016\n"
     ]
    }
   ],
   "source": [
    "train_pytorch_LInear(1000,0.001,x9,y9,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
